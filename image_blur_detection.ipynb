{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MasoudMoeini/Image-blur-detection/blob/main/image_blur_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-5xmsZ5RQes",
        "outputId": "0a2649c0-42e8-4279-fd60-1f7f1d71ec52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ],
      "source": [
        "%load_ext tensorboard\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import numpy as np\n",
        "import matplotlib . pyplot as plt\n",
        "from tensorflow.keras import layers, losses\n",
        "from notebook.services.config import ConfigManager\n",
        "cm = ConfigManager().update('notebook', {'limit_output': 100})\n",
        "# Base CNN\n",
        "x = tf.placeholder(tf.float32, (None,224, 224, 3))\n",
        "y = tf.placeholder(tf.float32, (None,224, 224, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8pmv42pPJ4SW"
      },
      "outputs": [],
      "source": [
        "rm -rf ./logs/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BbR2mvcKA5S",
        "outputId": "84562b60-d074-45c0-d56d-f5a17dc0e88d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.7/dist-packages/keras/legacy_tf_layers/convolutional.py:575: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: UserWarning: `tf.layers.max_pooling2d` is deprecated and will be removed in a future version. Please use `tf.keras.layers.MaxPooling2D` instead.\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/keras/legacy_tf_layers/pooling.py:600: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: `tf.layers.max_pooling2d` is deprecated and will be removed in a future version. Please use `tf.keras.layers.MaxPooling2D` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:34: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:39: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:44: UserWarning: `tf.layers.max_pooling2d` is deprecated and will be removed in a future version. Please use `tf.keras.layers.MaxPooling2D` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:49: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: `tf.layers.max_pooling2d` is deprecated and will be removed in a future version. Please use `tf.keras.layers.MaxPooling2D` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:68: UserWarning: `tf.layers.max_pooling2d` is deprecated and will be removed in a future version. Please use `tf.keras.layers.MaxPooling2D` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:78: UserWarning: `tf.layers.conv2d_transpose` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2DTranspose` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/keras/legacy_tf_layers/convolutional.py:1736: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:82: UserWarning: `tf.layers.conv2d_transpose` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2DTranspose` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:86: UserWarning: `tf.layers.conv2d_transpose` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2DTranspose` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:90: UserWarning: `tf.layers.conv2d_transpose` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2DTranspose` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:94: UserWarning: `tf.layers.conv2d_transpose` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2DTranspose` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:98: UserWarning: `tf.layers.conv2d_transpose` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2DTranspose` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:102: UserWarning: `tf.layers.conv2d_transpose` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2DTranspose` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:106: UserWarning: `tf.layers.conv2d_transpose` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2DTranspose` instead.\n"
          ]
        }
      ],
      "source": [
        "# Conv1\n",
        "# Input Tensor Shape: [batch_size, 224, 224, 3]\n",
        "# Output Tensor Shape: [batch_size, 224, 224, 32]\n",
        "conv1 = tf.layers.conv2d(x, filters=32, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu)\n",
        "\n",
        "# conv2\n",
        "# Input Tensor Shape: [batch_size, 224, 224, 32]\n",
        "# Output Tensor Shape: [batch_size, 224, 224, 32]\n",
        "conv2 = tf.layers.conv2d(conv1, filters=32, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu)\n",
        "\n",
        "# pool1\n",
        "# Input Tensor Shape: [batch_size, 224, 224, 32]\n",
        "# Output Tensor Shape: [batch_size, 112, 112, 32]\n",
        "pool1 = tf.layers.max_pooling2d(conv2, pool_size=[2,2], strides=2, padding=\"same\")\n",
        "\n",
        "#conv3\n",
        "# Input Tensor Shape: [batch_size, 112, 112, 32]\n",
        "# Output Tensor Shape: [batch_size, 112, 112, 64]\n",
        "conv3 = tf.layers.conv2d(pool1, filters=64, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu)\n",
        "\n",
        "#conv4\n",
        "# Input Tensor Shape: [batch_size, 112, 112, 64]\n",
        "# Output Tensor Shape: [batch_size, 112, 112, 64]\n",
        "conv4 = tf.layers.conv2d(conv3, filters=64, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu)\n",
        "\n",
        "#pool2\n",
        "# Input Tensor Shape: [batch_size, 112, 112, 64]\n",
        "# Output Tensor Shape: [batch_size, 56, 56, 64]\n",
        "pool2 = tf.layers.max_pooling2d(conv4, pool_size=[2,2], strides=2, padding=\"same\")\n",
        "\n",
        "#conv5\n",
        "# Input Tensor Shape: [batch_size, 56, 56, 64]\n",
        "# Output Tensor Shape: [batch_size, 56, 56, 128]\n",
        "conv5 = tf.layers.conv2d(pool2, filters=128, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu)\n",
        "\n",
        "#conv6\n",
        "# Input Tensor Shape: [batch_size, 56, 56, 128]\n",
        "# Output Tensor Shape: [batch_size, 56, 56, 128]\n",
        "conv6 = tf.layers.conv2d(conv5, filters=128, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu)\n",
        "\n",
        "#pool3\n",
        "# Input Tensor Shape: [batch_size, 56, 56, 128]\n",
        "# Output Tensor Shape: [batch_size, 28, 28, 128]\n",
        "pool3 = tf.layers.max_pooling2d(conv6, pool_size=[2,2], strides=2, padding=\"same\")\n",
        "\n",
        "#conv7\n",
        "# Input Tensor Shape: [batch_size, 28, 28, 128]\n",
        "# Output Tensor Shape: [batch_size, 28, 28, 256]\n",
        "conv7 = tf.layers.conv2d(pool3, filters=256, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu)\n",
        "#conv8\n",
        "# Input Tensor Shape: [batch_size, 28, 28, 256]\n",
        "# Output Tensor Shape: [batch_size, 28, 28, 256]\n",
        "conv8 = tf.layers.conv2d(conv7, filters=256, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu)\n",
        "\n",
        "#pool4\n",
        "# Input Tensor Shape: [batch_size, 28, 28, 256]\n",
        "# Output Tensor Shape: [batch_size, 14, 14, 256]\n",
        "\n",
        "pool4 = tf.layers.max_pooling2d(conv8, pool_size=[2,2], strides=2, padding=\"same\")\n",
        "#conv9\n",
        "# Input Tensor Shape: [batch_size, 14, 14, 256]\n",
        "# Output Tensor Shape: [batch_size, 14, 14, 512]\n",
        "conv9 = tf.layers.conv2d(pool4, filters=512, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu)\n",
        "\n",
        "#pool5\n",
        "# Input Tensor Shape: [batch_size, 14, 14, 512]\n",
        "# Output Tensor Shape: [batch_size, 7, 7, 512]\n",
        "pool5 = tf.layers.max_pooling2d(conv9, pool_size=[2,2], strides=2, padding=\"same\")\n",
        "\n",
        "#------------------------------------------decode---------------------------------------\n",
        "\n",
        "#dim = int(np.prod(pool5.get_shape()[1:])) #7*7*512\n",
        "#fcl = tf.reshape(pool5, shape=[-1, dim], name ='fc1')#[batch_size,7*7*512]\n",
        "# decoder\n",
        "\n",
        "# Input Tensor Shape: [batch_size, 7, 7, 512]\n",
        "# Output Tensor Shape: [batch_size, 14, 14, 512]\n",
        "net=tf.layers.conv2d_transpose(pool5,512,[3, 3],strides = 2,padding='SAME')\n",
        "\n",
        "# Input Tensor Shape: [batch_size, 14, 14, 512]\n",
        "# Output Tensor Shape: [batch_size, 28, 28, 256]\n",
        "net=tf.layers.conv2d_transpose(net,256,[3, 3],strides = 2,padding='SAME')\n",
        "\n",
        "# Input Tensor Shape: [batch_size, 28, 28, 256]\n",
        "# Output Tensor Shape: [batch_size, 56, 56, 128]\n",
        "net=tf.layers.conv2d_transpose(net,128,[3, 3],strides = 2,padding='SAME')\n",
        "\n",
        "# Input Tensor Shape: [batch_size, 56, 56, 128]\n",
        "# Output Tensor Shape: [batch_size, 112, 112, 128]\n",
        "net=tf.layers.conv2d_transpose(net,128,[3, 3],strides = 2,padding='SAME',activation = tf.nn.relu)\n",
        "\n",
        "# Input Tensor Shape: [batch_size, 112, 112, 128]\n",
        "# Output Tensor Shape: [batch_size, 224, 224, 64]\n",
        "net=tf.layers.conv2d_transpose(net,64,[3, 3],strides = 2,padding='SAME',activation = tf.nn.relu)\n",
        "\n",
        "# Input Tensor Shape: [batch_size, 224, 224, 64]\n",
        "# Output Tensor Shape: [batch_size, 224, 224, 32]\n",
        "net=tf.layers.conv2d_transpose(net,32,[3, 3],strides = 1,padding='SAME',activation = tf.nn.relu)\n",
        "\n",
        "# Input Tensor Shape: [batch_size, 224, 224, 32]\n",
        "# Output Tensor Shape: [batch_size, 224, 224, 16]\n",
        "net=tf.layers.conv2d_transpose(net,16,[3, 3],strides = 1, padding='SAME', activation = tf.nn.sigmoid)\n",
        "\n",
        "# Input Tensor Shape: [batch_size, 224, 224, 16]\n",
        "# Output Tensor Shape: [batch_size, 224, 224, 3]\n",
        "net=tf.layers.conv2d_transpose(net,3,[3, 3],strides = 1, padding='SAME', activation = tf.nn.sigmoid)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNzgCa05zB0d"
      },
      "outputs": [],
      "source": [
        "#normlizing output\n",
        "#wmax = tf.reduce_max(net, axis=2, keepdims=True) # along width dimension\n",
        "#output_max = tf.reduce_max(wmax, axis=1, keepdims=True) # along height dimension\n",
        "#normalized_output = net / output_max\n",
        "#alpha = 1.1\n",
        "#loss = tf.reduce_mean( tf.square( (1.0/(alpha - y)) * (normalized_output - y) ))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JkQpeiENg4tD"
      },
      "outputs": [],
      "source": [
        "!unzip -qq train.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "P-CIQbYmlE3j"
      },
      "outputs": [],
      "source": [
        "from datareader import DataReader\n",
        "train_images = DataReader('train')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "FBu6qF1NS4I6"
      },
      "outputs": [],
      "source": [
        "## Optimize\n",
        "batch_size=50\n",
        "num_batches= train_images.num_batches_of_size(batch_size)\n",
        "learning_rate = 0.0002\n",
        "n_epochs = 50\n",
        "loss = tf.reduce_mean(tf.square(net - y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "train  = optimizer.minimize(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPqqHoDRP0e_",
        "outputId": "1eab8a68-bb18-40d1-c10b-3da40ab01d43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch  0\n",
            "epoch 0 batch number 0    batch loss: 0.22533035278320312\n",
            "epoch 0 batch number 1    batch loss: 0.2334718257188797\n",
            "epoch 0 batch number 2    batch loss: 0.22322776913642883\n",
            "epoch 0 batch number 3    batch loss: 0.22471299767494202\n",
            "epoch 0 batch number 4    batch loss: 0.2277933955192566\n",
            "epoch 0 batch number 5    batch loss: 0.22371216118335724\n",
            "epoch 0 batch number 6    batch loss: 0.22622743248939514\n",
            "epoch 0 batch number 7    batch loss: 0.2229986935853958\n",
            "epoch 0 batch number 8    batch loss: 0.2202158123254776\n",
            "epoch 0 batch number 9    batch loss: 0.22000345587730408\n",
            "epoch 0 batch number 10    batch loss: 0.21937412023544312\n",
            "epoch 0 batch number 11    batch loss: 0.22103510797023773\n",
            "epoch 0 batch number 12    batch loss: 0.21932628750801086\n",
            "epoch 0 batch number 13    batch loss: 0.21571244299411774\n",
            "epoch 0 batch number 14    batch loss: 0.21846695244312286\n",
            "epoch 0 batch number 15    batch loss: 0.2123921811580658\n",
            "epoch 0 batch number 16    batch loss: 0.21059809625148773\n",
            "epoch 0 batch number 17    batch loss: 0.21682439744472504\n",
            "epoch 0 batch number 18    batch loss: 0.20992231369018555\n",
            "epoch 0 batch number 19    batch loss: 0.21136456727981567\n",
            " Average epoch losses: 0.22013549506664276 \n",
            "epoch  1\n",
            "epoch 1 batch number 0    batch loss: 0.2080525904893875\n",
            "epoch 1 batch number 1    batch loss: 0.19602815806865692\n",
            "epoch 1 batch number 2    batch loss: 0.2025044709444046\n",
            "epoch 1 batch number 3    batch loss: 0.2066832035779953\n",
            "epoch 1 batch number 4    batch loss: 0.1965334415435791\n",
            "epoch 1 batch number 5    batch loss: 0.19344152510166168\n",
            "epoch 1 batch number 6    batch loss: 0.20034684240818024\n",
            "epoch 1 batch number 7    batch loss: 0.20164377987384796\n",
            "epoch 1 batch number 8    batch loss: 0.2020573914051056\n",
            "epoch 1 batch number 9    batch loss: 0.19807444512844086\n",
            "epoch 1 batch number 10    batch loss: 0.19573073089122772\n",
            "epoch 1 batch number 11    batch loss: 0.19385498762130737\n",
            "epoch 1 batch number 12    batch loss: 0.19393010437488556\n",
            "epoch 1 batch number 13    batch loss: 0.188047856092453\n",
            "epoch 1 batch number 14    batch loss: 0.20024530589580536\n",
            "epoch 1 batch number 15    batch loss: 0.19178518652915955\n",
            "epoch 1 batch number 16    batch loss: 0.19046646356582642\n",
            "epoch 1 batch number 17    batch loss: 0.1834583878517151\n",
            "epoch 1 batch number 18    batch loss: 0.19369398057460785\n",
            "epoch 1 batch number 19    batch loss: 0.18952183425426483\n",
            " Average epoch losses: 0.1963050216436386 \n",
            "epoch  2\n",
            "epoch 2 batch number 0    batch loss: 0.19202838838100433\n",
            "epoch 2 batch number 1    batch loss: 0.19723357260227203\n",
            "epoch 2 batch number 2    batch loss: 0.18957777321338654\n",
            "epoch 2 batch number 3    batch loss: 0.18952864408493042\n",
            "epoch 2 batch number 4    batch loss: 0.18928317725658417\n",
            "epoch 2 batch number 5    batch loss: 0.19132128357887268\n",
            "epoch 2 batch number 6    batch loss: 0.18512322008609772\n",
            "epoch 2 batch number 7    batch loss: 0.19234706461429596\n",
            "epoch 2 batch number 8    batch loss: 0.19272953271865845\n",
            "epoch 2 batch number 9    batch loss: 0.19308656454086304\n",
            "epoch 2 batch number 10    batch loss: 0.18868635594844818\n",
            "epoch 2 batch number 11    batch loss: 0.1921965479850769\n",
            "epoch 2 batch number 12    batch loss: 0.19323763251304626\n",
            "epoch 2 batch number 13    batch loss: 0.19376759231090546\n",
            "epoch 2 batch number 14    batch loss: 0.19299213588237762\n",
            "epoch 2 batch number 15    batch loss: 0.1831541657447815\n",
            "epoch 2 batch number 16    batch loss: 0.18971233069896698\n",
            "epoch 2 batch number 17    batch loss: 0.18492022156715393\n",
            "epoch 2 batch number 18    batch loss: 0.18586698174476624\n",
            "epoch 2 batch number 19    batch loss: 0.190164253115654\n",
            " Average epoch losses: 0.19034788012504578 \n",
            "epoch  3\n",
            "epoch 3 batch number 0    batch loss: 0.1841025948524475\n",
            "epoch 3 batch number 1    batch loss: 0.18474484980106354\n",
            "epoch 3 batch number 2    batch loss: 0.19069485366344452\n",
            "epoch 3 batch number 3    batch loss: 0.18853864073753357\n",
            "epoch 3 batch number 4    batch loss: 0.1861082762479782\n",
            "epoch 3 batch number 5    batch loss: 0.18854212760925293\n",
            "epoch 3 batch number 6    batch loss: 0.18643273413181305\n",
            "epoch 3 batch number 7    batch loss: 0.1886017620563507\n",
            "epoch 3 batch number 8    batch loss: 0.18369382619857788\n",
            "epoch 3 batch number 9    batch loss: 0.18259193003177643\n",
            "epoch 3 batch number 10    batch loss: 0.18543918430805206\n",
            "epoch 3 batch number 11    batch loss: 0.18672433495521545\n",
            "epoch 3 batch number 12    batch loss: 0.17939040064811707\n",
            "epoch 3 batch number 13    batch loss: 0.19032935798168182\n",
            "epoch 3 batch number 14    batch loss: 0.18529535830020905\n",
            "epoch 3 batch number 15    batch loss: 0.18226024508476257\n",
            "epoch 3 batch number 16    batch loss: 0.17954231798648834\n",
            "epoch 3 batch number 17    batch loss: 0.17537212371826172\n",
            "epoch 3 batch number 18    batch loss: 0.17747275531291962\n",
            "epoch 3 batch number 19    batch loss: 0.17050224542617798\n",
            " Average epoch losses: 0.183818981051445 \n",
            "epoch  4\n",
            "epoch 4 batch number 0    batch loss: 0.1774192899465561\n",
            "epoch 4 batch number 1    batch loss: 0.1682746857404709\n",
            "epoch 4 batch number 2    batch loss: 0.17384731769561768\n",
            "epoch 4 batch number 3    batch loss: 0.1694534569978714\n",
            "epoch 4 batch number 4    batch loss: 0.1679844707250595\n",
            "epoch 4 batch number 5    batch loss: 0.16736657917499542\n",
            "epoch 4 batch number 6    batch loss: 0.16978013515472412\n",
            "epoch 4 batch number 7    batch loss: 0.16927672922611237\n",
            "epoch 4 batch number 8    batch loss: 0.16249269247055054\n",
            "epoch 4 batch number 9    batch loss: 0.1595454216003418\n",
            "epoch 4 batch number 10    batch loss: 0.1597297042608261\n",
            "epoch 4 batch number 11    batch loss: 0.165785551071167\n",
            "epoch 4 batch number 12    batch loss: 0.16670387983322144\n",
            "epoch 4 batch number 13    batch loss: 0.16242194175720215\n",
            "epoch 4 batch number 14    batch loss: 0.15366362035274506\n",
            "epoch 4 batch number 15    batch loss: 0.1563054323196411\n",
            "epoch 4 batch number 16    batch loss: 0.1570757031440735\n",
            "epoch 4 batch number 17    batch loss: 0.15610508620738983\n",
            "epoch 4 batch number 18    batch loss: 0.15891337394714355\n",
            "epoch 4 batch number 19    batch loss: 0.1546712964773178\n",
            " Average epoch losses: 0.16384080052375793 \n",
            "epoch  5\n",
            "epoch 5 batch number 0    batch loss: 0.15176032483577728\n",
            "epoch 5 batch number 1    batch loss: 0.15609902143478394\n",
            "epoch 5 batch number 2    batch loss: 0.16330569982528687\n",
            "epoch 5 batch number 3    batch loss: 0.14960387349128723\n",
            "epoch 5 batch number 4    batch loss: 0.15073204040527344\n",
            "epoch 5 batch number 5    batch loss: 0.14491280913352966\n",
            "epoch 5 batch number 6    batch loss: 0.15217182040214539\n",
            "epoch 5 batch number 7    batch loss: 0.14973272383213043\n",
            "epoch 5 batch number 8    batch loss: 0.14910480380058289\n",
            "epoch 5 batch number 9    batch loss: 0.14190353453159332\n",
            "epoch 5 batch number 10    batch loss: 0.14246274530887604\n",
            "epoch 5 batch number 11    batch loss: 0.14224225282669067\n",
            "epoch 5 batch number 12    batch loss: 0.1448693722486496\n",
            "epoch 5 batch number 13    batch loss: 0.13825106620788574\n",
            "epoch 5 batch number 14    batch loss: 0.13870395720005035\n",
            "epoch 5 batch number 15    batch loss: 0.1360495537519455\n",
            "epoch 5 batch number 16    batch loss: 0.12701746821403503\n",
            "epoch 5 batch number 17    batch loss: 0.13050153851509094\n",
            "epoch 5 batch number 18    batch loss: 0.13328105211257935\n",
            "epoch 5 batch number 19    batch loss: 0.13486206531524658\n",
            " Average epoch losses: 0.14387838542461395 \n",
            "epoch  6\n",
            "epoch 6 batch number 0    batch loss: 0.13468775153160095\n",
            "epoch 6 batch number 1    batch loss: 0.1370086967945099\n",
            "epoch 6 batch number 2    batch loss: 0.1411302536725998\n",
            "epoch 6 batch number 3    batch loss: 0.14088259637355804\n",
            "epoch 6 batch number 4    batch loss: 0.14215485751628876\n",
            "epoch 6 batch number 5    batch loss: 0.14278055727481842\n",
            "epoch 6 batch number 6    batch loss: 0.14177505671977997\n",
            "epoch 6 batch number 7    batch loss: 0.13839448988437653\n",
            "epoch 6 batch number 8    batch loss: 0.13107630610466003\n",
            "epoch 6 batch number 9    batch loss: 0.1312704086303711\n",
            "epoch 6 batch number 10    batch loss: 0.13203127682209015\n",
            "epoch 6 batch number 11    batch loss: 0.1265210211277008\n",
            "epoch 6 batch number 12    batch loss: 0.13188894093036652\n",
            "epoch 6 batch number 13    batch loss: 0.12658031284809113\n",
            "epoch 6 batch number 14    batch loss: 0.1255323886871338\n",
            "epoch 6 batch number 15    batch loss: 0.12725095450878143\n",
            "epoch 6 batch number 16    batch loss: 0.1290791779756546\n",
            "epoch 6 batch number 17    batch loss: 0.13274572789669037\n",
            "epoch 6 batch number 18    batch loss: 0.12865792214870453\n",
            "epoch 6 batch number 19    batch loss: 0.11694162338972092\n",
            " Average epoch losses: 0.1329195201396942 \n",
            "epoch  7\n",
            "epoch 7 batch number 0    batch loss: 0.14101068675518036\n",
            "epoch 7 batch number 1    batch loss: 0.11883391439914703\n",
            "epoch 7 batch number 2    batch loss: 0.1266307532787323\n",
            "epoch 7 batch number 3    batch loss: 0.1300000697374344\n",
            "epoch 7 batch number 4    batch loss: 0.12726987898349762\n",
            "epoch 7 batch number 5    batch loss: 0.11942557990550995\n",
            "epoch 7 batch number 6    batch loss: 0.12723752856254578\n",
            "epoch 7 batch number 7    batch loss: 0.1278267800807953\n",
            "epoch 7 batch number 8    batch loss: 0.12117601186037064\n",
            "epoch 7 batch number 9    batch loss: 0.12731501460075378\n",
            "epoch 7 batch number 10    batch loss: 0.12101497501134872\n",
            "epoch 7 batch number 11    batch loss: 0.12471567094326019\n",
            "epoch 7 batch number 12    batch loss: 0.11632996797561646\n",
            "epoch 7 batch number 13    batch loss: 0.12366839498281479\n",
            "epoch 7 batch number 14    batch loss: 0.12174297124147415\n",
            "epoch 7 batch number 15    batch loss: 0.11852649599313736\n",
            "epoch 7 batch number 16    batch loss: 0.11373336613178253\n",
            "epoch 7 batch number 17    batch loss: 0.1149078831076622\n",
            "epoch 7 batch number 18    batch loss: 0.12424657493829727\n",
            "epoch 7 batch number 19    batch loss: 0.1262083649635315\n",
            " Average epoch losses: 0.12359104305505753 \n",
            "epoch  8\n",
            "epoch 8 batch number 0    batch loss: 0.11578846722841263\n",
            "epoch 8 batch number 1    batch loss: 0.11960885673761368\n",
            "epoch 8 batch number 2    batch loss: 0.1207926869392395\n",
            "epoch 8 batch number 3    batch loss: 0.1120370402932167\n",
            "epoch 8 batch number 4    batch loss: 0.12754330039024353\n",
            "epoch 8 batch number 5    batch loss: 0.12120866030454636\n",
            "epoch 8 batch number 6    batch loss: 0.12320523709058762\n",
            "epoch 8 batch number 7    batch loss: 0.12867599725723267\n",
            "epoch 8 batch number 8    batch loss: 0.12229560315608978\n",
            "epoch 8 batch number 9    batch loss: 0.12200459092855453\n",
            "epoch 8 batch number 10    batch loss: 0.13770104944705963\n",
            "epoch 8 batch number 11    batch loss: 0.11413025110960007\n",
            "epoch 8 batch number 12    batch loss: 0.12215474992990494\n",
            "epoch 8 batch number 13    batch loss: 0.12456575036048889\n",
            "epoch 8 batch number 14    batch loss: 0.12035652250051498\n",
            "epoch 8 batch number 15    batch loss: 0.11743252724409103\n",
            "epoch 8 batch number 16    batch loss: 0.11985732614994049\n",
            "epoch 8 batch number 17    batch loss: 0.12997807562351227\n",
            "epoch 8 batch number 18    batch loss: 0.11185485124588013\n",
            "epoch 8 batch number 19    batch loss: 0.12686532735824585\n",
            " Average epoch losses: 0.12190284579992294 \n",
            "epoch  9\n",
            "epoch 9 batch number 0    batch loss: 0.11692538857460022\n",
            "epoch 9 batch number 1    batch loss: 0.11033214628696442\n",
            "epoch 9 batch number 2    batch loss: 0.12414737045764923\n",
            "epoch 9 batch number 3    batch loss: 0.1163548082113266\n",
            "epoch 9 batch number 4    batch loss: 0.12319255620241165\n",
            "epoch 9 batch number 5    batch loss: 0.10973741114139557\n",
            "epoch 9 batch number 6    batch loss: 0.12048432230949402\n",
            "epoch 9 batch number 7    batch loss: 0.10615352541208267\n",
            "epoch 9 batch number 8    batch loss: 0.11031152307987213\n",
            "epoch 9 batch number 9    batch loss: 0.12270625680685043\n",
            "epoch 9 batch number 10    batch loss: 0.11889369785785675\n",
            "epoch 9 batch number 11    batch loss: 0.11693735420703888\n",
            "epoch 9 batch number 12    batch loss: 0.10629308968782425\n",
            "epoch 9 batch number 13    batch loss: 0.11775825917720795\n",
            "epoch 9 batch number 14    batch loss: 0.12851987779140472\n",
            "epoch 9 batch number 15    batch loss: 0.11725129932165146\n",
            "epoch 9 batch number 16    batch loss: 0.11353225260972977\n",
            "epoch 9 batch number 17    batch loss: 0.1216239333152771\n",
            "epoch 9 batch number 18    batch loss: 0.12581469118595123\n",
            "epoch 9 batch number 19    batch loss: 0.11863906681537628\n",
            " Average epoch losses: 0.11728043854236603 \n",
            "epoch  10\n",
            "epoch 10 batch number 0    batch loss: 0.11675199866294861\n",
            "epoch 10 batch number 1    batch loss: 0.11392013728618622\n",
            "epoch 10 batch number 2    batch loss: 0.11281877756118774\n",
            "epoch 10 batch number 3    batch loss: 0.13210195302963257\n",
            "epoch 10 batch number 4    batch loss: 0.12076100707054138\n",
            "epoch 10 batch number 5    batch loss: 0.1234220340847969\n",
            "epoch 10 batch number 6    batch loss: 0.11134324967861176\n",
            "epoch 10 batch number 7    batch loss: 0.11310280859470367\n",
            "epoch 10 batch number 8    batch loss: 0.11765896528959274\n",
            "epoch 10 batch number 9    batch loss: 0.10971670597791672\n",
            "epoch 10 batch number 10    batch loss: 0.11176727712154388\n",
            "epoch 10 batch number 11    batch loss: 0.11215709894895554\n",
            "epoch 10 batch number 12    batch loss: 0.11356233060359955\n",
            "epoch 10 batch number 13    batch loss: 0.1135382354259491\n",
            "epoch 10 batch number 14    batch loss: 0.11176153272390366\n",
            "epoch 10 batch number 15    batch loss: 0.1096360832452774\n",
            "epoch 10 batch number 16    batch loss: 0.10679858922958374\n",
            "epoch 10 batch number 17    batch loss: 0.11654090881347656\n",
            "epoch 10 batch number 18    batch loss: 0.1064169704914093\n",
            "epoch 10 batch number 19    batch loss: 0.11181953549385071\n",
            " Average epoch losses: 0.11427980661392212 \n",
            "epoch  11\n",
            "epoch 11 batch number 0    batch loss: 0.11883893609046936\n",
            "epoch 11 batch number 1    batch loss: 0.11601065844297409\n",
            "epoch 11 batch number 2    batch loss: 0.11239472031593323\n",
            "epoch 11 batch number 3    batch loss: 0.11136098951101303\n",
            "epoch 11 batch number 4    batch loss: 0.12085891515016556\n",
            "epoch 11 batch number 5    batch loss: 0.1087426096200943\n",
            "epoch 11 batch number 6    batch loss: 0.11078959703445435\n",
            "epoch 11 batch number 7    batch loss: 0.10859023779630661\n",
            "epoch 11 batch number 8    batch loss: 0.1111903265118599\n",
            "epoch 11 batch number 9    batch loss: 0.10495031625032425\n",
            "epoch 11 batch number 10    batch loss: 0.11209610849618912\n",
            "epoch 11 batch number 11    batch loss: 0.113100565969944\n",
            "epoch 11 batch number 12    batch loss: 0.10750676691532135\n",
            "epoch 11 batch number 13    batch loss: 0.1094757467508316\n",
            "epoch 11 batch number 14    batch loss: 0.10543064773082733\n",
            "epoch 11 batch number 15    batch loss: 0.11451365053653717\n",
            "epoch 11 batch number 16    batch loss: 0.10955775529146194\n",
            "epoch 11 batch number 17    batch loss: 0.1004541739821434\n",
            "epoch 11 batch number 18    batch loss: 0.1071605235338211\n",
            "epoch 11 batch number 19    batch loss: 0.10566183924674988\n",
            " Average epoch losses: 0.11043425649404526 \n",
            "epoch  12\n",
            "epoch 12 batch number 0    batch loss: 0.10980717092752457\n",
            "epoch 12 batch number 1    batch loss: 0.10492084175348282\n",
            "epoch 12 batch number 2    batch loss: 0.11949297040700912\n",
            "epoch 12 batch number 3    batch loss: 0.10828414559364319\n",
            "epoch 12 batch number 4    batch loss: 0.11084619164466858\n",
            "epoch 12 batch number 5    batch loss: 0.11330430209636688\n",
            "epoch 12 batch number 6    batch loss: 0.1122455969452858\n",
            "epoch 12 batch number 7    batch loss: 0.1109251081943512\n",
            "epoch 12 batch number 8    batch loss: 0.11673098802566528\n",
            "epoch 12 batch number 9    batch loss: 0.10781937837600708\n",
            "epoch 12 batch number 10    batch loss: 0.10578059405088425\n",
            "epoch 12 batch number 11    batch loss: 0.11061222851276398\n",
            "epoch 12 batch number 12    batch loss: 0.10367566347122192\n",
            "epoch 12 batch number 13    batch loss: 0.11156699806451797\n",
            "epoch 12 batch number 14    batch loss: 0.10957519710063934\n",
            "epoch 12 batch number 15    batch loss: 0.11042716354131699\n",
            "epoch 12 batch number 16    batch loss: 0.11995719373226166\n",
            "epoch 12 batch number 17    batch loss: 0.10277517884969711\n",
            "epoch 12 batch number 18    batch loss: 0.10244473069906235\n",
            "epoch 12 batch number 19    batch loss: 0.1103208065032959\n",
            " Average epoch losses: 0.11007563024759293 \n",
            "epoch  13\n",
            "epoch 13 batch number 0    batch loss: 0.10839800536632538\n",
            "epoch 13 batch number 1    batch loss: 0.10986268520355225\n",
            "epoch 13 batch number 2    batch loss: 0.11110500991344452\n",
            "epoch 13 batch number 3    batch loss: 0.11305524408817291\n",
            "epoch 13 batch number 4    batch loss: 0.10475579649209976\n",
            "epoch 13 batch number 5    batch loss: 0.1039343997836113\n",
            "epoch 13 batch number 6    batch loss: 0.11494886875152588\n",
            "epoch 13 batch number 7    batch loss: 0.09650734066963196\n",
            "epoch 13 batch number 8    batch loss: 0.10055192559957504\n",
            "epoch 13 batch number 9    batch loss: 0.11629927158355713\n",
            "epoch 13 batch number 10    batch loss: 0.11363855004310608\n",
            "epoch 13 batch number 11    batch loss: 0.10953427106142044\n",
            "epoch 13 batch number 12    batch loss: 0.11271510273218155\n",
            "epoch 13 batch number 13    batch loss: 0.10704848170280457\n",
            "epoch 13 batch number 14    batch loss: 0.10913806408643723\n",
            "epoch 13 batch number 15    batch loss: 0.11802537739276886\n",
            "epoch 13 batch number 16    batch loss: 0.1072276383638382\n",
            "epoch 13 batch number 17    batch loss: 0.1129007562994957\n",
            "epoch 13 batch number 18    batch loss: 0.10875552892684937\n",
            "epoch 13 batch number 19    batch loss: 0.1089940145611763\n",
            " Average epoch losses: 0.10936981439590454 \n",
            "epoch  14\n",
            "epoch 14 batch number 0    batch loss: 0.11503823101520538\n",
            "epoch 14 batch number 1    batch loss: 0.1017790287733078\n",
            "epoch 14 batch number 2    batch loss: 0.11245660483837128\n",
            "epoch 14 batch number 3    batch loss: 0.10860708355903625\n",
            "epoch 14 batch number 4    batch loss: 0.11603779345750809\n",
            "epoch 14 batch number 5    batch loss: 0.10319805890321732\n",
            "epoch 14 batch number 6    batch loss: 0.10984674096107483\n",
            "epoch 14 batch number 7    batch loss: 0.10856731981039047\n",
            "epoch 14 batch number 8    batch loss: 0.11247387528419495\n",
            "epoch 14 batch number 9    batch loss: 0.10387052595615387\n",
            "epoch 14 batch number 10    batch loss: 0.09629333019256592\n",
            "epoch 14 batch number 11    batch loss: 0.10494740307331085\n",
            "epoch 14 batch number 12    batch loss: 0.09629461169242859\n",
            "epoch 14 batch number 13    batch loss: 0.1015821322798729\n",
            "epoch 14 batch number 14    batch loss: 0.097378209233284\n",
            "epoch 14 batch number 15    batch loss: 0.11244773119688034\n",
            "epoch 14 batch number 16    batch loss: 0.10597006231546402\n",
            "epoch 14 batch number 17    batch loss: 0.10154217481613159\n",
            "epoch 14 batch number 18    batch loss: 0.11182663589715958\n",
            "epoch 14 batch number 19    batch loss: 0.10015559941530228\n",
            " Average epoch losses: 0.1060156598687172 \n",
            "epoch  15\n",
            "epoch 15 batch number 0    batch loss: 0.11192115396261215\n",
            "epoch 15 batch number 1    batch loss: 0.10303084552288055\n",
            "epoch 15 batch number 2    batch loss: 0.11829659342765808\n",
            "epoch 15 batch number 3    batch loss: 0.10230613499879837\n",
            "epoch 15 batch number 4    batch loss: 0.10371991246938705\n",
            "epoch 15 batch number 5    batch loss: 0.10042358189821243\n",
            "epoch 15 batch number 6    batch loss: 0.11108497530221939\n",
            "epoch 15 batch number 7    batch loss: 0.11703693121671677\n",
            "epoch 15 batch number 8    batch loss: 0.09841976314783096\n",
            "epoch 15 batch number 9    batch loss: 0.1080617755651474\n",
            "epoch 15 batch number 10    batch loss: 0.10030526667833328\n",
            "epoch 15 batch number 11    batch loss: 0.10414119809865952\n",
            "epoch 15 batch number 12    batch loss: 0.10742051154375076\n",
            "epoch 15 batch number 13    batch loss: 0.09816130250692368\n",
            "epoch 15 batch number 14    batch loss: 0.0980706661939621\n",
            "epoch 15 batch number 15    batch loss: 0.103321872651577\n",
            "epoch 15 batch number 16    batch loss: 0.1104845181107521\n",
            "epoch 15 batch number 17    batch loss: 0.10410792380571365\n",
            "epoch 15 batch number 18    batch loss: 0.10034378618001938\n",
            "epoch 15 batch number 19    batch loss: 0.1104244738817215\n",
            " Average epoch losses: 0.10555416345596313 \n",
            "epoch  16\n",
            "epoch 16 batch number 0    batch loss: 0.09728479385375977\n",
            "epoch 16 batch number 1    batch loss: 0.10269200056791306\n",
            "epoch 16 batch number 2    batch loss: 0.09878766536712646\n",
            "epoch 16 batch number 3    batch loss: 0.1045321375131607\n",
            "epoch 16 batch number 4    batch loss: 0.10550661385059357\n",
            "epoch 16 batch number 5    batch loss: 0.10239174962043762\n",
            "epoch 16 batch number 6    batch loss: 0.10143990069627762\n",
            "epoch 16 batch number 7    batch loss: 0.11052767932415009\n",
            "epoch 16 batch number 8    batch loss: 0.09715544432401657\n",
            "epoch 16 batch number 9    batch loss: 0.09713007509708405\n",
            "epoch 16 batch number 10    batch loss: 0.10721839964389801\n",
            "epoch 16 batch number 11    batch loss: 0.10506556928157806\n",
            "epoch 16 batch number 12    batch loss: 0.10181590914726257\n",
            "epoch 16 batch number 13    batch loss: 0.10527729243040085\n",
            "epoch 16 batch number 14    batch loss: 0.11155000329017639\n",
            "epoch 16 batch number 15    batch loss: 0.1066342294216156\n",
            "epoch 16 batch number 16    batch loss: 0.10255256295204163\n",
            "epoch 16 batch number 17    batch loss: 0.09883008152246475\n",
            "epoch 16 batch number 18    batch loss: 0.1028861328959465\n",
            "epoch 16 batch number 19    batch loss: 0.09967917203903198\n",
            " Average epoch losses: 0.10294786840677261 \n",
            "epoch  17\n",
            "epoch 17 batch number 0    batch loss: 0.09422298520803452\n",
            "epoch 17 batch number 1    batch loss: 0.0973084568977356\n",
            "epoch 17 batch number 2    batch loss: 0.09755213558673859\n",
            "epoch 17 batch number 3    batch loss: 0.10130588710308075\n",
            "epoch 17 batch number 4    batch loss: 0.09843230992555618\n",
            "epoch 17 batch number 5    batch loss: 0.0933506190776825\n",
            "epoch 17 batch number 6    batch loss: 0.10359975695610046\n",
            "epoch 17 batch number 7    batch loss: 0.10692057758569717\n",
            "epoch 17 batch number 8    batch loss: 0.08907248824834824\n",
            "epoch 17 batch number 9    batch loss: 0.09225079417228699\n",
            "epoch 17 batch number 10    batch loss: 0.10914649069309235\n",
            "epoch 17 batch number 11    batch loss: 0.10230652987957001\n",
            "epoch 17 batch number 12    batch loss: 0.09953297674655914\n",
            "epoch 17 batch number 13    batch loss: 0.10037077963352203\n",
            "epoch 17 batch number 14    batch loss: 0.10430485010147095\n",
            "epoch 17 batch number 15    batch loss: 0.10351979732513428\n",
            "epoch 17 batch number 16    batch loss: 0.10873030126094818\n",
            "epoch 17 batch number 17    batch loss: 0.09960238635540009\n",
            "epoch 17 batch number 18    batch loss: 0.10111556202173233\n",
            "epoch 17 batch number 19    batch loss: 0.10131213068962097\n",
            " Average epoch losses: 0.10019788891077042 \n",
            "epoch  18\n",
            "epoch 18 batch number 0    batch loss: 0.09937489777803421\n",
            "epoch 18 batch number 1    batch loss: 0.10081867128610611\n",
            "epoch 18 batch number 2    batch loss: 0.09721975028514862\n",
            "epoch 18 batch number 3    batch loss: 0.09552272409200668\n",
            "epoch 18 batch number 4    batch loss: 0.10174067318439484\n",
            "epoch 18 batch number 5    batch loss: 0.1162661612033844\n",
            "epoch 18 batch number 6    batch loss: 0.0969468280673027\n",
            "epoch 18 batch number 7    batch loss: 0.0918462872505188\n",
            "epoch 18 batch number 8    batch loss: 0.1063530370593071\n",
            "epoch 18 batch number 9    batch loss: 0.09769217669963837\n",
            "epoch 18 batch number 10    batch loss: 0.10265449434518814\n",
            "epoch 18 batch number 11    batch loss: 0.10194481164216995\n",
            "epoch 18 batch number 12    batch loss: 0.09592310339212418\n",
            "epoch 18 batch number 13    batch loss: 0.10237356275320053\n",
            "epoch 18 batch number 14    batch loss: 0.10264481604099274\n",
            "epoch 18 batch number 15    batch loss: 0.1005028486251831\n",
            "epoch 18 batch number 16    batch loss: 0.1013629212975502\n",
            "epoch 18 batch number 17    batch loss: 0.09440521895885468\n",
            "epoch 18 batch number 18    batch loss: 0.0951659232378006\n",
            "epoch 18 batch number 19    batch loss: 0.09661756455898285\n",
            " Average epoch losses: 0.09986882656812668 \n",
            "epoch  19\n",
            "epoch 19 batch number 0    batch loss: 0.08897548913955688\n",
            "epoch 19 batch number 1    batch loss: 0.09784277528524399\n",
            "epoch 19 batch number 2    batch loss: 0.09651663154363632\n",
            "epoch 19 batch number 3    batch loss: 0.10263024270534515\n",
            "epoch 19 batch number 4    batch loss: 0.09134239703416824\n",
            "epoch 19 batch number 5    batch loss: 0.09676716476678848\n",
            "epoch 19 batch number 6    batch loss: 0.09834761172533035\n",
            "epoch 19 batch number 7    batch loss: 0.09681067615747452\n",
            "epoch 19 batch number 8    batch loss: 0.09209921211004257\n",
            "epoch 19 batch number 9    batch loss: 0.09247393906116486\n",
            "epoch 19 batch number 10    batch loss: 0.09904788434505463\n",
            "epoch 19 batch number 11    batch loss: 0.09369708597660065\n",
            "epoch 19 batch number 12    batch loss: 0.09826044738292694\n",
            "epoch 19 batch number 13    batch loss: 0.11493462324142456\n",
            "epoch 19 batch number 14    batch loss: 0.10348769277334213\n",
            "epoch 19 batch number 15    batch loss: 0.09224668890237808\n",
            "epoch 19 batch number 16    batch loss: 0.10198359191417694\n",
            "epoch 19 batch number 17    batch loss: 0.09492500871419907\n",
            "epoch 19 batch number 18    batch loss: 0.10305830091238022\n",
            "epoch 19 batch number 19    batch loss: 0.0991797223687172\n",
            " Average epoch losses: 0.09773136675357819 \n",
            "epoch  20\n",
            "epoch 20 batch number 0    batch loss: 0.0932050570845604\n",
            "epoch 20 batch number 1    batch loss: 0.10077354311943054\n",
            "epoch 20 batch number 2    batch loss: 0.09603901952505112\n",
            "epoch 20 batch number 3    batch loss: 0.09901002049446106\n",
            "epoch 20 batch number 4    batch loss: 0.09752926975488663\n",
            "epoch 20 batch number 5    batch loss: 0.09739743918180466\n",
            "epoch 20 batch number 6    batch loss: 0.09780082106590271\n",
            "epoch 20 batch number 7    batch loss: 0.09515740722417831\n",
            "epoch 20 batch number 8    batch loss: 0.10103821009397507\n",
            "epoch 20 batch number 9    batch loss: 0.09756876528263092\n",
            "epoch 20 batch number 10    batch loss: 0.09415730834007263\n",
            "epoch 20 batch number 11    batch loss: 0.0946228951215744\n",
            "epoch 20 batch number 12    batch loss: 0.0901835486292839\n",
            "epoch 20 batch number 13    batch loss: 0.09381236881017685\n",
            "epoch 20 batch number 14    batch loss: 0.09797047823667526\n",
            "epoch 20 batch number 15    batch loss: 0.10121434181928635\n",
            "epoch 20 batch number 16    batch loss: 0.10457275062799454\n",
            "epoch 20 batch number 17    batch loss: 0.09363982081413269\n",
            "epoch 20 batch number 18    batch loss: 0.10009174048900604\n",
            "epoch 20 batch number 19    batch loss: 0.09062018245458603\n",
            " Average epoch losses: 0.09682025015354156 \n",
            "epoch  21\n",
            "epoch 21 batch number 0    batch loss: 0.0966917872428894\n",
            "epoch 21 batch number 1    batch loss: 0.09305869787931442\n",
            "epoch 21 batch number 2    batch loss: 0.09531208127737045\n",
            "epoch 21 batch number 3    batch loss: 0.10332123190164566\n",
            "epoch 21 batch number 4    batch loss: 0.09817910194396973\n",
            "epoch 21 batch number 5    batch loss: 0.09963527321815491\n",
            "epoch 21 batch number 6    batch loss: 0.0935928151011467\n",
            "epoch 21 batch number 7    batch loss: 0.09802651405334473\n",
            "epoch 21 batch number 8    batch loss: 0.09266335517168045\n",
            "epoch 21 batch number 9    batch loss: 0.09208366274833679\n",
            "epoch 21 batch number 10    batch loss: 0.10006914287805557\n",
            "epoch 21 batch number 11    batch loss: 0.08889869600534439\n",
            "epoch 21 batch number 12    batch loss: 0.10604250431060791\n",
            "epoch 21 batch number 13    batch loss: 0.09539942443370819\n",
            "epoch 21 batch number 14    batch loss: 0.09980299323797226\n",
            "epoch 21 batch number 15    batch loss: 0.10766831040382385\n",
            "epoch 21 batch number 16    batch loss: 0.0923665314912796\n",
            "epoch 21 batch number 17    batch loss: 0.10236574709415436\n",
            "epoch 21 batch number 18    batch loss: 0.09778470546007156\n",
            "epoch 21 batch number 19    batch loss: 0.09821348637342453\n",
            " Average epoch losses: 0.09755880385637283 \n",
            "epoch  22\n",
            "epoch 22 batch number 0    batch loss: 0.10234087705612183\n",
            "epoch 22 batch number 1    batch loss: 0.09881623834371567\n",
            "epoch 22 batch number 2    batch loss: 0.09225335717201233\n",
            "epoch 22 batch number 3    batch loss: 0.09639100730419159\n",
            "epoch 22 batch number 4    batch loss: 0.08188245445489883\n",
            "epoch 22 batch number 5    batch loss: 0.09686076641082764\n",
            "epoch 22 batch number 6    batch loss: 0.09265097975730896\n",
            "epoch 22 batch number 7    batch loss: 0.09214279800653458\n",
            "epoch 22 batch number 8    batch loss: 0.09323938935995102\n",
            "epoch 22 batch number 9    batch loss: 0.09502442926168442\n",
            "epoch 22 batch number 10    batch loss: 0.10040811449289322\n",
            "epoch 22 batch number 11    batch loss: 0.09744223952293396\n",
            "epoch 22 batch number 12    batch loss: 0.0900067687034607\n",
            "epoch 22 batch number 13    batch loss: 0.09088855236768723\n",
            "epoch 22 batch number 14    batch loss: 0.10097268968820572\n",
            "epoch 22 batch number 15    batch loss: 0.09309542924165726\n",
            "epoch 22 batch number 16    batch loss: 0.10180456191301346\n",
            "epoch 22 batch number 17    batch loss: 0.10201333463191986\n",
            "epoch 22 batch number 18    batch loss: 0.11290620267391205\n",
            "epoch 22 batch number 19    batch loss: 0.09129960834980011\n",
            " Average epoch losses: 0.09612199664115906 \n",
            "epoch  23\n",
            "epoch 23 batch number 0    batch loss: 0.09606485813856125\n",
            "epoch 23 batch number 1    batch loss: 0.09717937558889389\n",
            "epoch 23 batch number 2    batch loss: 0.09939667582511902\n",
            "epoch 23 batch number 3    batch loss: 0.09945311397314072\n",
            "epoch 23 batch number 4    batch loss: 0.08756023645401001\n",
            "epoch 23 batch number 5    batch loss: 0.08913880586624146\n",
            "epoch 23 batch number 6    batch loss: 0.09410534054040909\n",
            "epoch 23 batch number 7    batch loss: 0.09315313398838043\n",
            "epoch 23 batch number 8    batch loss: 0.0978984460234642\n",
            "epoch 23 batch number 9    batch loss: 0.09389006346464157\n",
            "epoch 23 batch number 10    batch loss: 0.09829644113779068\n",
            "epoch 23 batch number 11    batch loss: 0.09179892390966415\n",
            "epoch 23 batch number 12    batch loss: 0.1002592071890831\n",
            "epoch 23 batch number 13    batch loss: 0.091063492000103\n",
            "epoch 23 batch number 14    batch loss: 0.08998322486877441\n",
            "epoch 23 batch number 15    batch loss: 0.09088127315044403\n",
            "epoch 23 batch number 16    batch loss: 0.08704431354999542\n",
            "epoch 23 batch number 17    batch loss: 0.09859481453895569\n",
            "epoch 23 batch number 18    batch loss: 0.09567300230264664\n",
            "epoch 23 batch number 19    batch loss: 0.10158079862594604\n",
            " Average epoch losses: 0.0946507677435875 \n",
            "epoch  24\n",
            "epoch 24 batch number 0    batch loss: 0.09923960268497467\n",
            "epoch 24 batch number 1    batch loss: 0.10268210619688034\n",
            "epoch 24 batch number 2    batch loss: 0.09784696996212006\n",
            "epoch 24 batch number 3    batch loss: 0.08199690282344818\n",
            "epoch 24 batch number 4    batch loss: 0.09166023135185242\n",
            "epoch 24 batch number 5    batch loss: 0.09409838169813156\n",
            "epoch 24 batch number 6    batch loss: 0.09139643609523773\n",
            "epoch 24 batch number 7    batch loss: 0.0899820625782013\n",
            "epoch 24 batch number 8    batch loss: 0.09010733664035797\n",
            "epoch 24 batch number 9    batch loss: 0.09238816052675247\n",
            "epoch 24 batch number 10    batch loss: 0.08784478902816772\n",
            "epoch 24 batch number 11    batch loss: 0.09667360037565231\n",
            "epoch 24 batch number 12    batch loss: 0.09985661506652832\n",
            "epoch 24 batch number 13    batch loss: 0.10473301261663437\n",
            "epoch 24 batch number 14    batch loss: 0.09101247042417526\n",
            "epoch 24 batch number 15    batch loss: 0.08922126144170761\n",
            "epoch 24 batch number 16    batch loss: 0.0880628302693367\n",
            "epoch 24 batch number 17    batch loss: 0.08810226619243622\n",
            "epoch 24 batch number 18    batch loss: 0.09042114019393921\n",
            "epoch 24 batch number 19    batch loss: 0.09166242182254791\n",
            " Average epoch losses: 0.09294942766427994 \n",
            "epoch  25\n",
            "epoch 25 batch number 0    batch loss: 0.09304750710725784\n",
            "epoch 25 batch number 1    batch loss: 0.09697048366069794\n",
            "epoch 25 batch number 2    batch loss: 0.09569469094276428\n",
            "epoch 25 batch number 3    batch loss: 0.09505901485681534\n",
            "epoch 25 batch number 4    batch loss: 0.09347183257341385\n",
            "epoch 25 batch number 5    batch loss: 0.09371882677078247\n",
            "epoch 25 batch number 6    batch loss: 0.09189209342002869\n",
            "epoch 25 batch number 7    batch loss: 0.08600913733243942\n",
            "epoch 25 batch number 8    batch loss: 0.09053771197795868\n",
            "epoch 25 batch number 9    batch loss: 0.09168354421854019\n",
            "epoch 25 batch number 10    batch loss: 0.0867878720164299\n",
            "epoch 25 batch number 11    batch loss: 0.08832070231437683\n",
            "epoch 25 batch number 12    batch loss: 0.08717052638530731\n",
            "epoch 25 batch number 13    batch loss: 0.0878424197435379\n",
            "epoch 25 batch number 14    batch loss: 0.08800555020570755\n",
            "epoch 25 batch number 15    batch loss: 0.09347488731145859\n",
            "epoch 25 batch number 16    batch loss: 0.0928698256611824\n",
            "epoch 25 batch number 17    batch loss: 0.09704990684986115\n",
            "epoch 25 batch number 18    batch loss: 0.08792408555746078\n",
            "epoch 25 batch number 19    batch loss: 0.08866805583238602\n",
            " Average epoch losses: 0.09130994230508804 \n",
            "epoch  26\n",
            "epoch 26 batch number 0    batch loss: 0.08971597254276276\n",
            "epoch 26 batch number 1    batch loss: 0.08577294647693634\n",
            "epoch 26 batch number 2    batch loss: 0.08753477782011032\n",
            "epoch 26 batch number 3    batch loss: 0.09561663866043091\n",
            "epoch 26 batch number 4    batch loss: 0.08913273364305496\n",
            "epoch 26 batch number 5    batch loss: 0.09335427731275558\n",
            "epoch 26 batch number 6    batch loss: 0.09714410454034805\n",
            "epoch 26 batch number 7    batch loss: 0.08414862304925919\n",
            "epoch 26 batch number 8    batch loss: 0.09023716300725937\n",
            "epoch 26 batch number 9    batch loss: 0.08772292733192444\n",
            "epoch 26 batch number 10    batch loss: 0.08530008792877197\n",
            "epoch 26 batch number 12    batch loss: 0.09102332592010498\n",
            "epoch 26 batch number 13    batch loss: 0.08291718363761902\n",
            "epoch 26 batch number 14    batch loss: 0.08617421984672546\n",
            "epoch 26 batch number 15    batch loss: 0.0914681926369667\n",
            "epoch 26 batch number 16    batch loss: 0.0928443893790245\n",
            "epoch 26 batch number 17    batch loss: 0.09391454607248306\n",
            "epoch 26 batch number 18    batch loss: 0.08164023607969284\n",
            "epoch 26 batch number 19    batch loss: 0.08906733244657516\n",
            " Average epoch losses: 0.08925817906856537 \n",
            "epoch  27\n",
            "epoch 27 batch number 0    batch loss: 0.0866839587688446\n",
            "epoch 27 batch number 1    batch loss: 0.09143851697444916\n",
            "epoch 27 batch number 2    batch loss: 0.09345368295907974\n",
            "epoch 27 batch number 3    batch loss: 0.0901581421494484\n",
            "epoch 27 batch number 4    batch loss: 0.08972646296024323\n",
            "epoch 27 batch number 5    batch loss: 0.0903855487704277\n",
            "epoch 27 batch number 6    batch loss: 0.08906353265047073\n",
            "epoch 27 batch number 7    batch loss: 0.08188038319349289\n",
            "epoch 27 batch number 8    batch loss: 0.07771957665681839\n",
            "epoch 27 batch number 9    batch loss: 0.08379793167114258\n",
            "epoch 27 batch number 10    batch loss: 0.08772825449705124\n",
            "epoch 27 batch number 11    batch loss: 0.08605495095252991\n",
            "epoch 27 batch number 12    batch loss: 0.09463391453027725\n",
            "epoch 27 batch number 13    batch loss: 0.0909898430109024\n",
            "epoch 27 batch number 14    batch loss: 0.09310246258974075\n",
            "epoch 27 batch number 15    batch loss: 0.0836263969540596\n",
            "epoch 27 batch number 16    batch loss: 0.087842658162117\n",
            "epoch 27 batch number 17    batch loss: 0.08539684116840363\n",
            "epoch 27 batch number 18    batch loss: 0.0850391611456871\n",
            "epoch 27 batch number 19    batch loss: 0.09505822509527206\n",
            " Average epoch losses: 0.0881890207529068 \n",
            "epoch  28\n",
            "epoch 28 batch number 0    batch loss: 0.08954933285713196\n",
            "epoch 28 batch number 1    batch loss: 0.09807704389095306\n",
            "epoch 28 batch number 2    batch loss: 0.08557020872831345\n",
            "epoch 28 batch number 3    batch loss: 0.08654768019914627\n",
            "epoch 28 batch number 4    batch loss: 0.09150436520576477\n",
            "epoch 28 batch number 5    batch loss: 0.08217243105173111\n",
            "epoch 28 batch number 6    batch loss: 0.08885889500379562\n",
            "epoch 28 batch number 7    batch loss: 0.08318108320236206\n",
            "epoch 28 batch number 8    batch loss: 0.0874372273683548\n",
            "epoch 28 batch number 9    batch loss: 0.08840737491846085\n",
            "epoch 28 batch number 10    batch loss: 0.08630602806806564\n",
            "epoch 28 batch number 11    batch loss: 0.09127568453550339\n",
            "epoch 28 batch number 12    batch loss: 0.08521603047847748\n",
            "epoch 28 batch number 13    batch loss: 0.08906596899032593\n",
            "epoch 28 batch number 14    batch loss: 0.08092891424894333\n",
            "epoch 28 batch number 15    batch loss: 0.08900221437215805\n",
            "epoch 28 batch number 16    batch loss: 0.08107684552669525\n",
            "epoch 28 batch number 17    batch loss: 0.09838312119245529\n",
            "epoch 28 batch number 18    batch loss: 0.08840551227331161\n",
            "epoch 28 batch number 19    batch loss: 0.08655058592557907\n",
            " Average epoch losses: 0.08787582814693451 \n",
            "epoch  29\n",
            "epoch 29 batch number 0    batch loss: 0.08398619294166565\n",
            "epoch 29 batch number 1    batch loss: 0.08881732076406479\n",
            "epoch 29 batch number 2    batch loss: 0.080534428358078\n",
            "epoch 29 batch number 3    batch loss: 0.08456514775753021\n",
            "epoch 29 batch number 4    batch loss: 0.08172639459371567\n",
            "epoch 29 batch number 5    batch loss: 0.09352446347475052\n",
            "epoch 29 batch number 6    batch loss: 0.09603770077228546\n",
            "epoch 29 batch number 7    batch loss: 0.09176028519868851\n",
            "epoch 29 batch number 8    batch loss: 0.08640024811029434\n",
            "epoch 29 batch number 9    batch loss: 0.08230889588594437\n",
            "epoch 29 batch number 10    batch loss: 0.08666050434112549\n",
            "epoch 29 batch number 11    batch loss: 0.07577504217624664\n",
            "epoch 29 batch number 12    batch loss: 0.08756176382303238\n",
            "epoch 29 batch number 13    batch loss: 0.09397934377193451\n",
            "epoch 29 batch number 14    batch loss: 0.08404543995857239\n",
            "epoch 29 batch number 15    batch loss: 0.08769670128822327\n",
            "epoch 29 batch number 16    batch loss: 0.08804845809936523\n",
            "epoch 29 batch number 17    batch loss: 0.08689682185649872\n",
            "epoch 29 batch number 18    batch loss: 0.08441869914531708\n",
            "epoch 29 batch number 19    batch loss: 0.08481837809085846\n",
            " Average epoch losses: 0.0864780992269516 \n",
            "epoch  30\n",
            "epoch 30 batch number 0    batch loss: 0.07807820290327072\n",
            "epoch 30 batch number 1    batch loss: 0.0796222910284996\n",
            "epoch 30 batch number 2    batch loss: 0.07913924008607864\n",
            "epoch 30 batch number 3    batch loss: 0.0944175124168396\n",
            "epoch 30 batch number 4    batch loss: 0.0861179530620575\n",
            "epoch 30 batch number 5    batch loss: 0.09054716676473618\n",
            "epoch 30 batch number 6    batch loss: 0.08210134506225586\n",
            "epoch 30 batch number 7    batch loss: 0.08821682631969452\n",
            "epoch 30 batch number 8    batch loss: 0.0907742902636528\n",
            "epoch 30 batch number 9    batch loss: 0.08806568384170532\n",
            "epoch 30 batch number 10    batch loss: 0.08274707198143005\n",
            "epoch 30 batch number 11    batch loss: 0.07827288657426834\n",
            "epoch 30 batch number 12    batch loss: 0.08861876279115677\n",
            "epoch 30 batch number 13    batch loss: 0.0789123922586441\n",
            "epoch 30 batch number 14    batch loss: 0.08063779771327972\n",
            "epoch 30 batch number 15    batch loss: 0.08464012295007706\n",
            "epoch 30 batch number 16    batch loss: 0.08004698157310486\n",
            "epoch 30 batch number 17    batch loss: 0.09477286785840988\n",
            "epoch 30 batch number 18    batch loss: 0.09487573057413101\n",
            "epoch 30 batch number 19    batch loss: 0.08779123425483704\n",
            " Average epoch losses: 0.08541981130838394 \n",
            "epoch  31\n",
            "epoch 31 batch number 0    batch loss: 0.07987312227487564\n",
            "epoch 31 batch number 1    batch loss: 0.08410494774580002\n",
            "epoch 31 batch number 2    batch loss: 0.09329864382743835\n",
            "epoch 31 batch number 3    batch loss: 0.08402019739151001\n",
            "epoch 31 batch number 4    batch loss: 0.08281593769788742\n",
            "epoch 31 batch number 5    batch loss: 0.0825522169470787\n",
            "epoch 31 batch number 6    batch loss: 0.0856192335486412\n",
            "epoch 31 batch number 7    batch loss: 0.07765054702758789\n",
            "epoch 31 batch number 8    batch loss: 0.08636815845966339\n",
            "epoch 31 batch number 9    batch loss: 0.08077214658260345\n",
            "epoch 31 batch number 10    batch loss: 0.0851690024137497\n",
            "epoch 31 batch number 11    batch loss: 0.08384504914283752\n",
            "epoch 31 batch number 12    batch loss: 0.07745103538036346\n",
            "epoch 31 batch number 13    batch loss: 0.07576356083154678\n",
            "epoch 31 batch number 14    batch loss: 0.08206139504909515\n",
            "epoch 31 batch number 15    batch loss: 0.0878501832485199\n",
            "epoch 31 batch number 16    batch loss: 0.10236894339323044\n",
            "epoch 31 batch number 17    batch loss: 0.08188419789075851\n",
            "epoch 31 batch number 18    batch loss: 0.09101758152246475\n",
            "epoch 31 batch number 19    batch loss: 0.08258361369371414\n",
            " Average epoch losses: 0.0843534842133522 \n",
            "epoch  32\n",
            "epoch 32 batch number 0    batch loss: 0.08261407911777496\n",
            "epoch 32 batch number 1    batch loss: 0.08358380943536758\n",
            "epoch 32 batch number 2    batch loss: 0.07601451128721237\n",
            "epoch 32 batch number 3    batch loss: 0.08277618139982224\n",
            "epoch 32 batch number 4    batch loss: 0.07798095792531967\n",
            "epoch 32 batch number 5    batch loss: 0.08922331780195236\n",
            "epoch 32 batch number 6    batch loss: 0.077895887196064\n",
            "epoch 32 batch number 7    batch loss: 0.08449217677116394\n",
            "epoch 32 batch number 8    batch loss: 0.07174014300107956\n",
            "epoch 32 batch number 9    batch loss: 0.08594999462366104\n",
            "epoch 32 batch number 10    batch loss: 0.07810264825820923\n",
            "epoch 32 batch number 11    batch loss: 0.0891306921839714\n",
            "epoch 32 batch number 12    batch loss: 0.07671450823545456\n",
            "epoch 32 batch number 13    batch loss: 0.08899538964033127\n",
            "epoch 32 batch number 14    batch loss: 0.07800018787384033\n",
            "epoch 32 batch number 15    batch loss: 0.08512129634618759\n",
            "epoch 32 batch number 16    batch loss: 0.08921638876199722\n",
            "epoch 32 batch number 17    batch loss: 0.08188264071941376\n",
            "epoch 32 batch number 18    batch loss: 0.08582466095685959\n",
            "epoch 32 batch number 19    batch loss: 0.08205308020114899\n",
            " Average epoch losses: 0.08236561715602875 \n",
            "epoch  33\n",
            "epoch 33 batch number 0    batch loss: 0.08163785934448242\n",
            "epoch 33 batch number 1    batch loss: 0.07842258363962173\n",
            "epoch 33 batch number 2    batch loss: 0.07862120866775513\n",
            "epoch 33 batch number 3    batch loss: 0.08366541564464569\n",
            "epoch 33 batch number 4    batch loss: 0.08385585248470306\n",
            "epoch 33 batch number 5    batch loss: 0.08060206472873688\n",
            "epoch 33 batch number 6    batch loss: 0.0779893547296524\n",
            "epoch 33 batch number 7    batch loss: 0.0981459766626358\n",
            "epoch 33 batch number 8    batch loss: 0.08640310168266296\n",
            "epoch 33 batch number 9    batch loss: 0.08149930089712143\n",
            "epoch 33 batch number 10    batch loss: 0.08898529410362244\n",
            "epoch 33 batch number 11    batch loss: 0.0785423144698143\n",
            "epoch 33 batch number 12    batch loss: 0.07593311369419098\n",
            "epoch 33 batch number 13    batch loss: 0.08884792029857635\n",
            "epoch 33 batch number 14    batch loss: 0.0730949193239212\n",
            "epoch 33 batch number 15    batch loss: 0.08634670823812485\n",
            "epoch 33 batch number 16    batch loss: 0.0803029015660286\n",
            "epoch 33 batch number 17    batch loss: 0.08403035998344421\n",
            "epoch 33 batch number 18    batch loss: 0.08753697574138641\n",
            "epoch 33 batch number 19    batch loss: 0.07551752775907516\n",
            " Average epoch losses: 0.08249904215335846 \n",
            "epoch  34\n",
            "epoch 34 batch number 0    batch loss: 0.07561762630939484\n",
            "epoch 34 batch number 1    batch loss: 0.08130238205194473\n",
            "epoch 34 batch number 2    batch loss: 0.07645676285028458\n",
            "epoch 34 batch number 3    batch loss: 0.0780559703707695\n",
            "epoch 34 batch number 4    batch loss: 0.0749846026301384\n",
            "epoch 34 batch number 5    batch loss: 0.07605133205652237\n",
            "epoch 34 batch number 6    batch loss: 0.08411425352096558\n",
            "epoch 34 batch number 7    batch loss: 0.08715585619211197\n",
            "epoch 34 batch number 8    batch loss: 0.08452648669481277\n",
            "epoch 34 batch number 9    batch loss: 0.0926307961344719\n",
            "epoch 34 batch number 10    batch loss: 0.0799473375082016\n",
            "epoch 34 batch number 11    batch loss: 0.08138177543878555\n",
            "epoch 34 batch number 12    batch loss: 0.09131168574094772\n",
            "epoch 34 batch number 13    batch loss: 0.08562818914651871\n",
            "epoch 34 batch number 14    batch loss: 0.09345847368240356\n",
            "epoch 34 batch number 15    batch loss: 0.08458367735147476\n",
            "epoch 34 batch number 16    batch loss: 0.08120214194059372\n",
            "epoch 34 batch number 17    batch loss: 0.0803319662809372\n",
            "epoch 34 batch number 18    batch loss: 0.0823369026184082\n",
            "epoch 34 batch number 19    batch loss: 0.07299042493104935\n",
            " Average epoch losses: 0.08220343291759491 \n",
            "epoch  35\n",
            "epoch 35 batch number 0    batch loss: 0.07347238063812256\n",
            "epoch 35 batch number 1    batch loss: 0.08683149516582489\n",
            "epoch 35 batch number 2    batch loss: 0.07154345512390137\n",
            "epoch 35 batch number 3    batch loss: 0.07820019125938416\n",
            "epoch 35 batch number 4    batch loss: 0.08083996176719666\n",
            "epoch 35 batch number 5    batch loss: 0.07754630595445633\n",
            "epoch 35 batch number 6    batch loss: 0.07555098086595535\n",
            "epoch 35 batch number 7    batch loss: 0.07536619156599045\n",
            "epoch 35 batch number 8    batch loss: 0.08436916768550873\n",
            "epoch 35 batch number 9    batch loss: 0.07443112879991531\n",
            "epoch 35 batch number 10    batch loss: 0.076360784471035\n",
            "epoch 35 batch number 11    batch loss: 0.08740653097629547\n",
            "epoch 35 batch number 12    batch loss: 0.07275578379631042\n",
            "epoch 35 batch number 13    batch loss: 0.07555565983057022\n",
            "epoch 35 batch number 14    batch loss: 0.07728809118270874\n",
            "epoch 35 batch number 15    batch loss: 0.07476817071437836\n",
            "epoch 35 batch number 16    batch loss: 0.08007118105888367\n",
            "epoch 35 batch number 17    batch loss: 0.07781441509723663\n",
            "epoch 35 batch number 18    batch loss: 0.07712050527334213\n",
            "epoch 35 batch number 19    batch loss: 0.08285091817378998\n",
            " Average epoch losses: 0.07800716906785965 \n",
            "epoch  36\n",
            "epoch 36 batch number 0    batch loss: 0.07365115731954575\n",
            "epoch 36 batch number 1    batch loss: 0.08627872169017792\n",
            "epoch 36 batch number 2    batch loss: 0.08793314546346664\n",
            "epoch 36 batch number 3    batch loss: 0.0752476379275322\n",
            "epoch 36 batch number 4    batch loss: 0.07424836605787277\n",
            "epoch 36 batch number 5    batch loss: 0.08407896012067795\n",
            "epoch 36 batch number 6    batch loss: 0.06868696212768555\n",
            "epoch 36 batch number 7    batch loss: 0.07239988446235657\n",
            "epoch 36 batch number 8    batch loss: 0.07166653126478195\n",
            "epoch 36 batch number 9    batch loss: 0.07767023891210556\n",
            "epoch 36 batch number 10    batch loss: 0.07407457381486893\n",
            "epoch 36 batch number 11    batch loss: 0.08031636476516724\n",
            "epoch 36 batch number 12    batch loss: 0.07580931484699249\n",
            "epoch 36 batch number 13    batch loss: 0.08191469311714172\n",
            "epoch 36 batch number 14    batch loss: 0.07696854323148727\n",
            "epoch 36 batch number 15    batch loss: 0.07213791459798813\n",
            "epoch 36 batch number 16    batch loss: 0.07631055265665054\n",
            "epoch 36 batch number 17    batch loss: 0.06997012346982956\n",
            "epoch 36 batch number 18    batch loss: 0.07819151878356934\n",
            "epoch 36 batch number 19    batch loss: 0.07526840269565582\n",
            " Average epoch losses: 0.07664117962121964 \n",
            "epoch  37\n",
            "epoch 37 batch number 0    batch loss: 0.0869724377989769\n",
            "epoch 37 batch number 1    batch loss: 0.06844689697027206\n",
            "epoch 37 batch number 2    batch loss: 0.07602468132972717\n",
            "epoch 37 batch number 3    batch loss: 0.07265061885118484\n",
            "epoch 37 batch number 4    batch loss: 0.07778625935316086\n",
            "epoch 37 batch number 5    batch loss: 0.07612453401088715\n",
            "epoch 37 batch number 6    batch loss: 0.07406509667634964\n",
            "epoch 37 batch number 7    batch loss: 0.07748275995254517\n",
            "epoch 37 batch number 8    batch loss: 0.07116769999265671\n",
            "epoch 37 batch number 9    batch loss: 0.07679372280836105\n",
            "epoch 37 batch number 10    batch loss: 0.06812215596437454\n",
            "epoch 37 batch number 11    batch loss: 0.07150328159332275\n",
            "epoch 37 batch number 12    batch loss: 0.07320771366357803\n",
            "epoch 37 batch number 13    batch loss: 0.07779613882303238\n",
            "epoch 37 batch number 14    batch loss: 0.06655129790306091\n",
            "epoch 37 batch number 15    batch loss: 0.07468342036008835\n",
            "epoch 37 batch number 16    batch loss: 0.07720495760440826\n",
            "epoch 37 batch number 17    batch loss: 0.07146009057760239\n",
            "epoch 37 batch number 18    batch loss: 0.07103079557418823\n",
            "epoch 37 batch number 19    batch loss: 0.07756427675485611\n",
            " Average epoch losses: 0.07433193922042847 \n",
            "epoch  38\n",
            "epoch 38 batch number 0    batch loss: 0.06881991028785706\n",
            "epoch 38 batch number 1    batch loss: 0.07135385274887085\n",
            "epoch 38 batch number 2    batch loss: 0.07367843389511108\n",
            "epoch 38 batch number 3    batch loss: 0.06958877295255661\n",
            "epoch 38 batch number 4    batch loss: 0.07337430119514465\n",
            "epoch 38 batch number 5    batch loss: 0.074777752161026\n",
            "epoch 38 batch number 6    batch loss: 0.0732419416308403\n",
            "epoch 38 batch number 7    batch loss: 0.07252311706542969\n",
            "epoch 38 batch number 8    batch loss: 0.06934686750173569\n",
            "epoch 38 batch number 9    batch loss: 0.07533390074968338\n",
            "epoch 38 batch number 10    batch loss: 0.06933300942182541\n",
            "epoch 38 batch number 11    batch loss: 0.0769062489271164\n",
            "epoch 38 batch number 12    batch loss: 0.07695741951465607\n",
            "epoch 38 batch number 13    batch loss: 0.0719427764415741\n",
            "epoch 38 batch number 14    batch loss: 0.06946463137865067\n",
            "epoch 38 batch number 15    batch loss: 0.07464843988418579\n",
            "epoch 38 batch number 16    batch loss: 0.07320762425661087\n",
            "epoch 38 batch number 17    batch loss: 0.08454595506191254\n",
            "epoch 38 batch number 18    batch loss: 0.07245451956987381\n",
            "epoch 38 batch number 19    batch loss: 0.07383248209953308\n",
            " Average epoch losses: 0.07326660305261612 \n",
            "epoch  39\n",
            "epoch 39 batch number 0    batch loss: 0.06904579699039459\n",
            "epoch 39 batch number 1    batch loss: 0.06865707039833069\n",
            "epoch 39 batch number 2    batch loss: 0.07124735414981842\n",
            "epoch 39 batch number 3    batch loss: 0.07028139382600784\n",
            "epoch 39 batch number 4    batch loss: 0.07099395245313644\n",
            "epoch 39 batch number 5    batch loss: 0.07984334230422974\n",
            "epoch 39 batch number 6    batch loss: 0.07289323955774307\n",
            "epoch 39 batch number 7    batch loss: 0.07674002647399902\n",
            "epoch 39 batch number 8    batch loss: 0.07579638063907623\n",
            "epoch 39 batch number 9    batch loss: 0.0731486827135086\n",
            "epoch 39 batch number 10    batch loss: 0.07228127866983414\n",
            "epoch 39 batch number 11    batch loss: 0.06785131245851517\n",
            "epoch 39 batch number 12    batch loss: 0.07118627429008484\n",
            "epoch 39 batch number 13    batch loss: 0.07221950590610504\n",
            "epoch 39 batch number 14    batch loss: 0.07151704281568527\n",
            "epoch 39 batch number 15    batch loss: 0.07677364349365234\n",
            "epoch 39 batch number 16    batch loss: 0.0717054158449173\n",
            "epoch 39 batch number 17    batch loss: 0.0700669139623642\n",
            "epoch 39 batch number 18    batch loss: 0.06879442930221558\n",
            "epoch 39 batch number 19    batch loss: 0.07360102981328964\n",
            " Average epoch losses: 0.0722322016954422 \n",
            "epoch  40\n",
            "epoch 40 batch number 0    batch loss: 0.060445405542850494\n",
            "epoch 40 batch number 1    batch loss: 0.06904012709856033\n",
            "epoch 40 batch number 2    batch loss: 0.06696361303329468\n",
            "epoch 40 batch number 3    batch loss: 0.07460440695285797\n",
            "epoch 40 batch number 4    batch loss: 0.06686359643936157\n",
            "epoch 40 batch number 5    batch loss: 0.0678795874118805\n",
            "epoch 40 batch number 6    batch loss: 0.07101733237504959\n",
            "epoch 40 batch number 7    batch loss: 0.07043507695198059\n",
            "epoch 40 batch number 8    batch loss: 0.07538773864507675\n",
            "epoch 40 batch number 9    batch loss: 0.07097756862640381\n",
            "epoch 40 batch number 10    batch loss: 0.0725555345416069\n",
            "epoch 40 batch number 11    batch loss: 0.06783158332109451\n",
            "epoch 40 batch number 12    batch loss: 0.06952464580535889\n",
            "epoch 40 batch number 13    batch loss: 0.07386233657598495\n",
            "epoch 40 batch number 14    batch loss: 0.06951487064361572\n",
            "epoch 40 batch number 15    batch loss: 0.06983526796102524\n",
            "epoch 40 batch number 16    batch loss: 0.07157750427722931\n",
            "epoch 40 batch number 17    batch loss: 0.07683587074279785\n",
            "epoch 40 batch number 18    batch loss: 0.06782811880111694\n",
            "epoch 40 batch number 19    batch loss: 0.07107838988304138\n",
            " Average epoch losses: 0.07020293921232224 \n",
            "epoch  41\n",
            "epoch 41 batch number 0    batch loss: 0.07248600572347641\n",
            "epoch 41 batch number 1    batch loss: 0.06416075676679611\n",
            "epoch 41 batch number 2    batch loss: 0.06330782920122147\n",
            "epoch 41 batch number 3    batch loss: 0.060991887003183365\n",
            "epoch 41 batch number 4    batch loss: 0.07335839420557022\n",
            "epoch 41 batch number 5    batch loss: 0.06712697446346283\n",
            "epoch 41 batch number 6    batch loss: 0.06751606613397598\n",
            "epoch 41 batch number 7    batch loss: 0.06611651927232742\n",
            "epoch 41 batch number 8    batch loss: 0.06688795983791351\n",
            "epoch 41 batch number 9    batch loss: 0.06450769305229187\n",
            "epoch 41 batch number 10    batch loss: 0.07223306596279144\n",
            "epoch 41 batch number 11    batch loss: 0.06738320738077164\n",
            "epoch 41 batch number 12    batch loss: 0.06038696691393852\n",
            "epoch 41 batch number 13    batch loss: 0.07187935709953308\n",
            "epoch 41 batch number 14    batch loss: 0.06978145241737366\n",
            "epoch 41 batch number 15    batch loss: 0.0746939554810524\n",
            "epoch 41 batch number 16    batch loss: 0.06978505849838257\n",
            "epoch 41 batch number 17    batch loss: 0.06209634989500046\n",
            "epoch 41 batch number 18    batch loss: 0.06529811769723892\n",
            "epoch 41 batch number 19    batch loss: 0.058696962893009186\n",
            " Average epoch losses: 0.0669347271323204 \n",
            "epoch  42\n",
            "epoch 42 batch number 0    batch loss: 0.06861836463212967\n",
            "epoch 42 batch number 1    batch loss: 0.07663369923830032\n",
            "epoch 42 batch number 2    batch loss: 0.06832255423069\n",
            "epoch 42 batch number 3    batch loss: 0.060920536518096924\n",
            "epoch 42 batch number 4    batch loss: 0.06856365501880646\n",
            "epoch 42 batch number 5    batch loss: 0.06525049358606339\n",
            "epoch 42 batch number 6    batch loss: 0.06839794665575027\n",
            "epoch 42 batch number 7    batch loss: 0.06269194185733795\n",
            "epoch 42 batch number 8    batch loss: 0.05771463364362717\n",
            "epoch 42 batch number 9    batch loss: 0.06376048177480698\n",
            "epoch 42 batch number 10    batch loss: 0.06243961676955223\n",
            "epoch 42 batch number 11    batch loss: 0.0678456574678421\n",
            "epoch 42 batch number 12    batch loss: 0.06263057887554169\n",
            "epoch 42 batch number 13    batch loss: 0.06319394707679749\n",
            "epoch 42 batch number 14    batch loss: 0.05839342996478081\n",
            "epoch 42 batch number 15    batch loss: 0.0670933797955513\n",
            "epoch 42 batch number 16    batch loss: 0.06846992671489716\n",
            "epoch 42 batch number 17    batch loss: 0.061965834349393845\n",
            "epoch 42 batch number 18    batch loss: 0.06309930980205536\n",
            "epoch 42 batch number 19    batch loss: 0.06445670127868652\n",
            " Average epoch losses: 0.06502313166856766 \n",
            "epoch  43\n",
            "epoch 43 batch number 0    batch loss: 0.06073368713259697\n",
            "epoch 43 batch number 1    batch loss: 0.0625310018658638\n",
            "epoch 43 batch number 2    batch loss: 0.0609724335372448\n",
            "epoch 43 batch number 3    batch loss: 0.06267188489437103\n",
            "epoch 43 batch number 4    batch loss: 0.06496428698301315\n",
            "epoch 43 batch number 5    batch loss: 0.06373653560876846\n",
            "epoch 43 batch number 6    batch loss: 0.06844211369752884\n",
            "epoch 43 batch number 7    batch loss: 0.06778273731470108\n",
            "epoch 43 batch number 8    batch loss: 0.06375662982463837\n",
            "epoch 43 batch number 9    batch loss: 0.07031157612800598\n",
            "epoch 43 batch number 10    batch loss: 0.0646887794137001\n",
            "epoch 43 batch number 11    batch loss: 0.06572675704956055\n",
            "epoch 43 batch number 12    batch loss: 0.06229372322559357\n",
            "epoch 43 batch number 13    batch loss: 0.061314623802900314\n",
            "epoch 43 batch number 14    batch loss: 0.06696701049804688\n",
            "epoch 43 batch number 15    batch loss: 0.06150653213262558\n",
            "epoch 43 batch number 16    batch loss: 0.07362985610961914\n",
            "epoch 43 batch number 17    batch loss: 0.06702614575624466\n",
            "epoch 43 batch number 18    batch loss: 0.06764649599790573\n",
            "epoch 43 batch number 19    batch loss: 0.06993736326694489\n",
            " Average epoch losses: 0.06533200293779373 \n",
            "epoch  44\n",
            "epoch 44 batch number 0    batch loss: 0.061674248427152634\n",
            "epoch 44 batch number 1    batch loss: 0.06431230157613754\n",
            "epoch 44 batch number 2    batch loss: 0.0650310143828392\n",
            "epoch 44 batch number 3    batch loss: 0.06520487368106842\n",
            "epoch 44 batch number 4    batch loss: 0.06452219188213348\n",
            "epoch 44 batch number 5    batch loss: 0.06627701222896576\n",
            "epoch 44 batch number 6    batch loss: 0.06352298706769943\n",
            "epoch 44 batch number 7    batch loss: 0.06852748245000839\n",
            "epoch 44 batch number 8    batch loss: 0.06116968020796776\n",
            "epoch 44 batch number 9    batch loss: 0.06454434990882874\n",
            "epoch 44 batch number 10    batch loss: 0.06102101132273674\n",
            "epoch 44 batch number 11    batch loss: 0.06291323900222778\n",
            "epoch 44 batch number 12    batch loss: 0.06386906653642654\n",
            "epoch 44 batch number 13    batch loss: 0.06538465619087219\n",
            "epoch 44 batch number 14    batch loss: 0.059230249375104904\n",
            "epoch 44 batch number 15    batch loss: 0.0631878599524498\n",
            "epoch 44 batch number 16    batch loss: 0.07549796253442764\n",
            "epoch 44 batch number 17    batch loss: 0.06900251656770706\n",
            "epoch 44 batch number 18    batch loss: 0.06741473078727722\n",
            "epoch 44 batch number 19    batch loss: 0.06580156087875366\n",
            " Average epoch losses: 0.06490544974803925 \n",
            "epoch  45\n",
            "epoch 45 batch number 0    batch loss: 0.0685507282614708\n",
            "epoch 45 batch number 1    batch loss: 0.06050192564725876\n",
            "epoch 45 batch number 2    batch loss: 0.06673669815063477\n",
            "epoch 45 batch number 3    batch loss: 0.06244312599301338\n",
            "epoch 45 batch number 4    batch loss: 0.05904081091284752\n",
            "epoch 45 batch number 5    batch loss: 0.06613874435424805\n",
            "epoch 45 batch number 6    batch loss: 0.054492928087711334\n",
            "epoch 45 batch number 7    batch loss: 0.06428419798612595\n",
            "epoch 45 batch number 8    batch loss: 0.05524510145187378\n",
            "epoch 45 batch number 9    batch loss: 0.06062084063887596\n",
            "epoch 45 batch number 10    batch loss: 0.06257744878530502\n",
            "epoch 45 batch number 11    batch loss: 0.064091756939888\n",
            "epoch 45 batch number 12    batch loss: 0.06721504777669907\n",
            "epoch 45 batch number 13    batch loss: 0.059604462236166\n",
            "epoch 45 batch number 14    batch loss: 0.059399720281362534\n",
            "epoch 45 batch number 15    batch loss: 0.058840978890657425\n",
            "epoch 45 batch number 16    batch loss: 0.06788302212953568\n",
            "epoch 45 batch number 17    batch loss: 0.06109413504600525\n",
            "epoch 45 batch number 18    batch loss: 0.06128344312310219\n",
            "epoch 45 batch number 19    batch loss: 0.0579250305891037\n",
            " Average epoch losses: 0.06189850717782974 \n",
            "epoch  46\n",
            "epoch 46 batch number 0    batch loss: 0.05744985491037369\n",
            "epoch 46 batch number 1    batch loss: 0.0634555071592331\n",
            "epoch 46 batch number 2    batch loss: 0.06408364325761795\n",
            "epoch 46 batch number 3    batch loss: 0.05947843939065933\n",
            "epoch 46 batch number 4    batch loss: 0.05706728622317314\n",
            "epoch 46 batch number 5    batch loss: 0.06726181507110596\n",
            "epoch 46 batch number 6    batch loss: 0.055455345660448074\n",
            "epoch 46 batch number 7    batch loss: 0.06291772425174713\n",
            "epoch 46 batch number 8    batch loss: 0.06527270376682281\n",
            "epoch 46 batch number 9    batch loss: 0.06058385968208313\n",
            "epoch 46 batch number 10    batch loss: 0.058212511241436005\n",
            "epoch 46 batch number 11    batch loss: 0.0691298246383667\n",
            "epoch 46 batch number 12    batch loss: 0.06228332594037056\n",
            "epoch 46 batch number 13    batch loss: 0.062392592430114746\n",
            "epoch 46 batch number 14    batch loss: 0.057279929518699646\n",
            "epoch 46 batch number 15    batch loss: 0.06207858398556709\n",
            "epoch 46 batch number 16    batch loss: 0.06009179353713989\n",
            "epoch 46 batch number 17    batch loss: 0.060965996235609055\n",
            "epoch 46 batch number 18    batch loss: 0.06171547994017601\n",
            "epoch 46 batch number 19    batch loss: 0.06253490597009659\n",
            " Average epoch losses: 0.06148555129766464 \n",
            "epoch  47\n",
            "epoch 47 batch number 0    batch loss: 0.0625256896018982\n",
            "epoch 47 batch number 1    batch loss: 0.06421752274036407\n",
            "epoch 47 batch number 2    batch loss: 0.05593695864081383\n",
            "epoch 47 batch number 3    batch loss: 0.06213010102510452\n",
            "epoch 47 batch number 4    batch loss: 0.06398959457874298\n",
            "epoch 47 batch number 5    batch loss: 0.05546077340841293\n",
            "epoch 47 batch number 6    batch loss: 0.05859077721834183\n",
            "epoch 47 batch number 7    batch loss: 0.060082800686359406\n",
            "epoch 47 batch number 8    batch loss: 0.06918463110923767\n",
            "epoch 47 batch number 9    batch loss: 0.06071121245622635\n",
            "epoch 47 batch number 10    batch loss: 0.0636233314871788\n",
            "epoch 47 batch number 11    batch loss: 0.0661725178360939\n",
            "epoch 47 batch number 12    batch loss: 0.06891762465238571\n",
            "epoch 47 batch number 13    batch loss: 0.06355061382055283\n",
            "epoch 47 batch number 14    batch loss: 0.05993657186627388\n",
            "epoch 47 batch number 15    batch loss: 0.0602574497461319\n",
            "epoch 47 batch number 16    batch loss: 0.061208054423332214\n",
            "epoch 47 batch number 17    batch loss: 0.06069471314549446\n",
            "epoch 47 batch number 18    batch loss: 0.05923483893275261\n",
            "epoch 47 batch number 19    batch loss: 0.05858588591217995\n",
            " Average epoch losses: 0.06175057962536812 \n",
            "epoch  48\n",
            "epoch 48 batch number 0    batch loss: 0.056134555488824844\n",
            "epoch 48 batch number 1    batch loss: 0.06016242131590843\n",
            "epoch 48 batch number 2    batch loss: 0.05613299086689949\n",
            "epoch 48 batch number 3    batch loss: 0.0599910244345665\n",
            "epoch 48 batch number 4    batch loss: 0.05881429463624954\n",
            "epoch 48 batch number 5    batch loss: 0.054880350828170776\n",
            "epoch 48 batch number 6    batch loss: 0.058212295174598694\n",
            "epoch 48 batch number 7    batch loss: 0.06191665679216385\n",
            "epoch 48 batch number 8    batch loss: 0.05546412989497185\n",
            "epoch 48 batch number 9    batch loss: 0.05530107766389847\n",
            "epoch 48 batch number 10    batch loss: 0.05945985019207001\n",
            "epoch 48 batch number 11    batch loss: 0.06135750934481621\n",
            "epoch 48 batch number 12    batch loss: 0.054316163063049316\n",
            "epoch 48 batch number 13    batch loss: 0.06122445687651634\n",
            "epoch 48 batch number 14    batch loss: 0.04908466339111328\n",
            "epoch 48 batch number 15    batch loss: 0.05394839867949486\n",
            "epoch 48 batch number 16    batch loss: 0.05309351906180382\n",
            "epoch 48 batch number 17    batch loss: 0.058447398245334625\n",
            "epoch 48 batch number 18    batch loss: 0.05413023754954338\n",
            "epoch 48 batch number 19    batch loss: 0.06091386079788208\n",
            " Average epoch losses: 0.057149291038513184 \n",
            "epoch  49\n",
            "epoch 49 batch number 0    batch loss: 0.05333158001303673\n",
            "epoch 49 batch number 1    batch loss: 0.05915777385234833\n",
            "epoch 49 batch number 2    batch loss: 0.053333040326833725\n",
            "epoch 49 batch number 3    batch loss: 0.058286771178245544\n",
            "epoch 49 batch number 4    batch loss: 0.05268726497888565\n",
            "epoch 49 batch number 5    batch loss: 0.05073323845863342\n",
            "epoch 49 batch number 6    batch loss: 0.051624372601509094\n",
            "epoch 49 batch number 7    batch loss: 0.058795955032110214\n",
            "epoch 49 batch number 8    batch loss: 0.05481452867388725\n",
            "epoch 49 batch number 9    batch loss: 0.05298631638288498\n",
            "epoch 49 batch number 10    batch loss: 0.058490633964538574\n",
            "epoch 49 batch number 11    batch loss: 0.05966348201036453\n",
            "epoch 49 batch number 12    batch loss: 0.057258229702711105\n",
            "epoch 49 batch number 13    batch loss: 0.05800154060125351\n",
            "epoch 49 batch number 14    batch loss: 0.05805712193250656\n",
            "epoch 49 batch number 15    batch loss: 0.05250588431954384\n",
            "epoch 49 batch number 16    batch loss: 0.050185803323984146\n",
            "epoch 49 batch number 17    batch loss: 0.05517515540122986\n",
            "epoch 49 batch number 18    batch loss: 0.05627822503447533\n",
            "epoch 49 batch number 19    batch loss: 0.055848944932222366\n",
            " Average epoch losses: 0.05536079406738281 \n",
            "Model saved in path: model.ckpt\n"
          ]
        }
      ],
      "source": [
        "init = tf.global_variables_initializer()\n",
        "epochs_average_loss=[]\n",
        "saver = tf.train.Saver()\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(n_epochs):\n",
        "        print('epoch ',epoch)\n",
        "        LOSS=[]\n",
        "        for i in range(num_batches):\n",
        "          images, blur, _ = train_images.get_batch(batch_size)\n",
        "          batch_loss = sess.run([loss, train], feed_dict={x: images, y: blur})\n",
        "          LOSS.append(batch_loss[0])\n",
        "          print('epoch {} batch number {}    batch loss: {}'.format(epoch, i ,batch_loss[0]))\n",
        "        mean_epoch_Losses=np.mean(LOSS)\n",
        "        epochs_average_loss.append(mean_epoch_Losses)\n",
        "        print(' Average epoch losses: {} '.format(mean_epoch_Losses))\n",
        "    save_path = saver.save(sess, \"Model/model.ckpt\")    \n",
        "    print(\"Model saved in path: %s\" % save_path) \n",
        "       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCv1y10rsafJ",
        "outputId": "ce279005-0b97-489d-ddab-422a54e9baa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: Model/ (stored 0%)\n"
          ]
        }
      ],
      "source": [
        "!zip \"blur-detection-model.zip\" \"Model\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t=np.arange(0,n_epochs,1)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.title('Average Loss Error for {} Epochs. Learning Rate: {}'.format(n_epochs,learning_rate),fontsize=22)\n",
        "plt.plot(t,epochs_average_loss,color='r',label='Mean squar Error loss')\n",
        "#plt.plot(t,Gaverageloss,color='r',label='Generator Overal Error Gloss')\n",
        "#plt.plot(t,ADVloss,color='g',label='Gnerator Adversarial Error G_Adv')\n",
        "#plt.plot(t,MSEloss,color='y',label='Generator MS Error G_MSE')\n",
        "plt.xticks(np.arange(0,n_epochs+1,5))\n",
        "plt.yticks(np.arange(0,0.3,0.05))\n",
        "plt.xlabel('EPOCHS',fontsize=18)\n",
        "plt.ylabel('Error',fontsize=18)\n",
        "plt.gca().legend(prop={'size': 18})\n",
        "plt.grid(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "cfMsboSQMmyU",
        "outputId": "37cb4087-ef60-4407-e8dc-a0830210ab52"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x720 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABJ0AAAJrCAYAAABKq+J5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd5hU5dnH8d/NUpYmCCIiIKCgLwhWxBhEwKggiIKxNxDUiBIbEIpGTCKCjZiCiiLFglgiSERFomCLoqhYEGyAUZQuCCxF4Hn/eM4ss7MzW2Zn98zOfj/XNdfunHqfMmdm7nme+5hzTgAAAAAAAEAqVQo7AAAAAAAAAGQekk4AAAAAAABIOZJOAAAAAAAASDmSTgAAAAAAAEg5kk4AAAAAAABIOZJOAAAAAAAASDmSTgAyiplVMrP/mZkzs7VmViXsmCoCM7st2OdTwo4l1cysebBthT7CjjWVzCzbzMaa2ddmtiPYxkUhxzSlkGOwtIB5K5nZtWa20My2mNkmM3vTzC5MMpYVRTwvuiS9wWUs6nV8W9ixxBO1z7uEHUtZMbMuwTbPDzuWgiQ493PM7JvgdXtU2DGWBTOrZmY3m9lnwfZvMLM5ZtatBMtM+tplZhcF024K5l0YLKvA74Bm1t3MXgnizwm252Yzq5Zg+qPNbKSZzQs+e/0SzDvPzC4vbH0AMlvlsAMAgBQ7VVLT4P/9JJ0p6V/hhYMMMzXsAMrQ7ZIGS1ot6XlJOZL+F2pEe70t6es4w3+MN7GZZUl6Tv568LOkVyRVk/QbSdPM7FfOueuTjGWOpFUFjC9oHJBp/iVpS/D//pLaS+or6WIzu8g590wqVhIk4TpL6uqcm5+KZZaUmdWU9JqkDpLWSpotaV/568xpZjbYOTeumMtM+tplZuMlXSNpu6RXJf0SzPdPSb8xs3Occ3vizPcHSXdK2i1pvqSf5Pf17ZLOMLPfOOdyoqavLOnD4OkWSe/Lv280kdRJUhdJF5jZWc657cXZfgCZgaQTgEzTP/i7UlLj4DlJJ6SEc65f2DGUoXODv52cc1+FGkl+E51zU4ox/Q3yX9o+l3Syc261JJlZK0lvSrrOzF5zzj2fRCxj0+VLLzLSe5Jayyd9y4MhzrkVkSdmVkvS45LOkvSAmb3gnNsWVnClbKx8wul1SWc457ZIkpkdL5+MusfM5jnnPirGMpO6dpnZb+UTTqsknRS5hptZQ0nzJPWR9HtJf4uZr32wHTnB+hYEw2vJJ9FOkjRa0o0xcX4gn6ia5ZzbEbW8dvKJ+dMkjZA0qhjbDiBD0NQRQMYws3ryH2ydpAvkf6XrZmYHhhoYUD41laQ0TDgVS9BS4A/B04GRL21S7rYNC57eXNaxAYVxzuU455Y659KllWGxBImXa4On9SWdEGI4pSb4/PE7SXskXR5JOElSkLi5S5LJJ16KusySXLsi6xkWfQ0PljEweDo8Tre34UGcd0YSTsF8WyRdHmzfNWZWN2rcLudce+fcM9EJp2Dcp1HbcElh2wwgM5F0ApBJLpZvdj7fOfeWfDP0LPmm/bnM7Oqg3sSMRAsys7bBND8GTcejxzU1s7+Z2Rdmts3Mfjazt82sn5lZnGXNj9QhMbOTzGy2ma0zsz1m1juYpoGZXW9mL5vZcjPbHtRgeDeov5BVQKxdzOw/QRw/m9lbZnaW7a1FtCLBfPXN7HYz+zSo9bDVzD40sxutDGphmVnNoEbEx8G6t5rZoqAuRI0E85wW7L81UTUjlprZJDM7JmbaumZ2h5ktDmpSbDez74PjUeQP/iXYvn7B/p8S7Ou/B8d2p5nNDKaJ1CjqZ2ZHmNkzZrbKzHab2Q1Ry2pmZveb2TLz9ZV+Ml8r46IE686tzRPMOznY9l1mdl8hca8wX5/Kgudx6xOZWRUzG2RmC4LzbpuZLTFfB6p+nOXmno9mVtnMhkQd+43J7eUiOUG+m8/3zrk34ox/Rr7byXFm1rgU44i3D4YH+2y7ma02s6lmdlAB8x9uZo+a2XfBebDOzF40s9MLWW83M3vOzH4Izr9V5q9Zw8yseoJ5GprZhOC82RGcu2PNLDvOtFnmr6v/NX/d2hlsz4dmdq+ZNSj+3kqN4l7nLIlrcVHO7ZjrQW0zuztY/g4zW2lmD5hPXMQuO25Np5h1mpldY/76mRNcH543s7YF7Jek3jeS4ZxbKWl98LRhTBxVzOxSM3vS/Hvq5mAbPjezO2P3SWR/yHf3kqR5VkANNSvm+3UJ9JBURdJ/nXPL44x/IjJdvPMugaSuXWbWRNKxknYG0+ThnHtdvjX4AZJ+FTVfVUmRa8kTceZbJukdSVXlt7eoIi27mhRjHgAZhO51ADJJpGvdlODvZPkPUJdLGhM13XRJf5XU08z2c86ti7OsSKLqCefcrshAM+sqaYakOvI1ZV6WVEv+g9tkSSdLuixBfOdKulq+mfxc+ZpTvwTjukm6T9L3kr6S9K78B8ITJB0v6VQz6+Ocy1Os2swuka8zVEm+psIXklpIminpngRxRJq8vyzpwGCd84NlHC9pXLBvejjndiZaRkmY2X7y3Q3aydeLmBOM6irfdP88MzvZObchap5+8vt4j6QFkr6V3/dNJfWT9KWCuhLmk1ZvS2ojaY2k/0jaKqlRMOxXyntOlKb95Gtc1JHvDrFQe7+ARXSU9KD8F4H5kmor6E5jZr+S9JKkupKWy59/9eTrZHQxs+6S+saeG4FW8h/4t8vvj8qSCkvwPBvEHHkNRNexWhXElB3E1CWIc17wt5P8r+8XBMdvWZzlm3yX1+6S3pB/PSRMtCTQ1cyOkD/+qyW9JWluvPokko4O/r4fb0HOuRwzWyzpqOCxspixJOspSWfIH++PJf1a/trR3cxOcs59ET2xmZ0p6Wn5xPpi+XOpify143Qzu90598eYeUzS/fLXHcmfe6/Lnz+t5bvRPCVpRUxsTeW7y5ik/0raR9KJ8se2jXx3n2iPyJ8v2+SPxTr5c+gQSTfJf/FdW8T9kjJJXueSuhZHVqnCz+068q/FxsE0n8nv26sldTBfo+cXFc8USecHy/tK0nHyx6iLmR0d+zpM9n0jWeZb09QKnq6OGd1Q0qPy7wNLJS2SP9/ay7eQOcfMjo96n14VxN49mDe2plru/8m8X5tZc/nrrCS1iO4qWIjCrjNfm9lP8jWeDpV/DZd0mYmuXZH5FhfQlfF9+XPwaPnXuCQdJqmGpA3OuW8KmK9jMN+0ImyD5N+HpAQ19wBUAM45Hjx48Cj3D/kPQE6+0GaNYFg1+S/3Tr4uTfT0TwbDr4uzrCxJPwTj20YNbyRpg6Rd8l+wLGpcU/kv905Sv5jlzQ+GO0lXJYi/taTj4wxvFLXc82PGNZYv2ukk9Y8Z1yeI00laETOuuqRlwbjhkipHjasnnxBzkm4rxv6/LZhnShGnfzqY/g1JdaOG7yv/hcxJejJmnkjMv46zvCaS2kQ9vyyY9oXo7Ys6vicXY9uaR45fMc/JflHHfY6k2nGmmRI1ze2SKsWMz5Yv3u3kE6VZUePayn+Bc5J+l+B4OPkvV1WTeE0l3Gb5riJO0hJJjWPOrWeDce8k2o/yCcOWScQUvb9iH4sltYszz7jI/itguc8H0wwqRiwrgnm6JHMuBccu+pytKumxYNx7MfMdIGlTMO6mmHFd5BOqTlK3mHE3BMNXSfpVzDiT/9JdJ8F583D0eSN/jdocjOsYNbxZMOx/khrG2eajJO1f3GNd0n2uJK9zSu5aXOi5rbzXg9mSakWNO1B7X+cXxzm+Tr4Fb6J1LpN0SNS4asE6nKSHY+ZL6n2jCPs7EkvzOOO6B+PWKnh/jhpXW1IvSVXiHL9JwXwPxFnm/ILOBSX/fh29X/NtSwHb/1wwz/UFTPNxMM0ZRVxmUtcuSdcFw2YUMN/fgmnuiRp2ZjDsowLmuzGY5tkibkMkce0k3Vucc4oHDx6Z86B7HYBMEWnl9LQL7qrifG2BJ2LGR0wJ/vaLs6xu8h9YP3DOfRY1/Ab5pMi9zrmpzjkXGeGc+07SlcHT3yeIca5z7qF4I5xzS1xU/YSo4T9qbz2Ec2JGD5BUU9KrzrlJMfPNUOIC6v3kf9V+2jk31kW15HK+ZVFf+RZY16a4+4Ek31VMflv2SLrSOZfb8sY595P8ftwj39qpadSsDSVtdM79VzGcc9875z6PmVaS/hO9fcG0u51zryUZe7zbgkceMxPM9ot8UmhzAYteKmmUy99S51z5L0grJP3BObc7ajs+096irEMSLHe9fGI1ZS3WzHfHitQEuc75rjORmLbJt9jYIulXZtYxwWJGOOfi3X2uMIvkv1C1kW+xcKB8a6GPg2H/sfxd5CItLLYWsNxI/ZXaScQU270n+lFQq7K/RJ+zwTH6vXzi/LiYfXelfOuPt13M3a+cL2L+j+Bp7nlgvltwpNZLP+fcuzHzOefca865TXFi+04x541zbol8Ukzyd8CK2D/4+6GLqjkTNd8i59yaOOsobf2UxHUuyWtxtMLO7S2SBri8NX9+kL+jmJR33xbVdS6qZUrw3venBMtL9n2j2Mx3VTxfPnm0U3678xREd85tds7928W07gquJYPkk0a/TWL1yb5f/yLf8usL7W2JXBSlcZ1JdpllPV9BRsm3ElytsmtdDCDN0L0OQLlnZtUkRWrbTI4ZPVn+Q+W5Zvb7qA/6c+Wboh9tZu2cL3YZ0Tf4OyVmWZEaBolu+fyB/Aeyo8ws2+W/NfBzhWxHZfmWByfIt2zIlv+VMPLh7tCYWToHfxM1cZ8m6bw4wwvcDufcD2b2lfyX+Fby3dZSqZP8dr3jYroQBev/3MwWyO+Hk7Q3cfiefHeRR+Vb/SyK/iIRI9IdYZiZrZP0QnRyqwSmFjDuw0TDXeFdNJ6PTihFyT3GsV/KAlPku0+1NLPG0QmgwH8KSXYl41j5Lyc/OOfmxo50zq0zs39LulC+lcbbcZaRsJ5aQZxzsfWotkqabWZz5buN/Uq+gO6gZJafpNjuPdEKuuPY47EDnHMbg313sfLuu8h5kOj8myTf9e1EM8sKzqX28l3cvnfOvVzgFuT3movfLWdp8PfAmGGb5buqjZTvkvxtMddXGpK+ziVxLY5W2Ln9gXMu3vkSb98WxS75bmNFXV6y7xtFtTzObxU/STrBOZfoGikzO1o+QdZcPikWWchOSQ3MbN/gR4miSur9OriG/l8x1oMEzOwySbfKH8MLXfxSBgAqAJJOADJBb/nuEl855/J8wXXOfWRmH0s6Ur7mxSPB8D1B8mKE/C/igyVffFq+iflO5f9QfnDw9/0iNACqr/y1YRJ+ETOzQ+XrabQuYJn7xDyPtOhItNxEwyPb8UwRtqOBUp90isS9vIBplsl/4YtutXKNfHe5S4PHJjN7T75e06PRX+Scc/PN7C75lh+PSXJmtlS+3sy/nHNzlATnXL8kZivKF/BE0xS4r5xz283sh2C6xirGOVcCRT1+0dNGW5MgoZE059xOMxsj39UktsBtJNFcs4BFRH7hTyZBNzZobVQcGwtIgq4I/kYX3S1sn6+Qbx2YLX/tWSPf7U3yLTaKK9Gd0n4O/uYWE3fObTaz/vKJr9GSRpvZSvmCw7MlTY+TgC8LSV3nkrwWRxTl3C7yvi2iH2Nbc0qSc+7nYLurxYxK9n2jqP4l/5rLkj+HO8m3OJpuZic45/LUszOzWvI/LMTWCYu1j3zyqqhK+n5dXKVxnUl2mWU9Xz5mdq78NWG3pAucc/MKmh5AZiPpBCATRLrO1TGzt+KM3z9qukeihk+RTzpdbGbDgg/u58t/6P+XiypiHYjctegp+cLMBdkRZ1hBX0aelf+SM0u+Xs4SSZucc7uDL0FfaO8vv7EStfaJV1RZ2rsds+UL/hYktuB1KiWKO/7Ezi0xs/+T7/54snwx066STpU0ysx+G92iwzk3zMwelHSWfKHejvJdKq40s1ck9Yz3Za0UFCXBUtg0xdpXxVx3stItpkjLjthE14rgbzMlFunGuaKAadJBcfZ5ssdHSnztiL8i5541s//Iv9ZOkn+tnRM8bjOzTkGXprKU7HWuJNfiopzbxdq3pbi84r5vFNWQ6JadZnaYpFflW5M9KN9lONoY+YTT5/K1txZKWhdp2Rkk1Rsp8T5PpKTv18W1IvibyutMssss6XwF3dih0G0ws7O190e7S4NumwAqMJJOAMq1oObPKcHT/bU3wRTPr83ssEiXLufcl2b2jnyLmtMl/Vt7azxNiTP/d5JaytdiKcqdZ4okSKS0k2+dcHacblYtE8z6g/zdZhJ9sGyeYPh3wXwPOOdmFy/alIj8onxwAdNExuX59Tn4IvJC8JCZ7StfM+J6+YRi45jpl8vfieq+YPoT5YvInyafhIxbYyuNFLivgrvIHRgzbWmLrKdFAdPEPX6lrH7wd0vM8EiXnuPizRTc6TBya/mP4k1TCuqaWZ0E9ZSaB3+j912ky8/B8l/g481TSf7LdSRZHmlRc1hJgy2KoOXW1OAhMztEvhh5V0l3am8X6LJS7OtcCa7F5Umy7xtJcc59YWZ95VuknhMkIN+MmiSShDrf5a2hKDOrKd+9MRml8n5dgMKuMy3lW3zlqOith5O9dkX+P9zMqidofXdczLSST9xvk1TPzA5x8e9g1yHOfNEx9Za/Q3Al+Vpy0+NNB6BioZA4gPKun/y17DXnnCV6yN8tTcpfUDxSA6pv8Cv2r+Trs8SrkfFS8Df2l9qSqhf8/SFBXZ+LE8z3RvD3wgTjEw0vre0oqjflf2X/VbDP8zCz1vK3Jt+jvdsYV1DjY2gw7YFm1qCQ6d/S3oTikcWOvOy9Hvy9MKgzE6uvfAuAr+PUcyotkVoojc0sX9FjM6svfzcqyd9hqqxE6tDE3l78Hfm7ZjUxs5PizHeupCqS3i/DfSjFeV2bWR35wuhS3n0XOQ/y3N49yuXB37eiWu99IN/Cp4mZdStZqMUXfGEdHTwN47WWzHUu2WtxeZLs+0bSnHOvynd9laS/xIyO7PN4LeEuUuIWTpEi94l+QC/r97kX5QuP/9rM4iXkI+fObFf0Gzskde0KWhV+KH9HzHzbb2ad5bs+rgrWEZlvp/but3jXp4Plf6TbKd+CMHZ8L/nPWpUlXeGceyx2GgAVE0knAOVWcMehfsHTwj7cRMZfamZZUcOfkv9lr5ekm4JhTyTodnW3fN2NkWZ2bbwkgJkdHjQtL46v5JMmbWM/WJrZ5Ur8JeCRIPZTg1+So+c7U4k/bD8k/wG/r5ndFvxamoeZtTCzS4q3GUUTFBn+l/x70ITgi3ZkvXUlTQjGPR3pkmNmNczspgRJpZ7B9D9L2hhM38fMTjKzPO9z5u+8FmkZlw7FjgvzjPyxaiFpTPT2mFkb7b1D1T1lFVDwq/mDwdO/mVmjqJiyJT0gX/fj3dgaayVhZkeZ2Rkxr1+ZWWUzGyx/VzvJF5mPjne3fDcpSXrAzPaPmreVpLHB09EqW7cGCdZILFXkb2NeR77YdHRX4Yfla6icaGbXRS8kuGZE7sB1b2R40CowcreoyWbWIWY+M7Ou0a+/ZJjZ0WZ2fvDaihVJPn4bM88YM1sa1OEqLclc55K9Fpcnyb5vlNRI+fo+nc2sa9TwSLfYa2Jiaa+C73YWSbIkqr2V1Pu1mTUOzs2llv9OmAkF3fEfkn8vmhTUqoos83j5Ox+6eNtkZo8G68tzA4QSXrsi67kzaGUVmW9/+ZtPSL4eXWx3yrFBnMOirxnB9kwKtu/+2Jp0ZtZDvmtqZUlXOedib+oCoAKjex2A8qyLfHeTbSr8Ns8vy/9i2Ei+0PC/pdxiqzPkf1H9XTDtlHgLcM59FzQdf1b+9tY3m9li+a4YdeW7ZTSVT2QVeKe6mOWuNbP75e+4Nc/MXpf/BbKdfNP5MfK1p+LFM1C+tdYUM/u9fL2RFvK/Rv5V0o3a+4twZL4tZtZTvovaKEm/N7NP5Ltd1Jb/EN9S0gLFucNWIXqa2bsFjJ/onJsoaaB8d6EukpaZ2fxgfFf5LggfS7o2ar6q8l+o7zKzT7X3y+Eh8nfpcpKGRd3hrbN8l7u1ZvaR/LGvI+nX8r+sL5VPbhWLmU0pZJJbnXOJCgUXW1Ao/Dz5X5+HSOpjZu/Lb0NX+V+5H1PZdxP8o/x+7yLpKzN7Tf512En+NfY/pb5VSHP5O4NtMLMP5V939eVfJwfKnw9/SFAk/q/ytYZ6BfG+Kr/vTpGv4fYP59zzceYriuFm1q+A8dOcc6/EDPuffEukRcG+2yR/bjaVb52Up0WTc26VmV0qf235m5ldIekz+e3uJP9F8PY4d6n7q/zr+QpJ75rZQklfy58/bYL1tQjWn6xm8t1pcoLj8p386/Vo+evzZvk7WEVrJN/Fq5GSc7+Z/VzA+D7OuR+Le51L9lpcniT7vpGC9X5u/uYdl8snyyOFpf8sn1y/w8zOl6+hdaB8Hb7p8vXB4nUFnCH/o9PdZnaq/PVAku52zn1RgvfrKtrbJbVKMTdzuHz3sy6SvgnOn7ryNQiz5OtdxeuWdlCwzv3ijEvq2hXUWXtA/r32U/M1136Rv0PgPvLF8v8ZZ773zWy4fJfY/wbXp43y76n7y79ebo6eJ0hkPSf/uv9ePjl+YrwdlOTNOACUd845Hjx48CiXDwV3JZP/UleU6f8eTD8jZvgpwXAnaWERlnOA/C+Li+S/UG2XL6o5X/5D5yEx088Plt2lgGVWki9y/aF816Wf5GtgdJf/su0krUgw78nydV42B4//Svqt/Id2J+m/CearI/8F6l35L5075D8wviP/ReCIYhyL26L2YUGP26LmqSXpFkmfyNe5yJFPNo2UVDNm+ZUlXS3/JWRpEG+OfPLpCUnHx0x/lPwvtm/J/yK+Q9Jq+Q/MN0iqXYxti+z/ojyOipqvXzBsSgHLnhJM06+QGJrJtyBaHmzLRvkuVxdLsgKOx21F3c44y3CSXAHjq8i3sHlPe18HS+W/rNQvYD/GPY+LEE8L+dpc/w2O6Xb5RNdX8r/AH1vI/JXkkwkfSNoq3wriLUkXJRnPiiKeEzfE2wfBOX2L/Bf+7fJfhh+T1LyAdbYNplkpnxRYL5+Q7FFIrGfIJ1/WBPP9KN/Ndaik7KKeN/HOafnr4fAgjuXyr8uNkj6Vb4HXrIDzPuFro4T7vHnUPMW6zimJa3FRzu14+y5mfJdg/PwiDi/KOhO+hpXk+0Zh6yrk/G0q/5p1kn4Ts42vBefzFvl6Qb8PjsWKRMuVT6gsCs65yPq7xExT3Pfr5irCthSwjdnyr+vPg239SdIrkroVMM98Ffy6S/raJf+D2tvBPFuDZVwrqVIh83WXNDeIf5ukxfLJpmpxpo3eZwU+irs/efDgkRkPc84JAJB5zOyP8l+q/umc+31h0wMoXWbWXD4x861zrnmowQBx8L4BAEg1ajoBQDlmZgeZWcM4w3vI/7rvFNxNCgAA3jcAAGUp1JpOZtZdvmhmlnydj7Ex42+Sr0OwS74eR3/nC9DKzHbLN92WpP85584ss8ABIH2cJl+Me5F8sV6Trw0RKa56u3NuYVjBAQDSDu8bAIAyE1r3uuDuM19KOlW+b/37ki50zn0eNU1XSQucczlB0cMuzrnzg3FbnHO14iwaACqM4A5mQ+TrcDSUVEPSBkkLJT3gnHshxPAARKF7HdIB7xsAgLIUZtLpBPmCed2C5yMkyTkX9/aoZna0fP/yjsFzkk4AAAAAAABpKsyaTo3lb6sb8X0wLJEB8ndGicg2s4Vm9m5wS1QAAAAAAACkiVBrOhWVmV0iqb2kzlGDmznnVprZwZJeM7NPnXPfxMx3laSrJKl69erHNm3atMxiLk179uxRpUrUgE9XHJ/0xbFJbxyf9MWxSW8cn/TFsUlvHJ/0xbFJbxyf9BXWsfnyyy/XOecaxBsXZtJppaToLFCTYFgeZnaKpJsldXbO7YgMd86tDP4uM7P5ko6WlCfp5Jx7SNJDktS+fXu3cGFm1EScP3++unTpEnYYSIDjk744NumN45O+ODbpjeOTvjg26Y3jk744NumN45O+wjo2ZvZtonFhpiffl9TKzFqYWVVJF0iaFT1BUMdpgqQznXNroobva2bVgv/3k9RR0ucCAAAAAABAWgitpZNzbpeZDZI0R1KWpEnOucVm9mdJC51zsyTdLamWpGfMTJL+55w7U/6WrhPMbI984mxs9F3vAAAAAAAAEK5Qazo5516U9GLMsFuj/j8lwXz/ldSudKMDAAAAAABAsqj+BQAAAAAAgJQj6QQAAAAAAICUI+kEAAAAAACAlCPpBAAAAAAAgJQLtZA4AAAAACD9bNq0SevWrdPOnTvLfN116tTRkiVLyny9KBqOT/pK5bGpWrWq9ttvP9WpU6dEyyHpBAAAAADItX37dq1evVpNmjRR9erVZWZluv7Nmzerdu3aZbpOFB3HJ32l6tg457Rt2zZ9//33qlatmrKzs5NeFt3rAAAAAAC51q5dqwYNGqhGjRplnnACED4zU40aNbTffvtp7dq1JVoWSScAAAAAQK7t27erVq1aYYcBIGS1a9fW9u3bS7QMkk4AAAAAgFy7du1S5cpUYgEqusqVK2vXrl0lWgZJJwAAAABAHnSrA5CK6wBJJwAAAAAAAKQcSScAAAAAAIAytmLFCpmZbrvttrBDKTUknQAAAAAAFc78+fNlZjIzDRo0KO40a9asUdWqVWVm6tKlS9kGiAI1b9489/jFezz++ONhhwhJVIcDAAAAAFRY2dnZmjZtmu69915Vq1Ytz7jHHntMzjkKq6epJk2aaMyYMXHHdezYsYyjQTy8cgAAAAAAFVafPn305JNP6vnnn9d5552XZ9zkyZPVo0cPvfrqqyFFVzE557R161bVqlWrwOnq1KmjSy65JKl1bN68WbVr1447btu2bapSpUqJk42//PKLdu/erezs7BItpzyjex0AAAAAoMI65phjdMQRR2jy5Ml5hr/33ntavHixLr/88oTzLly4UH369NF+++2natWq6bDDDtPo0aPz3Wb+vffeU79+/XTooYeqRo0aql27tjp27KgZM2bkW2a/fv1kZtq0aZMGDhyo/fffX9nZ2erYsaMWLFhQpG3asKjEOx8AACAASURBVGGDbrzxRh1yyCHKzs5W/fr1deyxx+ruu+/OM9327ds1dOhQHXjggapevbo6dOigV155JTeGaM2bN4/bxTDSTXHKlCm5wzZv3qxbbrlFxx9/fO6+admypYYPH66cnJyE848fP15t2rRRdna27rnnniJta1F06dJFzZs317Jly3TOOeeoXr162meffSTt3d9r165V//791bBhQ9WsWVPff/+9JF936dJLL1XDhg1VrVo1HXLIIRo5cmS+7bjttttkZlq8eLFuuukmNWnSRNnZ2Xr33XeLHe+uXbt055135u6L+vXrq0+fPvr000/zTfvoo4+qQ4cOqlu3rg444AAdfPDBuvjii7V27drcaRYvXqxzzz1XjRs3VrVq1XTAAQeoa9eumj17drFjKy5aOgEAAAAAKrT+/fvrpptu0sqVK9W4cWNJ0qRJk7T//vvrjDPOiDvP7NmzdfbZZ6tly5YaPHiw6tWrp3feeUe33nqrFi1apGeeeSZ32hkzZmjp0qU677zz1KxZM61fv15Tp07V2WefrSeeeEIXXXRRvuV369ZNDRo00K233qr169dr3Lhx6tmzp5YvX56whU7EueeeqzfeeENXX321jjjiCG3btk1LlizR/PnzNXTo0NzpLrzwQs2cOVO9evVSt27d9M033+jss89WixYtktmNuVauXKmJEyfqt7/9rS666CJVrlxZr7/+uu666y599NFHmjNnTr557rvvPq1fv15XXnmlDjjgADVt2rTQ9ezevVvr1q2LO65+/fp5EmdbtmxR586d1bFjR40ePVpr1qzJM/2pp56qAw44QH/84x9zW1l9++236tChgzZt2qRrrrlGrVq10vz58zVmzBi9/fbbevXVV/O1hrr44otVvXp1DR48WGamRo0aFWWX5VvG008/rVNPPVUDBw7UqlWrNH78eJ1wwgl68803dfTRR0vy3T/79u2rTp066c9//nNu8uzFF1/UmjVr1KBBA61fv14nn3yyJOnqq69Ws2bNtG7dOi1cuFALFixQz549ix1fsTjnKsTj2GOPdZli3rx5YYeAAnB80hfHJr1xfNIXxya9cXzSF8cmvXF8Evv8889DXf/PP/9cJuuZN2+ek+Tuvvtut27dOle1alU3evRo55xzOTk5rk6dOm7w4MHOOedq1qzpOnfunDvvtm3bXMOGDV2nTp3cL7/8kme548aNc5LynGNbtmzJt/6tW7e6Qw891LVu3TrP8L59+zpJbuDAgXmGP/30006Se/DBBwvcro0bN8adP9acOXOcJNe3b988w2fMmOEkOZ8u2KtZs2auc+fO+Y5PZD9Onjw5d9iOHTvczp07863zlltucZLcggUL8s2/7777utWrVxcYc2w8kTjjPdauXZs7befOnZ0kd/PNN+dbTmR/X3zxxfnGXXTRRU6Smz17dp7hQ4YMcZLcxIkTc4eNGjXKSXKdO3fOd04ksnz5cifJjRo1KnfYK6+84iS58847z+3Zsyd3+KJFi1xWVpY78cQTc4f16dPH1a5dO3d98V47zz//vJPknnrqqSLFFKso1wNJC12CXAwtnQAAAAAAhbvhBmnRolJfTfXdu6WsrKJNfNRR0n33lXid9evX15lnnqkpU6Zo5MiReu6557Rp0yb1798/7vRz587V6tWrNWbMGG3cuDHPuB49euimm27SK6+8ktsdrWbNmrnjc3JytG3bNjnndPLJJ+vBBx/Uzz//nNvdK+LGG2/M8zzSWuWrr74qcFuqV6+uatWqacGCBVqxYoWaN28ed7qZM2dKUp6WT5LUu3dvHXbYYfriiy8KXE9Bqlatmvv/rl27tHnzZu3evVunnHKKbr/9di1YsEAdOnTIM89ll12m/fffv1jrad68uR5++OG44+rUqZNv2JAhQxIuK3bcnj17NGvWLB199NHq0aNHnnEjRozQuHHjNGPGDA0YMCDPuBtuuKFEtaAiXS5vvvnmPC21jjzySPXq1UszZ87U2rVr1aBBA9WpU0c5OTmaPXu2zjzzzLjLi+yHl156Sd27d893npU2kk4AAAAAgArv8ssvV8+ePfXWW29p0qRJ6tChg9q0aRN32iVLlkhSwqSUJK1evTr3/zVr1uiWW27R888/n69blyRt3LgxXzLg4IMPzvO8fv36kqT169cXuB1Vq1bVfffdp+uvv14tWrRQmzZtdPLJJ6t37976zW9+kzvdsmXLVKlSJR166KH5ltG6desSJZ0k6f7779eDDz6oxYsXa8+ePXnG/fTTT/mmjxdHYWrWrKlTTjmlSNM2aNBAdevWTTg+dv1r167Vli1bdPjhh+ebtl69emrUqJGWLVtW6HKKa/ny5apUqZJat26db9zhhx+umTNnavny5WrQoIFGjhypN954Q71791b9+vXVsWNH9erVS+eff35uF8zOnTvrsssu05QpU/TEE0/ouOOO0ymnnKLzzz8/4fmdSiSdAAAAAACFS0GLoqLYVsBdxUpTt27d1LhxY/3pT3/SvHnz9MADDySc1vcoku6++24dddRRcac58MADc6c97bTTtGTJEl1//fVq37696tSpo6ysLE2ePFnTpk3Ll5SRpKwErb0i6y7I1VdfrbPOOkuzZ8/W66+/rmeffVb//Oc/df7552v69OmFzh9PbGHxiNii6ZI0btw4DR48WKeddpquu+46HXjggapatapWrlypfv36xd3eGjVqJBVXURW2/FStv7S3I1qrVq30+eef69VXX9Wrr76qefPm6corr9SoUaP0xhtv6JBDDpEkTZ06VUOHDtVLL72kN998U/fee69Gjx6t++67T4MGDSrVGEk6AQAAAAAqvKysLF122WUaM2aMqlevrgsvvDDhtK1atZJUtJY2n3zyiT7++GPdeuut+tOf/pRn3MSJE0seeAKNGjXSFVdcoSuuuEK7d+/WpZdeqieffFKDBw/Wcccdp4MPPlh79uzRl19+ma81T6QlV7R69eppw4YN+YbHa+3z2GOPqXnz5nrppZdUqVKl3OEvv/xyCrasbDRo0EC1a9fW4sWL84376aef9OOPPyZMOJZE5LgsWbJERxxxRJ5xn3/+uSTlKfRerVo19ejRQz169NDmzZv15ptvqmfPnho3bpzGjx+fO13btm3Vtm1bDR06VBs3btTxxx+v4cOH69prr02YUEyFSoVPAgAAAABA5rv66qs1atQoPfjggwXWvunWrZv2339/jR07Nm4iZtu2bdq8ebOkvS2WYlsoffbZZ7n1e1IpJydHOTk5eYZlZWXlJjAi8Z511lmSfGutaDNnzozbte7QQw/V0qVL9cMPP+QO27FjR57ERvT6zCzPNu/atUtjx45NcqvKXqVKldSrVy999NFH+ZJlY8eO1Z49e9SnT5+Ur7d3796SpDFjxuTZf5999plmzZqlE088UQ0aNJCkuHfuO+aYYyTtPc4bNmzI17Ksbt26atGihXJycrR9+/aUb0M0WjoBAAAAACDpoIMO0m233VbodDVr1tSjjz6aW3S7f//+atmypTZu3KilS5fqueee04wZM9SlSxe1bt1ahx9+uO666y7l5OTosMMO05dffqkJEyaoXbt2+uCDD1K6DV9++aU6d+6sPn36qG3bttp33321ZMkSPfDAA2rRooU6deokySfOevXqpalTp2rDhg3q3r27vvnmG02YMEFt27bVZ599lme5gwYN0vTp03XmmWfqmmuu0c6dO/XYY4/F7U52zjnnaMSIETr99NN19tln6+eff9a0adNUpUqVlG7rpk2b9Pjjj8cd165dOx155JElWv4dd9yhuXPnqnfv3rrmmmvUsmVLvfHGG3rqqad00kknqW/fviVafjynnnqqzjvvPE2fPl0//fSTzjjjDK1atUrjx49Xdna2/v73v+dOe9ppp6lu3brq1KmTmjZtqtWrV+vJJ5+UmenSSy+VJD366KP661//qj59+qhly5aqUqWKXn/9dc2ZM0fnnXeeqlevnvJtiEbSCQAAAACAYurWrZvef/99jR07Vo8//rjWrl2rfffdV4cccohuuumm3JZFWVlZmj17toYMGaKpU6dq69atatu2raZOnaqPP/445Umnpk2bqn///po3b55mzpypHTt2qHHjxrryyis1bNiwPEmip556SrfccoueeOIJzZ07V+3atdNzzz2nadOm5Us6dezYUVOmTNHtt9+uoUOHqnHjxho4cKDat2+fp0C55O+I55zTI488ouuvv14HHHCAzj//fF1++eUpLV79/fff5yZXYt18880lTjo1a9ZMCxYs0K233qrHH39cGzduVJMmTTRixAjdcsstJbpLXUGeeOIJHXPMMZoyZYoGDx6smjVrqnPnzvrLX/6idu3a5U43cOBAPf3005owYYI2bNigevXq6ZhjjtE//vEPde3aVZLUpUsXffTRR3rhhRf0448/KisrSy1atNA999xT6vWcJMmKUoQsE7Rv394tXLgw7DBSYv78+bm33kT64fikL45NeuP4pC+OTXrj+KQvjk164/gktmTJkrh3ziorm0MqJI69+vXrp6lTp8YtWs7xSV+lcWyKcj0wsw+cc+3jjaOmEwAAAAAAAFKOpBMAAAAAAABSjqQTAAAAAAAAUo6kEwAAAAAAyDVlypS49ZyA4iLpBAAAAAAAgJQj6QQAAAAAAICUI+kEAAAAAACAlCPpBAAAAADIg3o+AFJxHSDpBAAAAADIVblyZe3atSvsMACEbNeuXapcuXKJlkHSCQAAAACQKzs7W1u2bAk7DAAh27x5s7Kzs0u0DJJOAAAAAIBcDRo00Nq1a5WTk0M3O6ACcs4pJydH69atU4MGDUq0rJK1kwIAAAAAZJTs7Gw1bNhQq1at0o4dO8p8/du3by9x6wqUHo5P+krlsalWrZoaNmxY4uWRdAIAAAAA5FGnTh3VqVMnlHXPnz9fRx99dCjrRuE4PukrHY8N3esAAAAAAACQciSdAAAAAAAAkHIknQAAAAAAAJByJJ0AAAAAAACQciSdAAAAAAAAkHIknQAAAAAAAJByJJ0AAAAAAACQciSdAAAAAAAAkHIknQAAAAAAAJByJJ0AAAAAAACQciSdAAAAAAAAkHIknQAAAAAAAJByJJ0AAAAAAACQciSdAAAAAAAAkHIknQAAAAAAAJByJJ0AAAAAAACQciSdAAAAAAAAkHIknQAAAAAAAJByJJ0AAAAAAACQciSdAAAAAAAAkHIkncqbp55Sy3/8I+woAAAAAAAACkTSqbz58ks1ee45admysCMBAAAAAABIiKRTedO/v1ylStLDD4cdCQAAAAAAQEIkncqbxo21/oQTpEmTpJ07w44GAAAAAAAgLpJO5dAPZ5whrVkj/fvfYYcCAAAAAAAQF0mncmjDccdJBx0kTZgQdigAAAAAAABxkXQqj7KypCuukObOpaA4AAAAAABISySdyqv+/aVKlaSJE8OOBAAAAAAAIB+STuVV48bSGWf4guK//BJ2NAAAAAAAAHmQdCrPfvc7afVqadassCMBAAAAAADIg6RTedatm9S0qfTQQ2FHAgAAAAAAkAdJp/IsUlD8lVek5cvDjgYAAAAAACAXSafyLlJQ/OGHw44EAAAAAAAgF0mn8q5JEwqKAwAAAACAtEPSKRNcdZUvKP7vf4cdCQAAAAAAgCSSTpmhe3dfUHzChLAjAQAAAAAAkETSKTNQUBwAAAAAAKQZkk6ZIlJQfOLEsCMBAAAAAAAg6ZQxmjSRevakoDgAAAAAAEgLJJ0yye9+J61aRUFxAAAAAAAQOpJOmaR7d9/i6aGHwo4EAAAAAABUcCSdMgkFxQEAAAAAQJog6ZRpBgyQzCgoDgAAAAAAQkXSKdNQUBwAAAAAAKQBkk6Z6KqrfEHxF14IOxIAAAAAAFBBkXTKRJGC4hMmhB0JAAAAAACooEg6ZaLKlSkoDgAAAAAAQkXSKVP17+8Lij/ySNiRAAAAAACACoikU6Zq2lTq0cMnnSgoDgAAAAAAyhhJp0xGQXEAAAAAABASkk6Z7PTTfUHxhx4KOxIAAAAAAFDBkHTKZJUrSwMGSHPmSCtWhB0NAAAAAACoQEg6ZboBA3xB8YkTw44EAAAAAABUICSdMl2koPikSRQUBwAAAAAAZYakU0Vw1VXSjz9Ks2eHHQkAAAAAAKggSDpVBKefLjVuLE2YEHYkAAAAAACggiDpVBFUrixdcQUFxQEAAAAAQJkh6VRR9O/vC4o/8kjYkQAAAAAAgAqApFNFcdBBvpvdI49QUBwAAAAAAJQ6kk4VCQXFAQAAAABAGSHpVJH06OELij/0UNiRAAAAAACADBdq0snMupvZF2b2tZkNjzP+JjP73Mw+MbNXzaxZ1Li+ZvZV8OhbtpGXU5UrSwMGSC+/LH37bdjRAAAAAACADBZa0snMsiSNl3S6pDaSLjSzNjGTfSSpvXPuCEnPSrormLeepFGSjpfUQdIoM9u3rGIv1wYM8AXFJ04MOxIAAAAAAJDBwmzp1EHS1865Zc65nZKmSzoregLn3DznXE7w9F1JTYL/u0ma65zb4Jz7SdJcSd3LKO7yLbqg+K5dYUcDAAAAAAAyVJhJp8aSvot6/n0wLJEBkl5Kcl5Eo6A4AAAAAAAoZeacC2fFZudI6u6cuyJ4fqmk451zg+JMe4mkQZI6O+d2mNkQSdnOuduD8X+UtM05d0/MfFdJukqSGjZseOz06dNLdZvKypYtW1SrVq2k57fdu/WrCy7QlkMO0adjx6YwMkglPz4oPRyb9MbxSV8cm/TG8UlfHJv0xvFJXxyb9MbxSV9hHZuuXbt+4JxrH29c5bIOJspKSU2jnjcJhuVhZqdIullBwilq3i4x886Pndc595CkhySpffv2rkuXLrGTlEvz589Xibdl4EBVu/12dWnRQmrWrPDpUWQpOT4oFRyb9MbxSV8cm/TG8UlfHJv0xvFJXxyb9MbxSV/peGzC7F73vqRWZtbCzKpKukDSrOgJzOxoSRMknemcWxM1ao6k08xs36CA+GnBMBTVFVf4v488Em4cAAAAAAAgI4WWdHLO7ZLvMjdH0hJJTzvnFpvZn83szGCyuyXVkvSMmS0ys1nBvBsk/UU+cfW+pD8Hw1BUFBQHAAAAAAClKMzudXLOvSjpxZhht0b9f0oB806SNKn0oqsArrpK6t3bFxQ/66zCpwcAAAAAACiiMLvXIWw9e0oHHig99FDYkQAAAAAAgAxD0qkiq1xZGjBAeukl6dtvw44GAAAAAABkEJJOFd2AAf7viBHS1q3hxgIAAAAAADIGSaeKrlkzafhw6cknpTZtfH0nAAAAAACAEiLpBOmOO6Q335Rq1ZLOOEM691zphx/CjgoAAAAAAJRjJJ3gnXii9NFH0ujR0gsvSK1bS+PHS7t3hx0ZAAAAAAAoh0g6Ya+qVaWRI6VPP5WOP14aNEj69a+ljz8OOzIAAAAAAFDOkHRCfi1bSnPmSE88IS1fLh17rPSHP1BoHAAAAAAAFBlJJ8RnJl10kbR0qXT55dLdd0uHHy69+GLYkQEAAAAAgHKApBMKVq+e9PDD0htvSDVqSD17SuedJ/34Y9iRAQAAAACANEbSCUXTqZO0aJF0++3SrFnS//2f9MAD0p49YUcGAAAAAADSEEknFF3VqtLNN0uffSYdd5x0zTW+0Pgnn4QdGQAAAAAASDMknVB8LVtKc+dKjz0mffONdMwx0rBhFBoHAAAAAAC5SDohOWbSJZf4QuP9+kl33SW1bSu99FLYkQEAAAAAgDRA0gklU7++NHGi9PrrUna21KOHdMEFFBoHAAAAAKCCI+mE1DjpJF9o/C9/kWbOlFq3liZPlpwLOzIAAAAAABACkk5InWrVpFtukT79VDrySKl/f6l3b2n16rAjAwAAAAAAZYykE1KvVStp3jzp3nulOXN8rad//SvsqAAAAAAAQBki6YTSUamSdNNN0ocfSgcdJJ1zjnTppdLGjWFHBgAAAAAAygBJJ5SuNm2kd9+VRo2SnnxSatdOmjs37KgAAAAAAEApI+mE0lelinTbbdI770i1akmnnSYNGiRt3Rp2ZAAAAAAAoJSQdELZOe44393uxhul8eOlo47yiSgAAAAAAJBxSDqhbFWvLo0bJ732mrRzp3TiidLNN/v/AQAAAABAxiDphHB07Sp9+qnUr590xx1Shw7SJ5+EHRUAAAAAAEgRkk4Izz77SI88Is2aJf34o+9+d+ed0u7dYUcGAAAAAABKiKQTwterl/TZZ9IZZ0jDh0udO0tffx12VAAAAAAAoARIOiE9NGggPfus9NhjPgF15JHSgw9KzoUdGQAAAAAASAJJJ6QPM+mSS3zSqWNHaeBA6fTTpZUrw44MAAAAAAAUE0knpJ8mTaQ5c6Tx46U335TatpWmTaPVEwAAAAAA5QhJJ6QnM+maa6RFi6TWraWLL/atoPbsCTsyAAAAAABQBCSdkN5atfKtnUaO9K2d/v3vsCMCAAAAAABFQNIJ6S8rS/rTn6SDD5buuINudgAAAAAAlAMknVA+VK4sDRsmvfee9NprYUcDAAAAAAAKQdIJ5UffvlKjRr61EwAAAAAASGsknVB+VKsmDRniWzq9+27Y0QAAAAAAgAKQdEL5ctVVUr160pgxYUcCAAAAAAAKQNIJ5UutWtL110uzZkmffhp2NAAAAAAAIAGSTih/Bg3yyaexY8OOBAAAAAAAJEDSCeVPvXrSwIHS9OnSN9+EHQ0AAAAAAIiDpBPKpxtvlKpUke66K+xIAAAAAABAHCSdUD41aiT17y9NmSKtXBl2NAAAAAAAIAZJJ5RfQ4dKu3dL994bdiQAAAAAACAGSSeUXy1aSBddJE2YIK1bF3Y0AAAAAAAgCkknlG/Dh0s5OdLf/x52JAAAAAAAIApJJ5RvbdpIffpI//iH9PPPYUcDAAAAAAACJJ1Q/o0YIW3cKD34YNiRAAAAAACAAEknlH/HHSedeqo0bpy0bVvY0QAAAAAAAJF0QqYYOVJavVqaPDnsSAAAAAAAgEg6IVN07iydcIJ0113SL7+EHQ0AAAAAABUeSSdkBjPf2unbb6Unnww7GgAAAAAAKjySTsgcPXtKRxwhjRkj7dkTdjQAAAAAAFRoJJ2QOcz8neyWLpVmzgw7GgAAAAAAKjSSTsgs554rtWwp3XGH5FzY0QAAAAAAUGGRdEJmycqShg2TPvhAmjs37GgAAAAAAKiwSDoh81x6qdS4sW/tBAAAAAAAQkHSCZmnWjVpyBDp9delt98OOxoAAAAAACokkk7ITFdeKdWv7+9kBwAAAAAAyhxJJ2SmmjWlG26QZs+WFi0KOxoAAAAAACockk7IXIMGSbVrS2PHhh0JAAAAAAAVDkknZK66daVrr5Weflr68suwowEAAAAAoEIh6YTMdsMNvrD4XXeFHQkAAAAAABUKSSdktoYNpSuukB59VPruu7CjAQAAAACgwiDphMw3ZIjknHTvvWFHAgAAAABAhUHSCZmvWTPpkkukhx6S1q4NOxoAAAAAACoEkk6oGIYNk7Zvl/72t7AjAQAAAACgQiDphIrh//5P+u1vpX/+U9q0KexoAAAAAADIeCSdUHGMGOETTg88EHYkAAAAAABkPJJOqDiOOUbq3l0aN07KyQk7GgAAAAAAMhpJJ1QsI0f6YuKTJoUdCQAAAAAAGY2kEyqWTp2kE0+U7rpL2rkz7GgAAAAAAMhYJJ1Q8YwcKX33nTRtWtiRAAAAAACQsUg6oeLp3l066ihp7Fhp9+6wowEAAAAAICORdELFY+ZbO33xhTRjRtjRAAAAAACQkUg6oWI6+2zp0EOl0aMl58KOBgAAAACAjEPSCRVTVpY0fLi0aJH08sthRwMAAAAAQMYh6YSK6+KLpaZNpTvuCDsSAAAAAAAyDkknVFxVq0pDh0pvvSW9+WbY0QAAAAAAkFFIOqFiGzBAatCA1k4AAAAAAKQYSSdUbDVqSDfe6Os6ffhh2NEAAAAAAJAxSDoB11wj7bOPNGZM2JEAAAAAAJAxSDoBdepIgwZJ//qXtHRp2NEAAAAAAJARSDoBknTDDVJ2tnTnnWFHAgAAAABARiDpBEi+mPhVV0mPPy59+23Y0QAAAAAAUO6RdAIiBg+WzKS77w47EgAAAAAAyj2STkBE06bSZZdJEydKq1aFHQ0AAAAAAOUaSScg2rBh0i+/SPfdF3YkAAAAAACUaySdgGitWknnnivdf7/0009hRwMAAAAAQLlF0gmINWKEtHmzNH582JEAAAAAAFBukXQCYh15pNSzp+9it3Vr2NEAAAAAAFAukXQC4hk5Ulq/Xnr44bAjAQAAAACgXCLpBMTz619LnTtL99wj7dgRdjQAAAAAAJQ7JJ2AREaOlFaulB57LOxIAAAAAAAod0g6AYmceqrUvr00dqy0a1fY0QAAAAAAUK6QdAISMfOtnb75RnrmmbCjAQAAAACgXCHpBBTkrLOk1q2lO+6Q9uwJOxoAAAAAAMoNkk5AQSpVkkaMkD77TJo9O+xoAAAAAAAoN0g6AYW54AKpeXNp9GjJubCjAQAAAACgXCDpBBSmShXpD3+QFiyQ5s8POxoAAAAAAMoFkk5AUVx+udSwoa/tBAAAAAAACkXSCSiK7Gxp8GDpP/+R3nsv7GgAAAAAAEh7oSadzKy7mX1hZl+b2fA4408ysw/NbJeZnRMzbreZLQoes8oualRYV18t1a0rjRkTdiQAAAAAAKS90JJOZpYlabyk0yW1kXShmbWJmex/kvpJmhZnEducc0cFjzNLNVhAkmrXlq67Tpo509/NDgAAAAAAJBRmS6cOkr52zi1zzu2UNF3SWdETOOdWOOc+kbQnjACBfK67TqpZUxo7NuxIAAAAAABIa2EmnRpL+i7q+ffBsKLKNrOFZvaumfVObWhAAvXr+252Tz4pLVsWdjQAAAAAAKQtdsuPFwAAIABJREFUc86Fs2Jfo6m7c+6K4Pmlko53zg2KM+0USS84556NGtbYObfSzA6W9Jqk3zjnvomZ7ypJV0lSw4YNj50+fXqpbU9Z2rJli2rV+n/27jvOjqr+//jr7G56IwUSUiQJSYAkpEgKRSBIkaLBICJFv+APvshX4GtFAVEQBQQFvioqohCKYBAQCBCkFxXB0EMLBAKSGFqoCek5vz/OXe/uZjekzO7M7r6ej8d5zNyZuZvP5TyG3H3nnDOd8y6j1Wr71ltsf+ihvLb33jz/zW+ucd7+KS77ptjsn+Kyb4rN/iku+6bY7J/ism+Kzf4prrz6ZrfddnskxjiuvnNVTV1MDfOBATVe9y8dWycxxvml7UshhHuBscCLda65CLgIYNy4cXHSpEkbV3FB3HvvvbSUz9Js3XknfadOpe+FF0LfvrVO2T/FZd8Um/1TXPZNsdk/xWXfFJv9U1z2TbHZP8VVxL7Jc3rdTGBoCGFQCKEtcDCwTk+hCyF0DyG0K+33AnYCnmm0SqW6vvMdWLkSzjsv70okSZIkSSqk3EKnGONK4DjgNuBZ4E8xxqdDCKeHECYDhBDGhxDmAZ8HfhtCeLr09m2Ah0MITwD3AD+JMRo6qekMHgyHHAIXXggLF+ZdjSRJkiRJhZPn9DpijDOAGXWO/aDG/kzStLu673sA2LbRC5TW5sQT4cor4Ze/hNNOy7saSZIkSZIKJc/pdVLzNnIk7L8//OIX8MEHeVcjSZIkSVKhGDpJG+Okk+Cdd9I0O0mSJEmS9B+GTtLGmDgRdt8dzj0Xli7NuxpJkiRJkgrD0EnaWN/7Hrz+OkydmnclkiRJkiQVhqGTtLEmTYLtt4dzzoEVK/KuRpIkSZKkQjB0kjZWCHDyyfDyyzBtWt7VSJIkSZJUCIZOUhb22w+23RbOOgtWr867GkmSJEmScmfoJGWhoiI9ye7ZZ+l95515VyNJkiRJUu4MnaSsfP7zsP32DDvvPHjoobyrkSRJkiQpV4ZOUlaqquDGG1nesyd85jPw0kt5VyRJkiRJUm4MnaQsbbYZT551FqxaBfvuC2+/nXdFkiRJkiTlwtBJytiSj30MbrgB5s6FKVNg2bK8S5IkSZIkqckZOkmNYeed4dJL4f774ctf9ol2kiRJkqRWpyrvAqQW65BD4OWX4eSTYfBg+PGP865IkiRJkqQmY+gkNaYTT0zT7M44AwYOhKOOyrsiSZIkSZKahKGT1JhCgF/9Cv71LzjmGPjYx2CvvfKuSpIkSZKkRueaTlJja9MGrrkGRo6EAw+EJ57IuyJJkiRJkhqdoZPUFLp0gZtvhq5dYb/9YP78vCuSJEmSJKlRGTpJTaV/f7jlFnj//RQ8ffBB3hVJkiRJktRoDJ2kpjR6dJpq99RTcNBBsGJF3hVJkiRJktQoDJ2kpvapT8GFF8Jf/gLHHgsx5l2RJEmSJEmZ8+l1Uh6OOgrmzoUzz4TBg+HEE/OuSJIkSZKkTBk6SXn50Y9S8HTSSTBwIBx8cN4VSZIkSZKUGUMnKS8VFTB1KsybB4cfDv36wc47512VJEmSJEmZcE0nKU/t2sENN8CgQfDZz8Ls2XlXJEmSJElSJgydpLz16AEzZkBlJey7L7z5Zt4VSZIkSZK00QydpCIYPBhuugn+/W+YPBmWLMm7IkmSJEmSNoqhk1QUEyfCVVfBQw/BF78Iq1fnXZEkSZIkSRvM0EkqkilT4Lzz4M9/hhNOyLsaSZIkSZI2mE+vk4rma1+Dl15K4dOgQXDccXlXJEmSJEnSeluvkU4hhM4hhBdDCF9vrIKkVi8EOP/8tLbT176W1nqSJEmSJKmZWa/QKca4COgJLGqcciQB6Ul2V10FH/84HHww3H9/3hVJkiRJkrReNmRNpweBcVkXIqmOTp3g5pthiy1gn33g7rvzrkiSJEmSpHW2IaHTicBBIYQvhxBC1gVJqqF3b7jnHhg8GPbbD26/Pe+KJEmSJElaJxsSOp0HvAP8HngjhPBgCOHuOu2ubMuUWrHq4GmrrdI6TzNm5F2RJEmSJEkfaUNCp8Gl9/2LtLZTb2BQnTY4qwIlAb16pel1I0bAlCkwfXreFUmSJEmStFZV6/uGGOPARqhD0kfp0QPuugs+9Sn43Ofg6qvhgAPyrkqSJEmSpHptyEgnSXnZZJO0rtP48XDQQfCnP+VdkSRJkiRJ9VrvkU7VQghdgT0oT6V7CbgjxvhBFoVJakC3bnDbbWlh8UMOgRUr4LDD8q5KkiRJkqRaNih0CiEcBZwLdAaqn2AXgUUhhG/GGC/OqD5J9enSBW69FT79afjSl2DlSjj88LyrkiRJkiTpP9Z7el0IYTJwEfAm8A1gz1L7BvAGcFEI4TNZFimpHp06wS23wO67w5e/DBeb9UqSJEmSimNDRjp9B3gWmBhjXFTj+F0hhKnAg8B3gZsyqE/S2nTsCDfdlJ5od9RRaardMcfkXZUkSZIkSRu0kPho4NI6gRMApfWcLitdI6kptG8PN9yQptr9z//AL3+Zd0WSJEmSJG1Q6BQ+4nzckEIkbYR27eC669KIp//9XzjvvLwrkiRJkiS1chsSOj0BHBFC6FT3RAihM3BE6RpJTaltW7j6avj85+Fb34Kzz867IkmSJElSK7Yhazr9FPgz8GgI4RfAM6XjI4DjgSHAAdmUJ2m9tGkDV10FVVVw4omwfDl8//t5VyVJkiRJaoXWO3SKMd4QQjgOOBv4JeXpdAFYDBwXY7wxuxIlrZeqKrjiirT9wQ9g5Uo47TQIHzUzVpIkSZKk7GzISCdijL8OIVwF7AkMKh1+CbgjxvheVsVJ2kCVlTB1ahr5dPrp6al2Z5xh8CRJkiRJajLrFTqV1myaDlwZY7wYuKZRqpK08Sor4Xe/S8HTWWelqXY//anBkyRJkiSpSaxX6BRjXBRCGA9c2Uj1SMpSRQX85jcpeDr33DTV7vzzDZ4kSZIkSY1uQ6bXPQ5sk3UhkhpJCPCLX6Tg6fzz04inCy5IgZQkSZIkSY1kQ0KnU4HrQwi3xBjvybogSY0ghDTSqU0bOOccmDsXdt8dtt0WRo2CPn0c/SRJkiRJytSGhE5fBP4F3BlCeAJ4HviwzjUxxnjkxhYnKUMhwE9+At26wa9/DX/5S/lcz57lAGrbbVMbORI6dcqvXkmSJElSs7YhodMRNfbHlFpdETB0koomBDj55NTefhtmzYInn0zbWbPg4oth8eLytYMH1w6iRo2CLbdMi5RLkiRJkrQW6x06xRhdCEZqCXr0gF13Ta3a6tXw8su1g6gnn4Qbb0znADp0gOHD1wyjNtssl48hSZIkSSqm9QqdQgidgV8At8YYr2mckiTlpqIijW4aPBg++9ny8SVL4JlnagdRM2bA1Knla3bcEU45Bfbe2/WhJEmSJEnrFzrFGBeFEA4G/t5I9Ugqog4dYLvtUqvpjTdSCDVzJvzmN7DvvumaU06ByZN9Qp4kSZIktWIb8hvhM8DAjOuQ1Bxttll6Ct6JJ8ILL6Q1od59F6ZMgdGjYdo0WLUq7yolSZIkSTnYkNDpHOB/QgjDsi5GUjPWti38v/8Hzz0Hf/hDCpsOOSSt/3TZZbBiRd4VSpIkSZKa0IaETlsDrwKzQgjXhRB+EkL4QZ32/YzrlNRcVFXBYYfBU0/BtddCx45wxBEwbBj89rewbFneFUqSJEmSmsB6P70OOK3G/pQGronAjzbgZ0tqKSoq4HOfgwMOgFtugR/9CI45Jm2/8x046qgUSEmSJEmSWqQNGek0aB3a4KwKlNTMhQCf/jQ8+CDcfjtsuSV87WswaBCccw588EHeFUqSJEmSGsF6h04xxlfW1oA3gcrsS5XUrIUAe+4J992X2pgx8N3vwsCBafTTu+/mXaEkSZIkKUPrFDqFEJaHEA6u8bpLCGF6CGHbei6fAryQVYGSWqBddoHbbkujn3baCX7wA9hiCzjlFHjrrbyrkyRJkiRlYF1HOlXVubYt8Glg08wrktR6TJwI06fDY4/BXnvBmWem8Onb34YFC/KuTpIkSZK0ETZkTSdJytaYMXDNNemJdwccAOefn6bdTZ4Ml1zi6CdJkiRJaoYMnSQVx/DhcMUVMHs2fPWr8MQTcOSR0Ls3TJoE//d/8MoreVcpSZIkSVoHhk6SimfIkDTa6eWX4ZFH4Hvfg4UL4RvfSCOgPv7xtPj4rFkQY97VSpIkSZLqYegkqbhCSAHT6aengOmFF+Ccc6BDBzj1VBg1CoYOhRNOgL//HVavzrtiSZIkSVJJ1Xpcu28IoU9pvyMQgc+HEMbUuW67TCqTpLqGDEkB0wknwGuvpUXIr78efv5z+NnP0jS8/feHKVNgt92gXbu8K5YkSZKkVmt9QqdDS62mrzRwrfNdJDWuPn3g6KNTe+89uPXWFEBddRVcdBF07Qr77psCqH32gS5d8q5YkiRJklqVdQ2ddmvUKiRpY3TrBgcfnNrSpXDXXXDDDXDjjTBtGrRtC3vsQZ+RI2H8eOjUKe+KJUmSJKnFW6fQKcZ4X2MXIkmZaN8e9tsvtQsvhAceSAHU9dez9YwZ6dgXvwhf+UpaE0qSJEmS1ChcSFxSy1VZCTvvDOeeCy++yKO//GVa8+nii2H0aNhxR7jsMliyJO9KJUmSJKnFMXSS1DqEwPsjR8Lll8P8+XDeefD223DEEdC3L3z96/Dss3lXKUmSJEkthqGTpNanZ0/4xjdSyHTPPbD33vDrX8Pw4bDrrmkx8mXL8q5SkiRJkpo1QydJrVcIMGkS/PGPMG8enH12GgV12GHQrx+ccAK88ELeVUqSJElSs2ToJEkAm20G3/kOPP883HFHCqP+7/9g2DDYfXe45hpYvjzvKiVJkiSp2TB0kqSaKipgjz3g2mvhX/+CH/8YXnwRDjoIBgyAk0+GuXPzrlKSJEmSCs/QSZIasvnm8L3vpdBpxgzYfvs0BW/LLdM6UNdfD+++m0ZAxZh3tZIkSZJUKFV5FyBJhVdZCfvsk9q8eXDxxfC738EBB9S+pkMH6NgxtY3Z7949LXbesyf06AFt2uT32SVJkiRpAxk6SdL66N8fTj01jYC67ba0BtSHH6a2ZMma+0uWwHvvwYIFtY99+OG6PyGva9cUPlUHUevSunRJC6VLkiRJUk4MnSRpQ1RVwX77pbahVq2CpUtrB1WLF8M778DChQ23F19M23ffbfhnt2lTDqoGDIApU+DAA9NrSZIkSWoChk6SlJfKSujUKbUNsXLlRwdUCxfC00/DMcfAcceltagOPRQmT97wP1eSJEmS1oGhkyQ1V1VVsOmmqa1NjPDEE3DVVfDHP8LNN6fA6bOfTQHUnnu6bpQkSZKkzPn0Oklq6UKAMWPgnHPglVfg3nvhsMPSE/n22y89pe+rX4W//Q1Wr867WkmSJEkthKGTJLUmFRWw667w29/Ca6/B9Omwxx5w6aWw884weDCcdBLMmpV3pZIkSZKaOUMnSWqt2raFz3wGpk2D11+HK66A4cPhpz+FUaNg223hrLPg5ZfzrlSSJElSM2ToJEmCLl3gi19MU+4WLIALLoCuXeHkk2HQIPjEJ+DXv4Y338y7UkmSJEnNhAuJS5Jq23RTOPbY1ObOTSOhrrwyvf7f/4W99oLddkuhVOfOqXXpUv+2yr9mJEmSpNbK3wYkSQ0bNCit8XTiiWmdp+on4N1667q9v127hgOpOsd6VFTAuHHptSRJkqRmz9BJkvTRQkjrPI0aldZ5WrQotQ8+WHO/7rbusfffh3//u/ax5csZBXDqqbDTTmk01V57wdixafFzSZIkSc2OoZMkaf2EkEYodekCm2+ezc/88EMe/81vGPP663D77WktqZNPhl69YM89UwC1557Qr182f54kSZKkRmfoJEnKX8eOvLvddjBpEpxzDrz2Gtx5Zwqgbr89TekDGDGiPApql12gY8dcy5YkSZLUMOcsSJKKp0+f9DS9yy9PT9N74gn46U+hb9/0FL199oHu3WGPPVJI9fjjsHp13lVLkiRJqsHQSZJUbNXrSX3722nU0zvvwG23wfHHwxtvwHe/m9Z+2nzz2kGVJEmSpFw5vU6S1Lx06FCeYgdpUfI77ihPxbvyynR8221h331T22EHaNMmv5olSZKkVsiRTpKk5q1vXzj88BQ2vfYaPPoo/OQn0LMnnHsu7LorbLopHHQQXHppukaSJElSo3OkkySp5aioSFPtxo5N0+7efz8tSD5jRmrXXJOu+/jHy6OgJkyAysp865YkSZJaoFxHOoUQ9g4hzA4hzAkhnFjP+V1CCI+GEFaGEA6sc+7wEMILpXZ401UtSWo2unaFAw6A3/8e5s9PC46feWZ66t2ZZ8KOO0Lv3nDYYWmk1Ftv5V2xJEmS1GLkNtIphFAJ/ArYE5gHzAwhTI8xPlPjsn8BRwDfrvPeHsCpwDggAo+U3vtOU9QuSWqGQoDRo1M76SR4++20FtSMGXDrrXDVVemaiRPLo6DGjk2jpyRJkiSttzy/SU8A5sQYX4oxLgemAfvXvCDG+HKM8Umg7nOwPwXcEWN8uxQ03QHs3RRFS5JaiB494AtfgMsuS+s8/fOfcOqpsHp12o4bl9aL+vKX07S8d9/Nu2JJkiSpWclzTad+wKs1Xs8DJm7Ee/tlVJckqbWpqIDx41M79VR480247bY0CurGG9MC5JWVMHw4dOqUnqDXoQO0b1/eX9uxtR3v29cn60mSJKlFCjHGfP7gtEbT3jHGo0qvvwRMjDEeV8+1lwI3xxivLb3+NtA+xvjj0uvvA0tijD+r876jgaMBevfuvd20adMa8RM1nUWLFtG5c+e8y1AD7J/ism+Kraj9E1atosuzz9LzwQfpNHcuFcuXU7FsGZXLlqX90uv/HFuxYr1+/qq2bflg6615f/hw3hsxgvdHjGBF9+6N9Gk2TFH7Ron9U1z2TbHZP8Vl3xSb/VNcefXNbrvt9kiMcVx95/Ic6TQfGFDjdf/SsXV976Q677237kUxxouAiwDGjRsXJ02aVPeSZunee++lpXyWlsj+KS77ptgK3T+77w7HrfFvIvVbvRqWLYMlS2q3pUvXPPbhh1Q+/TSb/OMfbHLddVD9jyNbbgk77JDajjvCyJFQld9f2YXuG9k/BWbfFJv9U1z2TbHZP8VVxL7JM3SaCQwNIQwihUgHA4eu43tvA84MIVT/U/BewEnZlyhJ0nqqqChPnVsfS5fCo4/CAw/AP/4Bd94Jf/hDOtepU1rgvDqE2n77tCaVJEmSVGC5hU4xxpUhhONIAVIlcEmM8ekQwunAwzHG6SGE8cD1QHfgMyGEH8YYR8QY3w4h/IgUXAGcHmN8O5cPIklSFtq3T4HSjjum1zHCK6+UQ6h//AN+8hNYtSqd33rrcgi1ww6wzTY+aU+SJEmFkudIJ2KMM4AZdY79oMb+TNLUufreewlwSaMWKElSXkKAgQNTO7Q0EHjxYnj44XIQNX06TJ2aznXrlkZAbb99es/mm0OfPmnbq1dxAqkY0+do0wbatcu7GkmSJDWiXEMnSZK0Hjp1gl13TQ1SgDNnTjmEeuABOP30dLymykro3TuFUNVBVEPb9ZkWGCN8+CEsXAhvvVW71Xes+viyZSkEGzgQhg2DrbYqb7faCvr1S6GbJEmSmjVDJ0mSmqsQYOjQ1A4/PB378EN47TVYsKDh7WOPweuvp0XP6+radY0wauDChXDttfUHSUuXNlxbjx5plFWvXjBoEIwfDz17prZ4MTz/PMyeDX/9a3pdrWPH+sOoYcNSfZIkSWoWDJ0kSWpJOnaEwYNTW5tVq1JotLZw6uGHYcECBi5eDN27p7CoVy/o3x/GjCkHSjVb9TXdu6cRVusiRvj3v1MANXt2OYyaOROuuaZ2ONanz5pB1FZbpVCrTZsN/+8mSZKkzBk6SZLUGlVPuevd+yMvvfeuu5i0++6NV0sIaUpdv37wyU/WPrdsGbz4Yu0wavZsuP76FJpVq6qCLbdMAdTWW6dWve+T/iRJknJh6CRJktZuXUcsNYZ27WD48NTqevvtNcOo556DW2+FFSvK1/XqVTuEqt4OGpTCKkmSJDUKv2lJkqTmqUcP2GGH1GpauRJefjkFUNVB1OzZ6Wl/F19cvq5NGxgyZM3RUVttlaYHSpIkaaMYOkmSpJalqiqFSUOGwKc/Xftc9eiommHUc8/BzTensKraZpulEGrbbWHUqNRGjoTOnZv2s0iSJDVjhk6SJKn1aGh01IoVMHdu7TDqmWfgsstg0aLydVtuWTuIGjUqLdqe5xRESZKkgjJ0kiRJatMmPQlv2DD4zGfKx1evhldegVmz4Mkny2369PJT9Tp2hBEjagdR226bnuQnSZLUihk6SZIkNaSiIi04PmgQTJ5cPr5kSRoJVR1CzZoFN95Ye82ovn1rh1CjRqUpe23bNv3nkCRJyoGhkyRJ0vrq0AG22y61ajHC66+XQ6jqQOruu2H58nRNVVVaqHzkyNpt0CCn6EmSpBbH0EmSJCkLIUCfPqnttVf5+IoV8PzzKYB66qnUZs6Eq68uX9OhAwwfvmYY1a9f+rmSJEnNkKGTJElSY2rTJq35NGJE7eOLFsGzz6YQatastL399rR4ebVu3coB1LbblvddL0qSJDUDhk6SJEl56NwZxo9PraaFC+Hpp8ujop56Ko2K+u1vy9f06fOfAKp3hw5p/aihQx0VJUmSCsXQSZIkqUh69oRddkmtWoywYEHtIOqpp+Cii9jmww/hrLOge3eYMAG23x4mTkz7joiSJEk5MnSSJEkquhDSaKa+fWuvF7VqFTMvv5zxq1fDgw/CQw/Bj34Eq1en80OGpABq4sQURo0e7dPzJElSkzF0kiRJaq4qK1k8aBBMmgRHHpmOLVoEDz+cAqiHHkpPz7vyynSuXTsYO7YcRE2cmJ6c57Q8SZLUCAydJEmSWpLOnVMINWlSeh0jzJtXDqEeegguugh+/vN0ftNNa4dQ48fDJpvkVb0kSWpBDJ0kSZJashBgwIDUDjwwHVuxIq0JVTOIuvnm8nu6dy+/p3//8n716/79oUOHfD6PJElqNgydJEmSWps2bdI0u7Fj4Zhj0rH33oOZM+HRR+GVV9LoqFdfTYHUW2+t+TN69Wo4lBowAPr1S9P5JElSq2XoJEmSJOjWDfbYI7W6liyB+fNTCFWzzZuXAqq//Q3eeWfN9222WQqg+vRJI6Paty9va+6v77EOHaBjR9eikiSp4AydJEmStHYdOqQn4Q0Z0vA1ixeXR0dVB1LV+wsWwNKlKbxaurT2/obq3BmGDk1t2LDydtgw6NFjw3+uJEnKjKGTJEmSNl6nTrDVVqmtqxhh2bLaIVTdbUPn5s+H55+HRx6Ba6+F1avLP7dHj3IAVTOUGjo0hVWSJKlJGDpJkiQpHyGUp9BtzBPzli+HuXNTCPX88/DCC2l7991w+eW1r+3bt/7RUYMHuwaVJEkZM3SSJElS89a2bcOjrBYvhjlzykFU9faGG+DNN8vXhZDWnqq5GHrNbf/+KbBq06bpPpckSc2coZMkSZJark6dYPTo1Op6550UQlW36jWonn0Wbr8dFi2qfX1DwVTNfYMpSZL+w9BJkiRJrVP37jBhQmp1xQjvv19eFL16YfTq7UcFU/37M6J9exg7NgVS1a06mKrya7gkqeXzbztJkiSprhCgW7fURo6s/5rqYKpuIFXadpw9Gx57bM1gqqICNt+8PEKq5sip6v3NN4fKysb/nJIkNSJDJ0mSJGlD1AymRoxY4/TMe+9l0q67wnvvlQOpmuHUq6/CrFkwYwZ8+GHtN1dWphFRdcOoYcNg/HjYdNMm+pCSJG04QydJkiSpsYSQnsy3ySZrHzH1zju1w6ia4dSjj8L06bB0afk9Awem8GnChLTdbjvo3LlJPpIkSevK0EmSJEnKUwjQo0dqo0bVf02MsHAhPP00/POfMHNmatdck85XVMA225RDqPHj089q27bpPockSXUYOkmSJElFFwL06gW77ppatTfegIcfLgdRN90EU6emc23bwpgx5SBqwoQ0Pa+iIp/PIElqdQydJEmSpOZqs81g331TgzQi6pVXUgBVHURdeilccEE637VrmopXc0RUt26wciWsWpW2Nffrbtfl2OrV0K4dtG//0a1NmxSoSZJaJEMnSZIkqaUIIa33NHAgfP7z6diqVfDcc7WDqPPOgxUr8qy0bF3CqUGD4KtfbXhdLElSIRk6SZIkSS1ZZWV6ut6IEXDEEenYsmXwxBNpkfIlS6CqKl1X33Z9z4UAy5enhc8basuWrf18zbZwIdx3H/zmN7DnnvD1r8PeeztNUJKaAUMnSZIkqbVp1y5NsZswIe9K1s3ChXDRRWma4H77wdZbw9e+Bv/1X9CxY97VSZIa4D8PSJIkSSq2nj3hpJPg5Zfhyiuhc2f4n/+BAQPg5JNh/vy8K5Qk1cPQSZIkSVLz0KYNHHpoWpvqr3+FSZPg7LPTGlaHHZae5CdJKgxDJ0mSJEnNSwjwiU/AddfBnDlw/PFw003paXw77wx//nNaQF2SlCtDJ0mSJEnN16BB6Wl88+bB+eenqXaf+xwMGZJev/9+3hVKUqtl6CRJkiSp+evaNT3Z7oUX0gio/v3hm99M2298A+bOzbtCSWp1DJ0kSZIktRyVlXDAAWnNp5kzYfLk9NS7IUPKx2PMu0pJahUMnSRJkiS1TOPGwR/+kJ56993vwn33wS67wPjxbHH55TBtGjzyCLz3Xt6VSlKLVJV3AZIkSZLUqPr1gzPPhFNOgSuugAsuYNDUqTB1avmaTTeFoUPLbciQ8n6XLvnVLknNmKGTJEmSpNahY0f4ylfgK1/h/r/8hV36909rQNVsd94Jl11W+329e9cOoar3hwwxkJKktTB0kiRJktTqrG7fHkaOTK2uxYvhxRdTCDVnTjmQuu02uPTS2tf26ZPCp2GzTwd5AAAbz0lEQVTDYPvt0/S9YcMghCb5HJJUZIZOkiRJklRTp04walRqdS1aVA6kqtucOTB9OlxySbpms81S+LTLLrDrrinYqnA5XUmtj6GTJEmSJK2rzp1h9OjUaooRnn8e7r8/tfvug2uvTec22QR23rkcRI0dC23aNH3tktTEDJ0kSZIkaWOFAFttldp//3c69sor5RDq/vvhppvS8U6dYMcdyyHUhAnQvn1+tUtSIzF0kiRJkqTGsMUW8KUvpQawYAH89a/lEOr730/H27WDiRPLIdQOO6QRVZLUzBk6SZIkSVJT2HxzOOig1ADefhv+9rdyCHXWWfDjH0NlJWy3XVqYfIstoH9/GDAgtT59oMpf4yQ1D/7fSpIkSZLy0KMHTJ6cGsAHH8A//lEOoS6+OD1Jr6bKyhReDRhQDqNqhlL9+6dgqrKy6T+PJNVh6CRJkiRJRdClC+y1V2qQFid/912YNw9efTW16v158+CJJ+Dmm2HJkto/p7IS+vatHUTV3fbubTAlqdEZOkmSJElSEYUA3buntu229V8TI7zzzpqhVPX+I4/AjTfC0qW131c9Yqp//zVbv35p27cvtG3b+J9TUotl6CRJkiRJzVUIaZpejx4wenT918QICxeWg6j589O2us2aBbfeuuZUPkgjomoGUfUFVB07Nu5nlNRsGTpJkiRJUksWAvTqldrYsfVfEyO8/37tMKq6zZ8Pc+emJ++9886a7x0yBCZMSE/gmzgRxoxJT+ST1OoZOkmSJElSaxcCdOuW2ogRDV+3eHEKoapHS73yCjz6KNx7L1x1VbqmTZsUPFUHURMmwNChUFHRJB9FUnEYOkmSJEmS1k2nTjBsWGp1zZ8PDz2U2j//CZdeCr/6VTq3ySYpfKoZRG22WZOWLqnpGTpJkiRJkjZev35wwAGpAaxaBc8+Ww6hHnoIzjwTVq9O5wcOrD0tb+xY14eSWhhDJ0mSJElS9iorYeTI1I48Mh1bvDhNx6sOoh58EP70p/L1o0Yx9GMfS8HUrrumY5KaLUMnSZIkSVLT6NQJdt45tWqvvZYCqNJoqD633QY33pienHfQQfCFL8AOO7gmlNQMGTpJkiRJkvLTpw9Mnpwa8Pe//IVdFi2CadPgd7+DX/4SBgxI4dPBB8PHP54WPpdUeEbFkiRJkqTCWN2+PRx4IFx7Lbz+OlxxBYweDT//OYwblxYx//734emn8y5V0kcwdJIkSZIkFVPXrvDFL8JNN6VpeL//fVqA/Mwzy+tF/fjH8MILeVcqqR6GTpIkSZKk4uvRIy1Ifscd8O9/wwUXpGPf/34a/TRuHPzsZ/Cvf+VdqaQSQydJkiRJUvPSuzcceyzcf38Kmc49Ny00fsIJsMUW8IlPpFDqtdfyrlRq1QydJEmSJEnN14AB8M1vpqffzZkDZ5wBH3wAxx8P/frB7run9aAeeAA+/DDvaqVWxafXSZIkSZJahi23hJNPTu2ZZ+Dqq9NT8L7+9XS+ogKGD4fttkvT8bbbLi1S3rFjvnVLLZShkyRJkiSp5Rk+HH74QzjttLQG1COPwMMPp+2tt8Jll6XrKivLQVR1GDV6NHTokGv5Uktg6CRJkiRJarlCSNPs+vWDyZPTsRhh/vzaQdQtt8Cll6bzlZUwYkTtIGrUqMYNomJMtUotiKGTJEmSJKl1CQH6909t//3TsRhh3rzaQdTNN8PUqel8dRBVPS2vSxdYuhSWLEnb6lbz9fqcW7EC2reHTp2gc+f12zZ0rlcvR2wpV4ZOkiRJkiSFkBYlHzAAPvvZdCxGePXV2kHU9OlwySX1/4w2bVJw1L59Cnvq7nfvXvt4zXNt2qTwadEiWLy49nbevDWPr1r10Z+pbdu0kPqUKWmUV+/e2f33ktaBoZMkSZIkSfUJAT72sdSmTEnHqkdELVtWOzRq1w6qmuhX7BjTn7948ZoBVc3tM8/A9dfD0UfDV74CO+2UPseUKTBoUNPUqlbN0EmSJEmSpHVVPSIq7xqqR0r17Ln2a3/2M3jyyRQ+XX89fOtbqY0aVQ6gRo1yPSk1ioq8C5AkSZIkSY0khPQ0vtNOgyeegBdfhHPPha5d4fTTYcwY2HLLFET99a/rNm1PWkeGTpIkSZIktRaDB8M3v5kCpgUL4KKLYJtt4IILYJddoG9f+O//hhkz0hQ+aSMYOkmSJEmS1Br17p0CpltugTffhGnTYLfd4OqrYb/90tPvvvCFdPz99/OuVs2QazpJkiRJktTade2aAqYvfCGNcLrrrrQG1I03wp/+9J8n4Q3q3h3+/nfo3Bk6dWq4VZ/v0AEqMh7vsnp1qrGh1qULDBuW7Z+pDWLoJEmSJEmSytq1g333Te3CC+Ef//hPAPWxuXNT6LM+OnZsOJTq2BFWriwHRkuXrj1QWrYMVqz46D9zjz3glFPSlEEXSc+NoZMkSZIkSapfZSV84hOpnXsu991zD5N23BEWL67dFi1a81hDrfrat9+GDz+Eqqr0JL527VLr3r28X7fVvK6h9txzcN55MGkS7LRTCp8+9SnDpxwYOkmSJEmSpHUTQjnc6dEj72rqN3kyHH88XHIJnH027LMPbLddCp8mT85+up8a5H9pSZIkSZLUsnToAMceC3PmwO9/D+++C1OmwKhR8Mc/wqpVeVfYKhg6SZIkSZKklqltWzjyyDTl7sorIUY49FDYZps0Emr58rwrbNEMnSRJkiRJUstWVZXCplmz4Lrr0hPujjwShg6FX/86LWCuzBk6SZIkSZKk1qGiAg44AB5+GGbMgP790zS8QYPg3HPTIufKjKGTJEmSJElqXUJIC4z/7W9wzz0wYgR8+9swcCCccUZaA0obzdBJkiRJkiS1TiHApElw553wwAOw/fbpKXdbbJG2b72Vd4XNmqGTJEmSJEnSDjvAzTfDo4/CXnvBmWem8Olb34LZs2H16rwrbHYMnSRJkiRJkqqNHQvXXANPPw2f+xz8/Oew9dawySaw885w/PFw8cXwyCMuQP4RqvIuQJIkSZIkqXC22QYuvxx++EO4+2547DF4/HG49NLyguOVlem6MWNqt549cy29KAydJEmSJEmSGjJoEBx5ZPn16tXw0kspgKpu99wDf/hD+Zr+/WuHUGPHpkXKK1rXhDNDJ0mSJEmSpHVVUQFDhqR24IHl42++CU88UQ6iHnsMZsworwXVtSuMHl0OonbdFbbcMp/P0EQMnSRJkiRJkjbWppvCHnukVm3JEnjqqdqjoi65BBYvhrPOghNPzK/eJmDoJEmSJEmS1Bg6dIDx41Ortno1vPgidO6cX11NxNBJkiRJkiSpqVRUwNCheVfRJFrXClaSJEmSJElqEoZOkiRJkiRJypyhkyRJkiRJkjJn6CRJkiRJkqTMGTpJkiRJkiQpc7mGTiGEvUMIs0MIc0IIJ9Zzvl0I4erS+YdCCANLxweGEJaEEB4vtQubunZJkiRJkiQ1rCqvPziEUAn8CtgTmAfMDCFMjzE+U+OyI4F3YoxDQggHA2cDXyidezHGOKZJi5YkSZIkSdI6yXOk0wRgTozxpRjjcmAasH+da/YHLivtXwvsHkIITVijJEmSJEmSNkCeoVM/4NUar+eVjtV7TYxxJfAe0LN0blAI4bEQwn0hhJ0bu1hJkiRJkiStuxBjzOcPDuFAYO8Y41Gl118CJsYYj6txzVOla+aVXr8ITAQ+ADrHGBeGELYDbgBGxBjfr/NnHA0cDdC7d+/tpk2b1gSfrPEtWrSIzp07512GGmD/FJd9U2z2T3HZN8Vm/xSXfVNs9k9x2TfFZv8UV159s9tuuz0SYxxX37nc1nQC5gMDarzuXzpW3zXzQghVQDdgYUxJ2TKAGOMjpTBqGPBwzTfHGC8CLgIYN25cnDRpUiN8jKZ377330lI+S0tk/xSXfVNs9k9x2TfFZv8Ul31TbPZPcdk3xWb/FFcR+ybP6XUzgaEhhEEhhLbAwcD0OtdMBw4v7R8I3B1jjCGETUsLkRNCGAwMBV5qorolSZIkSZL0EXIb6RRjXBlCOA64DagELokxPh1COB14OMY4HbgYuCKEMAd4mxRMAewCnB5CWAGsBo6JMb7d9J9CkiRJkiRJ9clzeh0xxhnAjDrHflBjfynw+Xredx1wXaMXKEmSJEmSpA2S5/Q6SZIkSZIktVCGTpIkSZIkScqcoZMkSZIkSZIyZ+gkSZIkSZKkzBk6SZIkSZIkKXOGTpIkSZIkScqcoZMkSZIkSZIyZ+gkSZIkSZKkzBk6SZIkSZIkKXOGTpIkSZIkScqcoZMkSZIkSZIyZ+gkSZIkSZKkzBk6SZIkSZIkKXOGTpIkSZIkScqcoZMkSZIkSZIyZ+gkSZIkSZKkzBk6SZIkSZIkKXOGTpIkSZIkScqcoZMkSZIkSZIyZ+gkSZIkSZKkzBk6SZIkSZIkKXOGTpIkSZIkScqcoZMkSZIkSZIyZ+gkSZIkSZKkzBk6SZIkSZIkKXOGTpIkSZIkScqcoZMkSZIkSZIyZ+gkSZIkSZKkzBk6SZIkSZIkKXOGTpIkSZIkScqcoZMkSZIkSZIyZ+gkSZIkSZKkzBk6SZIkSZIkKXOGTpIkSZIkScqcoZMkSZIkSZIyZ+gkSZIkSZKkzBk6SZIkSZIkKXOGTpIkSZIkScqcoZMkSZIkSZIyZ+gkSZIkSZKkzBk6SZIkSZIkKXOGTpIkSZIkScqcoZMkSZIkSZIyZ+gkSZIkSZKkzBk6SZIkSZIkKXOGTpIkSZIkScqcoZMkSZIkSZIyZ+gkSZIkSZKkzBk6SZIkSZIkKXOGTpIkSZIkScqcoZMkSZIkSZIyZ+gkSZIkSZKkzBk6SZIkSZIkKXOGTpIkSZIkScqcoZMkSZIkSZIyZ+gkSZIkSZKkzBk6SZIkSZIkKXOGTpIkSZIkScqcoZMkSZIkSZIyZ+gkSZIkSZKkzBk6SZIkSZIkKXOGTpIkSZIkScqcoZMkSZIkSZIyZ+gkSZIkSZKkzBk6SZIkSZIkKXOGTpIkSZIkScqcoZMkSZIkSZIyZ+gkSZIkSZKkzBk6SZIkSZIkKXOGTpIkSZIkScqcoZMkSZIkSZIyZ+gkSZIkSZKkzBk6SZIkSZIkKXOGTpIkSZIkScqcoZMkSZIkSZIyZ+gkSZIkSZKkzBk6SZIkSZIkKXOGTpIkSZIkScqcoZMkSZIkSZIyZ+gkSZIkSZKkzBk6SZIkSZIkKXOGTpIkSZIkScqcoZMkSZIkSZIyZ+gkSZIkSZKkzBk6SZIkSZIkKXOGTpIkSZIkScqcoZMkSZIkSZIyZ+gkSZIkSZKkzBk6SZIkSZIkKXOGTpIkSZIkScqcoZMkSZIkSZIyZ+gkSZIkSZKkzBk6SZIkSZIkKXOGTpIkSZIkScqcoZMkSZIkSZIyZ+gkSZIkSZKkzBk6SZIkSZIkKXOGTpIkSZIkScqcoZMkSZIkSZIyZ+gkSZIkSZKkzOUaOoUQ9g4hzA4hzAkhnFjP+XYhhKtL5x8KIQysce6k0vHZIYRPNWXdkiRJkiRJWrvcQqcQQiXwK2AfYDhwSAhheJ3LjgTeiTEOAc4Hzi69dzhwMDAC2Bv4dennSZIkSZIkqQDyHOk0AZgTY3wpxrgcmAbsX+ea/YHLSvvXAruHEELp+LQY47IY41xgTunnSZIkSZIkqQDyDJ36Aa/WeD2vdKzea2KMK4H3gJ7r+F5JkiRJkiTlpCrvAhpTCOFo4OjSy0UhhNl51pOhXsBbeRehBtk/xWXfFJv9U1z2TbHZP8Vl3xSb/VNc9k2x2T/FlVffbNHQiTxDp/nAgBqv+5eO1XfNvBBCFdANWLiO7yXGeBFwUYY1F0II4eEY47i861D97J/ism+Kzf4pLvum2Oyf4rJvis3+KS77ptjsn+IqYt/kOb1uJjA0hDAohNCWtDD49DrXTAcOL+0fCNwdY4yl4weXnm43CBgK/LOJ6pYkSZIkSdJHyG2kU4xxZQjhOOA2oBK4JMb4dAjhdODhGON04GLgihDCHOBtUjBF6bo/Ac8AK4FjY4yrcvkgkiRJkiRJWkOuazrFGGcAM+oc+0GN/aXA5xt47xnAGY1aYHG1uCmDLYz9U1z2TbHZP8Vl3xSb/VNc9k2x2T/FZd8Um/1TXIXrm5Bmq0mSJEmSJEnZyXNNJ0mSJEmSJLVQhk7NTAhh7xDC7BDCnBDCiXnXo7IQwsshhFkhhMdDCA/nXU9rF0K4JITwRgjhqRrHeoQQ7gghvFDads+zxtasgf45LYQwv3QPPR5C2DfPGlurEMKAEMI9IYRnQghPhxC+Vjru/ZOztfSN904BhBDahxD+GUJ4otQ/PywdHxRCeKj03e3q0gN01ITW0jeXhhDm1rh3xuRda2sWQqgMITwWQri59Np7pyDq6RvvnYKo73fQon1nM3RqRkIIlcCvgH2A4cAhIYTh+ValOnaLMY4p2mMqW6lLgb3rHDsRuCvGOBS4q/Ra+biUNfsH4PzSPTSmtO6fmt5K4FsxxuHA9sCxpb9rvH/y11DfgPdOESwDPhljHA2MAfYOIWwPnE3qnyHAO8CROdbYWjXUNwAn1Lh3Hs+vRAFfA56t8dp7pzjq9g147xRJ3d9BC/WdzdCpeZkAzIkxvhRjXA5MA/bPuSapkGKM95OeelnT/sBlpf3LgM82aVH6jwb6RwUQY1wQY3y0tP8B6UtmP7x/creWvlEBxGRR6WWbUovAJ4FrS8e9d3Kwlr5RQYQQ+gP7Ab8vvQ547xRC3b5Rs1Co72yGTs1LP+DVGq/n4ZfNIonA7SGER0IIR+ddjOrVO8a4oLT/GtA7z2JUr+NCCE+Wpt85fStnIYSBwFjgIbx/CqVO34D3TiGUpqA8DrwB3AG8CLwbY1xZusTvbjmp2zcxxup754zSvXN+CKFdjiW2dv8HfAdYXXrdE++doqjbN9W8d4qhvt9BC/WdzdBJys4nYowfJ01/PDaEsEveBalhMT2603/lLJbfAFuSpj4sAM7Nt5zWLYTQGbgO+HqM8f2a57x/8lVP33jvFESMcVWMcQzQnzRCfeucS1JJ3b4JIYwETiL10XigB/DdHEtstUIInwbeiDE+knctqm0tfeO9Uxxr/R20CN/ZDJ2al/nAgBqv+5eOqQBijPNL2zeA60lfNlUsr4cQNgcobd/IuR7VEGN8vfRLwWrgd3gP5SaE0IYUalwZY/xz6bD3TwHU1zfeO8UTY3wXuAfYAdgkhFBVOuV3t5zV6Ju9S1NWY4xxGTAV75287ARMDiG8TFo+5JPAz/HeKYI1+iaE8AfvneJo4HfQQn1nM3RqXmYCQ0tPcmgLHAxMz7kmASGETiGELtX7wF7AU2t/l3IwHTi8tH84cGOOtaiO6r8cS6bgPZSL0joaFwPPxhjPq3HK+ydnDfWN904xhBA2DSFsUtrvAOxJWnfrHuDA0mXeOzlooG+eq/FLWSCteeK9k4MY40kxxv4xxoGk32/ujjEehvdO7hromy967xTDWn4HLdR3tqqPvkRFEWNcGUI4DrgNqAQuiTE+nXNZSnoD16f/71IFXBVj/Eu+JbVuIYQ/ApOAXiGEecCpwE+AP4UQjgReAQ7Kr8LWrYH+mVR65G4EXga+kluBrdtOwJeAWaX1TwBOxvunCBrqm0O8dwphc+Cy0tOGK4A/xRhvDiE8A0wLIfwYeIwUHKppNdQ3d4cQNgUC8DhwTJ5Fag3fxXunqK703imEen8HDSHMpEDf2UKa4idJkiRJkiRlx+l1kiRJkiRJypyhkyRJkiRJkjJn6CRJkiRJkqTMGTpJkiRJkiQpc4ZOkiRJkiRJypyhkyRJkiRJkjJn6CRJkgSEECaFEOJa2soa19Y9tzSE8EII4bwQQo96fna7EML/hhAeCCG8W7p+TgjhNyGEwWupKYQQDggh3BRCWBBCWF56/wMhhJNq/lkhhNNKtYz7iM/37TrHu4UQTgkhPF762YtCCHNDCDeEEI7asP+akiRJUJV3AZIkSQXzR2BGPcdX13n9OHBuab8HsC/wDWDPEMJ2McblACGE3sCtwFjgDuA0YBEwGjgCODyEcEiM8caaPzyE0BG4Gvg08AxwEfAK0BnYHvgBMAX+fzt3E2JVGcdx/PtfhCU1FuPCqEVEswtFI8mwRZuIjIpeTMmC6U0jojJoJIKYglwURa7MzE1hC62kF6WYIIjALCxJW6RGVmY0WhkZKcG/xTm37pzO6PXeO0ne7wcOz9zn7fzvbvjx3IfZ7X7RiOgDPgbOB9YDa4Aj5ee5wP3A6nb3lyRJvc3QSZIkaaytmflyC/P2VuatiIg3KUKia4F1ERHAOorAaXFmrmreICKeBd4HXomIizNzR9PwynKvp4GhzGwOvVZExNnAfcf53aruAgaABzLzuepgREzrcH9JktTD/HmdJElS97xTtheU7dXAZcC6auAEkJlfAUuA04DhRn9ETAduBTYDD1cCp8bafZn5SIf1DpTte3WDmflDh/tLkqQe5kknSZKksSZHxNSa/iOZ+esx1jZCnP1le2PZ/itwarIJ+A6YFxGTMvMwcEM59kJmZitFN5kyTv1Tavp2l+1gRAxl5p81cyRJktriSSdJkqSxhoHRmmdtZd4pETG1fAYi4kHgHuAg0Lif6cKy3Trey8pQ6VPgVP4JrRrrPmuj/pFx6t9QM3c18C2wFNgbEesjYigi5kaE/ydKkqSOeNJJkiRprFUU9zBVjVY+X1HTtw24OzN/LD/3le3BY7yzcYKqcRqpr9J/PO4Fvqzpn0FxP9TfMvPniLgIeAi4nuKEVeOU1dcRsTgz322jBkmSJEMnSZKkip2ZOdLCvI+AR8u/DwN7MvObypzmMOmno+xVDaca685ooY6qLZn5SbUzImp/OpeZo8AyYFlE9ANzgPnAIuD1iJiRmbvaqEOSJPU4j01LkiS1Z39mjpTPBzWBE8D2sp11jL1mAn8AOyvrZnahzpZl5oHMfCszbwOWA5OBBf9lDZIk6eRh6CRJkjRxXivbO8ebEBFXAucCG8tLxJvX3RERMYH1Hc3msj3nBL1fkiT9zxk6SZIkTZw3gA+BmyPi9upgRJwHPE9xyumxRn9mbgNeAi4FltcFTxExLSKe7KS4iJgTEWeOM3xd2X7RyTskSVLv8k4nSZKksWZFxKJxxjZk5m+tbpSZGRE3AZuAFyNiPrAROARMBwYp/h9bmJnbK8uXAGcBQ8C8iHgV2AOcDsymuPj789a/Vq1bgMGIeBvYAhwA+oGrgMspAqc1Hb5DkiT1KEMnSZKksRaWT50B4Lgu1c7MfRFxCUWItAB4ApgEfA+sBZ7KzN01636PiGsowqXBcn0/RWC1A3ic4pRUJ1YCv1AETEuBqRSXou8ChoFnMvNQh++QJEk9KjLzRNcgSZIkSZKkk4x3OkmSJEmSJKnrDJ0kSZIkSZLUdYZOkiRJkiRJ6jpDJ0mSJEmSJHWdoZMkSZIkSZK6ztBJkiRJkiRJXWfoJEmSJEmSpK4zdJIkSZIkSVLXGTpJkiRJkiSp6wydJEmSJEmS1HV/AX0jRAiYCewMAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYjYPWWZwXoc"
      },
      "outputs": [],
      "source": [
        "from testdatareader import TDataReader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "batch_size=1\n",
        "#test_data=TDataReader('train')\n",
        "num_batches= train_images.num_batches_of_size(batch_size)\n",
        "A=np.arange(1601,1700,1)  \n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    for vb in range(num_batches):\n",
        "            images,images_names = train_images.get_batch(batch_size)\n",
        "            predictions = sess.run([net], feed_dict={x:images})\n",
        "            p = predictions[0]\n",
        "            p_arr = (p * 255.0).astype(np.uint8)\n",
        "            p_arr=p_arr.reshape(224,224)\n",
        "            p_image = Image.fromarray(p_arr)\n",
        "            \n",
        "            #print(p_image.shape)\n",
        "            p_image.save('predictions/'+ str(vb) + '_prediction' + '.' + 'jpeg')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "image_blur_detection.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMFYLhavR/Qs0RYLndRErAx",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}