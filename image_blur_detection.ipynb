{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MasoudMoeini/Image-blur-detection/blob/main/image_blur_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-5xmsZ5RQes",
        "outputId": "1c71da5c-9705-41a9-a9dc-b5eeccc131d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ],
      "source": [
        "%load_ext tensorboard\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import numpy as np\n",
        "import matplotlib . pyplot as plt\n",
        "from tensorflow.keras import layers, losses\n",
        "from notebook.services.config import ConfigManager\n",
        "cm = ConfigManager().update('notebook', {'limit_output': 100})\n",
        "# Base CNN\n",
        "x = tf.placeholder(tf.float32, (None,224, 224, 3))\n",
        "y = tf.placeholder(tf.float32, (None,224, 224, 3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJfwkZX9TC3O"
      },
      "outputs": [],
      "source": [
        "#!unzip -qq BlurDatasetResultShi.zip\n",
        "#!unzip -qq ccv_data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "8pmv42pPJ4SW"
      },
      "outputs": [],
      "source": [
        "rm -rf ./logs/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BbR2mvcKA5S",
        "outputId": "5751e283-48b5-4104-ea69-5d2de2b39fcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.7/dist-packages/keras/legacy_tf_layers/convolutional.py:575: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: UserWarning: `tf.layers.max_pooling2d` is deprecated and will be removed in a future version. Please use `tf.keras.layers.MaxPooling2D` instead.\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/keras/legacy_tf_layers/pooling.py:600: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: `tf.layers.max_pooling2d` is deprecated and will be removed in a future version. Please use `tf.keras.layers.MaxPooling2D` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:34: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:39: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:44: UserWarning: `tf.layers.max_pooling2d` is deprecated and will be removed in a future version. Please use `tf.keras.layers.MaxPooling2D` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:49: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: `tf.layers.max_pooling2d` is deprecated and will be removed in a future version. Please use `tf.keras.layers.MaxPooling2D` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:68: UserWarning: `tf.layers.max_pooling2d` is deprecated and will be removed in a future version. Please use `tf.keras.layers.MaxPooling2D` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:79: UserWarning: `tf.layers.conv2d_transpose` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2DTranspose` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/keras/legacy_tf_layers/convolutional.py:1736: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:83: UserWarning: `tf.layers.conv2d_transpose` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2DTranspose` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:87: UserWarning: `tf.layers.conv2d_transpose` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2DTranspose` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:91: UserWarning: `tf.layers.conv2d_transpose` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2DTranspose` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:95: UserWarning: `tf.layers.conv2d_transpose` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2DTranspose` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:99: UserWarning: `tf.layers.conv2d_transpose` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2DTranspose` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:103: UserWarning: `tf.layers.conv2d_transpose` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2DTranspose` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:107: UserWarning: `tf.layers.conv2d_transpose` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2DTranspose` instead.\n"
          ]
        }
      ],
      "source": [
        "# Conv1\n",
        "# Input Tensor Shape: [batch_size, 224, 224, 3]\n",
        "# Output Tensor Shape: [batch_size, 224, 224, 32]\n",
        "conv1 = tf.layers.conv2d(x, filters=32, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu)\n",
        "\n",
        "# conv2\n",
        "# Input Tensor Shape: [batch_size, 224, 224, 32]\n",
        "# Output Tensor Shape: [batch_size, 224, 224, 32]\n",
        "conv2 = tf.layers.conv2d(conv1, filters=32, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu)\n",
        "\n",
        "# pool1\n",
        "# Input Tensor Shape: [batch_size, 224, 224, 32]\n",
        "# Output Tensor Shape: [batch_size, 112, 112, 32]\n",
        "pool1 = tf.layers.max_pooling2d(conv2, pool_size=[2,2], strides=2, padding=\"same\")\n",
        "\n",
        "#conv3\n",
        "# Input Tensor Shape: [batch_size, 112, 112, 32]\n",
        "# Output Tensor Shape: [batch_size, 112, 112, 64]\n",
        "conv3 = tf.layers.conv2d(pool1, filters=64, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu)\n",
        "\n",
        "#conv4\n",
        "# Input Tensor Shape: [batch_size, 112, 112, 64]\n",
        "# Output Tensor Shape: [batch_size, 112, 112, 64]\n",
        "conv4 = tf.layers.conv2d(conv3, filters=64, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu)\n",
        "\n",
        "#pool2\n",
        "# Input Tensor Shape: [batch_size, 112, 112, 64]\n",
        "# Output Tensor Shape: [batch_size, 56, 56, 64]\n",
        "pool2 = tf.layers.max_pooling2d(conv4, pool_size=[2,2], strides=2, padding=\"same\")\n",
        "\n",
        "#conv5\n",
        "# Input Tensor Shape: [batch_size, 56, 56, 64]\n",
        "# Output Tensor Shape: [batch_size, 56, 56, 128]\n",
        "conv5 = tf.layers.conv2d(pool2, filters=128, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu)\n",
        "\n",
        "#conv6\n",
        "# Input Tensor Shape: [batch_size, 56, 56, 128]\n",
        "# Output Tensor Shape: [batch_size, 56, 56, 128]\n",
        "conv6 = tf.layers.conv2d(conv5, filters=128, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu)\n",
        "\n",
        "#pool3\n",
        "# Input Tensor Shape: [batch_size, 56, 56, 128]\n",
        "# Output Tensor Shape: [batch_size, 28, 28, 128]\n",
        "pool3 = tf.layers.max_pooling2d(conv6, pool_size=[2,2], strides=2, padding=\"same\")\n",
        "\n",
        "#conv7\n",
        "# Input Tensor Shape: [batch_size, 28, 28, 128]\n",
        "# Output Tensor Shape: [batch_size, 28, 28, 256]\n",
        "conv7 = tf.layers.conv2d(pool3, filters=256, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu)\n",
        "#conv8\n",
        "# Input Tensor Shape: [batch_size, 28, 28, 256]\n",
        "# Output Tensor Shape: [batch_size, 28, 28, 256]\n",
        "conv8 = tf.layers.conv2d(conv7, filters=256, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu)\n",
        "\n",
        "#pool4\n",
        "# Input Tensor Shape: [batch_size, 28, 28, 256]\n",
        "# Output Tensor Shape: [batch_size, 14, 14, 256]\n",
        "\n",
        "pool4 = tf.layers.max_pooling2d(conv8, pool_size=[2,2], strides=2, padding=\"same\")\n",
        "#conv9\n",
        "# Input Tensor Shape: [batch_size, 14, 14, 256]\n",
        "# Output Tensor Shape: [batch_size, 14, 14, 512]\n",
        "conv9 = tf.layers.conv2d(pool4, filters=512, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu)\n",
        "\n",
        "#pool5\n",
        "# Input Tensor Shape: [batch_size, 14, 14, 512]\n",
        "# Output Tensor Shape: [batch_size, 7, 7, 512]\n",
        "pool5 = tf.layers.max_pooling2d(conv9, pool_size=[2,2], strides=2, padding=\"same\")\n",
        "\n",
        "#------------------------------------------decode---------------------------------------\n",
        "\n",
        "\n",
        "#dim = int(np.prod(pool5.get_shape()[1:])) #7*7*512\n",
        "#fcl = tf.reshape(pool5, shape=[-1, dim], name ='fc1')#[batch_size,7*7*512]\n",
        "# decoder\n",
        "\n",
        "# Input Tensor Shape: [batch_size, 7, 7, 512]\n",
        "# Output Tensor Shape: [batch_size, 14, 14, 512]\n",
        "net=tf.layers.conv2d_transpose(pool5,512,[3, 3],strides = 2,padding='SAME')\n",
        "\n",
        "# Input Tensor Shape: [batch_size, 14, 14, 512]\n",
        "# Output Tensor Shape: [batch_size, 28, 28, 256]\n",
        "net=tf.layers.conv2d_transpose(net,256,[3, 3],strides = 2,padding='SAME')\n",
        "\n",
        "# Input Tensor Shape: [batch_size, 28, 28, 256]\n",
        "# Output Tensor Shape: [batch_size, 56, 56, 128]\n",
        "net=tf.layers.conv2d_transpose(net,128,[3, 3],strides = 2,padding='SAME')\n",
        "\n",
        "# Input Tensor Shape: [batch_size, 56, 56, 128]\n",
        "# Output Tensor Shape: [batch_size, 112, 112, 128]\n",
        "net=tf.layers.conv2d_transpose(net,128,[3, 3],strides = 2,padding='SAME',activation = tf.nn.tanh)\n",
        "\n",
        "# Input Tensor Shape: [batch_size, 112, 112, 128]\n",
        "# Output Tensor Shape: [batch_size, 224, 224, 64]\n",
        "net=tf.layers.conv2d_transpose(net,64,[3, 3],strides = 2,padding='SAME',activation = tf.nn.tanh)\n",
        "\n",
        "# Input Tensor Shape: [batch_size, 224, 224, 64]\n",
        "# Output Tensor Shape: [batch_size, 224, 224, 32]\n",
        "net=tf.layers.conv2d_transpose(net,32,[3, 3],strides = 1,padding='SAME',activation = tf.nn.tanh)\n",
        "\n",
        "# Input Tensor Shape: [batch_size, 224, 224, 32]\n",
        "# Output Tensor Shape: [batch_size, 224, 224, 16]\n",
        "net=tf.layers.conv2d_transpose(net,16,[3, 3],strides = 1, padding='SAME', activation = tf.nn.tanh)\n",
        "\n",
        "# Input Tensor Shape: [batch_size, 224, 224, 16]\n",
        "# Output Tensor Shape: [batch_size, 224, 224, 3]\n",
        "net=tf.layers.conv2d_transpose(net,3,[3, 3],strides = 1, padding='SAME', activation = tf.nn.tanh)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "GNzgCa05zB0d"
      },
      "outputs": [],
      "source": [
        "#normlizing output\n",
        "#wmax = tf.reduce_max(net, axis=2, keepdims=True) # along width dimension\n",
        "#output_max = tf.reduce_max(wmax, axis=1, keepdims=True) # along height dimension\n",
        "#normalized_output = net / output_max\n",
        "#alpha = 1.1\n",
        "#loss = tf.reduce_mean( tf.square( (1.0/(alpha - y)) * (normalized_output - y) ))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "JkQpeiENg4tD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba96b8a5-74df-4a87-a4c6-b39594425119"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace train/.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n"
          ]
        }
      ],
      "source": [
        "!unzip -qq train.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "P-CIQbYmlE3j"
      },
      "outputs": [],
      "source": [
        "from datareader import DataReader\n",
        "train_images = DataReader('train')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "FBu6qF1NS4I6"
      },
      "outputs": [],
      "source": [
        "## Optimize\n",
        "batch_size=20\n",
        "num_batches= train_images.num_batches_of_size(batch_size)\n",
        "learning_rate = 0.0002\n",
        "n_epochs = 20\n",
        "loss = tf.reduce_mean(tf.square(net - y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "train  = optimizer.minimize(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPqqHoDRP0e_",
        "outputId": "6a0b177c-000d-47ba-badf-a43635cb5faf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch  0\n",
            "epoch 0 batch number 0    batch loss: 0.3480686843395233\n",
            "epoch 0 batch number 1    batch loss: 0.3693336248397827\n",
            "epoch 0 batch number 2    batch loss: 0.36101552844047546\n",
            "epoch 0 batch number 3    batch loss: 0.3739626109600067\n",
            "epoch 0 batch number 4    batch loss: 0.37600284814834595\n",
            "epoch 0 batch number 5    batch loss: 0.35219237208366394\n",
            "epoch 0 batch number 6    batch loss: 0.32842782139778137\n",
            "epoch 0 batch number 7    batch loss: 0.2914895713329315\n",
            "epoch 0 batch number 8    batch loss: 0.24817733466625214\n",
            "epoch 0 batch number 9    batch loss: 0.27422410249710083\n",
            "epoch 0 batch number 10    batch loss: 0.25481921434402466\n",
            "epoch 0 batch number 11    batch loss: 0.25684693455696106\n",
            "epoch 0 batch number 12    batch loss: 0.2194904088973999\n",
            "epoch 0 batch number 13    batch loss: 0.22987763583660126\n",
            "epoch 0 batch number 14    batch loss: 0.24600566923618317\n",
            "epoch 0 batch number 15    batch loss: 0.2392856478691101\n",
            "epoch 0 batch number 16    batch loss: 0.24735543131828308\n",
            "epoch 0 batch number 17    batch loss: 0.2159971445798874\n",
            "epoch 0 batch number 18    batch loss: 0.2218032330274582\n",
            "epoch 0 batch number 19    batch loss: 0.203151136636734\n",
            "epoch 0 batch number 20    batch loss: 0.2177724838256836\n",
            "epoch 0 batch number 21    batch loss: 0.21185435354709625\n",
            "epoch 0 batch number 22    batch loss: 0.20158687233924866\n",
            "epoch 0 batch number 23    batch loss: 0.2009458839893341\n",
            "epoch 0 batch number 24    batch loss: 0.21110069751739502\n",
            "epoch 0 batch number 25    batch loss: 0.20251311361789703\n",
            "epoch 0 batch number 26    batch loss: 0.21331407129764557\n",
            "epoch 0 batch number 27    batch loss: 0.20289961993694305\n",
            "epoch 0 batch number 28    batch loss: 0.18808332085609436\n",
            "epoch 0 batch number 29    batch loss: 0.20520387589931488\n",
            "epoch 0 batch number 30    batch loss: 0.2004626989364624\n",
            "epoch 0 batch number 31    batch loss: 0.18714174628257751\n",
            "epoch 0 batch number 32    batch loss: 0.1945965588092804\n",
            "epoch 0 batch number 33    batch loss: 0.1957574337720871\n",
            "epoch 0 batch number 34    batch loss: 0.20184050500392914\n",
            "epoch 0 batch number 35    batch loss: 0.18004541099071503\n",
            "epoch 0 batch number 36    batch loss: 0.18593013286590576\n",
            "epoch 0 batch number 37    batch loss: 0.19682593643665314\n",
            "epoch 0 batch number 38    batch loss: 0.19398090243339539\n",
            "epoch 0 batch number 39    batch loss: 0.189403235912323\n",
            "epoch 0 batch number 40    batch loss: 0.18503330647945404\n",
            "epoch 0 batch number 41    batch loss: 0.17450660467147827\n",
            "epoch 0 batch number 42    batch loss: 0.19669978320598602\n",
            "epoch 0 batch number 43    batch loss: 0.1870298981666565\n",
            "epoch 0 batch number 44    batch loss: 0.1774388998746872\n",
            "epoch 0 batch number 45    batch loss: 0.18021759390830994\n",
            "epoch 0 batch number 46    batch loss: 0.18784482777118683\n",
            "epoch 0 batch number 47    batch loss: 0.17784535884857178\n",
            "epoch 0 batch number 48    batch loss: 0.17311084270477295\n",
            "epoch 0 batch number 49    batch loss: 0.18925966322422028\n",
            " Average epoch losses: 0.22935545444488525 \n",
            "epoch  1\n",
            "epoch 1 batch number 0    batch loss: 0.17755226790905\n",
            "epoch 1 batch number 1    batch loss: 0.188750222325325\n",
            "epoch 1 batch number 2    batch loss: 0.16181518137454987\n",
            "epoch 1 batch number 3    batch loss: 0.1604050248861313\n",
            "epoch 1 batch number 4    batch loss: 0.18196792900562286\n",
            "epoch 1 batch number 5    batch loss: 0.1822642982006073\n",
            "epoch 1 batch number 6    batch loss: 0.16836580634117126\n",
            "epoch 1 batch number 7    batch loss: 0.17930281162261963\n",
            "epoch 1 batch number 8    batch loss: 0.16467353701591492\n",
            "epoch 1 batch number 9    batch loss: 0.1809086799621582\n",
            "epoch 1 batch number 10    batch loss: 0.16810886561870575\n",
            "epoch 1 batch number 11    batch loss: 0.1712973415851593\n",
            "epoch 1 batch number 12    batch loss: 0.16373790800571442\n",
            "epoch 1 batch number 13    batch loss: 0.15691062808036804\n",
            "epoch 1 batch number 14    batch loss: 0.1668621152639389\n",
            "epoch 1 batch number 15    batch loss: 0.16494184732437134\n",
            "epoch 1 batch number 16    batch loss: 0.16195276379585266\n",
            "epoch 1 batch number 17    batch loss: 0.15098725259304047\n",
            "epoch 1 batch number 18    batch loss: 0.15573862195014954\n",
            "epoch 1 batch number 19    batch loss: 0.14904575049877167\n",
            "epoch 1 batch number 20    batch loss: 0.1566169112920761\n",
            "epoch 1 batch number 21    batch loss: 0.16390763223171234\n",
            "epoch 1 batch number 22    batch loss: 0.1574709564447403\n",
            "epoch 1 batch number 23    batch loss: 0.14121966063976288\n",
            "epoch 1 batch number 24    batch loss: 0.14924034476280212\n",
            "epoch 1 batch number 25    batch loss: 0.1438753753900528\n",
            "epoch 1 batch number 26    batch loss: 0.1613798439502716\n",
            "epoch 1 batch number 27    batch loss: 0.14100360870361328\n",
            "epoch 1 batch number 28    batch loss: 0.15145209431648254\n",
            "epoch 1 batch number 29    batch loss: 0.15368346869945526\n",
            "epoch 1 batch number 30    batch loss: 0.14703629910945892\n",
            "epoch 1 batch number 31    batch loss: 0.14438311755657196\n",
            "epoch 1 batch number 32    batch loss: 0.1414392739534378\n",
            "epoch 1 batch number 33    batch loss: 0.14386974275112152\n",
            "epoch 1 batch number 34    batch loss: 0.13155528903007507\n",
            "epoch 1 batch number 35    batch loss: 0.13972248136997223\n",
            "epoch 1 batch number 36    batch loss: 0.16909225285053253\n",
            "epoch 1 batch number 37    batch loss: 0.1425359696149826\n",
            "epoch 1 batch number 38    batch loss: 0.14607374370098114\n",
            "epoch 1 batch number 39    batch loss: 0.14234034717082977\n",
            "epoch 1 batch number 40    batch loss: 0.14621764421463013\n",
            "epoch 1 batch number 41    batch loss: 0.1452445685863495\n",
            "epoch 1 batch number 42    batch loss: 0.14792029559612274\n",
            "epoch 1 batch number 43    batch loss: 0.14314529299736023\n",
            "epoch 1 batch number 44    batch loss: 0.13065582513809204\n",
            "epoch 1 batch number 45    batch loss: 0.14865072071552277\n",
            "epoch 1 batch number 46    batch loss: 0.15202870965003967\n",
            "epoch 1 batch number 47    batch loss: 0.14055109024047852\n",
            "epoch 1 batch number 48    batch loss: 0.13375550508499146\n",
            "epoch 1 batch number 49    batch loss: 0.14515048265457153\n",
            " Average epoch losses: 0.15513615310192108 \n",
            "epoch  2\n",
            "epoch 2 batch number 0    batch loss: 0.1372256875038147\n",
            "epoch 2 batch number 1    batch loss: 0.1485707014799118\n",
            "epoch 2 batch number 2    batch loss: 0.1285460740327835\n",
            "epoch 2 batch number 3    batch loss: 0.12691278755664825\n",
            "epoch 2 batch number 4    batch loss: 0.14411208033561707\n",
            "epoch 2 batch number 5    batch loss: 0.13338738679885864\n",
            "epoch 2 batch number 6    batch loss: 0.13739097118377686\n",
            "epoch 2 batch number 7    batch loss: 0.1514989137649536\n",
            "epoch 2 batch number 8    batch loss: 0.1369965374469757\n",
            "epoch 2 batch number 9    batch loss: 0.14342761039733887\n",
            "epoch 2 batch number 10    batch loss: 0.12554529309272766\n",
            "epoch 2 batch number 11    batch loss: 0.14308764040470123\n",
            "epoch 2 batch number 12    batch loss: 0.15441514551639557\n",
            "epoch 2 batch number 13    batch loss: 0.141942098736763\n",
            "epoch 2 batch number 14    batch loss: 0.13977433741092682\n",
            "epoch 2 batch number 15    batch loss: 0.1315462589263916\n",
            "epoch 2 batch number 16    batch loss: 0.13808956742286682\n",
            "epoch 2 batch number 17    batch loss: 0.13580933213233948\n",
            "epoch 2 batch number 18    batch loss: 0.1351541131734848\n",
            "epoch 2 batch number 19    batch loss: 0.12465174496173859\n",
            "epoch 2 batch number 20    batch loss: 0.13995960354804993\n",
            "epoch 2 batch number 21    batch loss: 0.12352826446294785\n",
            "epoch 2 batch number 22    batch loss: 0.13308601081371307\n",
            "epoch 2 batch number 23    batch loss: 0.1490028351545334\n",
            "epoch 2 batch number 24    batch loss: 0.13193629682064056\n",
            "epoch 2 batch number 25    batch loss: 0.12428770214319229\n",
            "epoch 2 batch number 26    batch loss: 0.14446574449539185\n",
            "epoch 2 batch number 27    batch loss: 0.1349538266658783\n",
            "epoch 2 batch number 28    batch loss: 0.12507452070713043\n",
            "epoch 2 batch number 29    batch loss: 0.11501676589250565\n",
            "epoch 2 batch number 30    batch loss: 0.13079850375652313\n",
            "epoch 2 batch number 31    batch loss: 0.12801454961299896\n",
            "epoch 2 batch number 32    batch loss: 0.10941708087921143\n",
            "epoch 2 batch number 33    batch loss: 0.12315833568572998\n",
            "epoch 2 batch number 34    batch loss: 0.12545740604400635\n",
            "epoch 2 batch number 35    batch loss: 0.13833527266979218\n",
            "epoch 2 batch number 36    batch loss: 0.12226444482803345\n",
            "epoch 2 batch number 37    batch loss: 0.11742132157087326\n",
            "epoch 2 batch number 38    batch loss: 0.12233194708824158\n",
            "epoch 2 batch number 39    batch loss: 0.13409030437469482\n",
            "epoch 2 batch number 40    batch loss: 0.1337316781282425\n",
            "epoch 2 batch number 41    batch loss: 0.11470349133014679\n",
            "epoch 2 batch number 42    batch loss: 0.12540151178836823\n",
            "epoch 2 batch number 43    batch loss: 0.12749801576137543\n",
            "epoch 2 batch number 44    batch loss: 0.11604539304971695\n",
            "epoch 2 batch number 45    batch loss: 0.12765492498874664\n",
            "epoch 2 batch number 46    batch loss: 0.12368053197860718\n",
            "epoch 2 batch number 47    batch loss: 0.14224307239055634\n",
            "epoch 2 batch number 48    batch loss: 0.12530829012393951\n",
            "epoch 2 batch number 49    batch loss: 0.11838346719741821\n",
            " Average epoch losses: 0.13170671463012695 \n",
            "epoch  3\n",
            "epoch 3 batch number 0    batch loss: 0.1256377100944519\n",
            "epoch 3 batch number 1    batch loss: 0.1354164034128189\n",
            "epoch 3 batch number 2    batch loss: 0.12701377272605896\n",
            "epoch 3 batch number 3    batch loss: 0.1342678964138031\n",
            "epoch 3 batch number 4    batch loss: 0.109415702521801\n",
            "epoch 3 batch number 5    batch loss: 0.1155528873205185\n",
            "epoch 3 batch number 6    batch loss: 0.11872927099466324\n",
            "epoch 3 batch number 7    batch loss: 0.12988340854644775\n",
            "epoch 3 batch number 8    batch loss: 0.1220756471157074\n",
            "epoch 3 batch number 9    batch loss: 0.120154470205307\n",
            "epoch 3 batch number 10    batch loss: 0.1327572762966156\n",
            "epoch 3 batch number 11    batch loss: 0.12984861433506012\n",
            "epoch 3 batch number 12    batch loss: 0.1033501997590065\n",
            "epoch 3 batch number 13    batch loss: 0.12411344051361084\n",
            "epoch 3 batch number 14    batch loss: 0.11600127071142197\n",
            "epoch 3 batch number 15    batch loss: 0.13039357960224152\n",
            "epoch 3 batch number 16    batch loss: 0.13188214600086212\n",
            "epoch 3 batch number 17    batch loss: 0.11920233815908432\n",
            "epoch 3 batch number 18    batch loss: 0.11519046872854233\n",
            "epoch 3 batch number 19    batch loss: 0.13216054439544678\n",
            "epoch 3 batch number 20    batch loss: 0.1285332888364792\n",
            "epoch 3 batch number 21    batch loss: 0.12609104812145233\n",
            "epoch 3 batch number 22    batch loss: 0.13441675901412964\n",
            "epoch 3 batch number 23    batch loss: 0.1304435431957245\n",
            "epoch 3 batch number 24    batch loss: 0.11837694048881531\n",
            "epoch 3 batch number 25    batch loss: 0.11173064261674881\n",
            "epoch 3 batch number 26    batch loss: 0.13469518721103668\n",
            "epoch 3 batch number 27    batch loss: 0.14383205771446228\n",
            "epoch 3 batch number 28    batch loss: 0.11791211366653442\n",
            "epoch 3 batch number 29    batch loss: 0.12751685082912445\n",
            "epoch 3 batch number 30    batch loss: 0.12024036049842834\n",
            "epoch 3 batch number 31    batch loss: 0.1147470474243164\n",
            "epoch 3 batch number 32    batch loss: 0.11482539772987366\n",
            "epoch 3 batch number 33    batch loss: 0.13591976463794708\n",
            "epoch 3 batch number 34    batch loss: 0.12092780321836472\n",
            "epoch 3 batch number 35    batch loss: 0.1299014389514923\n",
            "epoch 3 batch number 36    batch loss: 0.13559524714946747\n",
            "epoch 3 batch number 37    batch loss: 0.13071024417877197\n",
            "epoch 3 batch number 38    batch loss: 0.1261211782693863\n",
            "epoch 3 batch number 39    batch loss: 0.11639514565467834\n",
            "epoch 3 batch number 40    batch loss: 0.11578720808029175\n",
            "epoch 3 batch number 41    batch loss: 0.1155412495136261\n",
            "epoch 3 batch number 42    batch loss: 0.11030185222625732\n",
            "epoch 3 batch number 43    batch loss: 0.12690387666225433\n",
            "epoch 3 batch number 44    batch loss: 0.10787676274776459\n",
            "epoch 3 batch number 45    batch loss: 0.11807799339294434\n",
            "epoch 3 batch number 46    batch loss: 0.11562452465295792\n",
            "epoch 3 batch number 47    batch loss: 0.13270844519138336\n",
            "epoch 3 batch number 48    batch loss: 0.1099313348531723\n",
            "epoch 3 batch number 49    batch loss: 0.12482710182666779\n",
            " Average epoch losses: 0.12339118868112564 \n",
            "epoch  4\n",
            "epoch 4 batch number 0    batch loss: 0.12894022464752197\n",
            "epoch 4 batch number 1    batch loss: 0.1198577955365181\n",
            "epoch 4 batch number 2    batch loss: 0.12007761746644974\n",
            "epoch 4 batch number 3    batch loss: 0.10777903348207474\n",
            "epoch 4 batch number 4    batch loss: 0.1310177445411682\n",
            "epoch 4 batch number 5    batch loss: 0.10914155095815659\n",
            "epoch 4 batch number 6    batch loss: 0.1128200814127922\n",
            "epoch 4 batch number 7    batch loss: 0.11376288533210754\n",
            "epoch 4 batch number 8    batch loss: 0.12251263111829758\n",
            "epoch 4 batch number 9    batch loss: 0.12456341087818146\n",
            "epoch 4 batch number 10    batch loss: 0.10923893004655838\n",
            "epoch 4 batch number 11    batch loss: 0.11883650720119476\n",
            "epoch 4 batch number 12    batch loss: 0.11304286122322083\n",
            "epoch 4 batch number 13    batch loss: 0.10232508927583694\n",
            "epoch 4 batch number 14    batch loss: 0.10814746469259262\n",
            "epoch 4 batch number 15    batch loss: 0.11437225341796875\n",
            "epoch 4 batch number 16    batch loss: 0.1295459121465683\n",
            "epoch 4 batch number 17    batch loss: 0.1449328511953354\n",
            "epoch 4 batch number 18    batch loss: 0.12413755804300308\n",
            "epoch 4 batch number 19    batch loss: 0.13289575278759003\n",
            "epoch 4 batch number 20    batch loss: 0.12297870963811874\n",
            "epoch 4 batch number 21    batch loss: 0.12933330237865448\n",
            "epoch 4 batch number 22    batch loss: 0.1222185418009758\n",
            "epoch 4 batch number 23    batch loss: 0.12247870117425919\n",
            "epoch 4 batch number 24    batch loss: 0.11687906831502914\n",
            "epoch 4 batch number 25    batch loss: 0.1220313012599945\n",
            "epoch 4 batch number 26    batch loss: 0.10566866397857666\n",
            "epoch 4 batch number 27    batch loss: 0.1253442019224167\n",
            "epoch 4 batch number 28    batch loss: 0.11898695677518845\n",
            "epoch 4 batch number 29    batch loss: 0.11602311581373215\n",
            "epoch 4 batch number 30    batch loss: 0.12820467352867126\n",
            "epoch 4 batch number 31    batch loss: 0.10700638592243195\n",
            "epoch 4 batch number 32    batch loss: 0.11445420235395432\n",
            "epoch 4 batch number 33    batch loss: 0.12402927130460739\n",
            "epoch 4 batch number 34    batch loss: 0.11280453950166702\n",
            "epoch 4 batch number 35    batch loss: 0.12004279345273972\n",
            "epoch 4 batch number 36    batch loss: 0.10810136049985886\n",
            "epoch 4 batch number 37    batch loss: 0.11232905834913254\n",
            "epoch 4 batch number 38    batch loss: 0.1214873418211937\n",
            "epoch 4 batch number 39    batch loss: 0.12167930603027344\n",
            "epoch 4 batch number 40    batch loss: 0.1158488467335701\n",
            "epoch 4 batch number 41    batch loss: 0.11655425280332565\n",
            "epoch 4 batch number 42    batch loss: 0.12363842129707336\n",
            "epoch 4 batch number 43    batch loss: 0.11373640596866608\n",
            "epoch 4 batch number 44    batch loss: 0.10923347622156143\n",
            "epoch 4 batch number 45    batch loss: 0.12871740758419037\n",
            "epoch 4 batch number 46    batch loss: 0.12244009226560593\n",
            "epoch 4 batch number 47    batch loss: 0.11472024768590927\n",
            "epoch 4 batch number 48    batch loss: 0.13116686046123505\n",
            "epoch 4 batch number 49    batch loss: 0.1192040666937828\n",
            " Average epoch losses: 0.11910579353570938 \n",
            "epoch  5\n",
            "epoch 5 batch number 0    batch loss: 0.11949016898870468\n",
            "epoch 5 batch number 1    batch loss: 0.11036882549524307\n",
            "epoch 5 batch number 2    batch loss: 0.10984115302562714\n",
            "epoch 5 batch number 3    batch loss: 0.10011988133192062\n",
            "epoch 5 batch number 4    batch loss: 0.11305942386388779\n",
            "epoch 5 batch number 5    batch loss: 0.11978729814291\n",
            "epoch 5 batch number 6    batch loss: 0.1231277585029602\n",
            "epoch 5 batch number 7    batch loss: 0.10996481776237488\n",
            "epoch 5 batch number 8    batch loss: 0.09758235514163971\n",
            "epoch 5 batch number 9    batch loss: 0.12104222178459167\n",
            "epoch 5 batch number 10    batch loss: 0.11300516128540039\n",
            "epoch 5 batch number 11    batch loss: 0.11743135750293732\n",
            "epoch 5 batch number 12    batch loss: 0.11637495458126068\n",
            "epoch 5 batch number 13    batch loss: 0.11415909975767136\n",
            "epoch 5 batch number 14    batch loss: 0.11373009532690048\n",
            "epoch 5 batch number 15    batch loss: 0.1081828698515892\n",
            "epoch 5 batch number 16    batch loss: 0.10364043712615967\n",
            "epoch 5 batch number 17    batch loss: 0.10720592737197876\n",
            "epoch 5 batch number 18    batch loss: 0.1062183529138565\n",
            "epoch 5 batch number 19    batch loss: 0.10041020065546036\n",
            "epoch 5 batch number 20    batch loss: 0.1245623454451561\n",
            "epoch 5 batch number 21    batch loss: 0.11559238284826279\n",
            "epoch 5 batch number 22    batch loss: 0.10976963490247726\n",
            "epoch 5 batch number 23    batch loss: 0.10923854261636734\n",
            "epoch 5 batch number 24    batch loss: 0.11158079653978348\n",
            "epoch 5 batch number 25    batch loss: 0.11723337322473526\n",
            "epoch 5 batch number 26    batch loss: 0.12325777858495712\n",
            "epoch 5 batch number 27    batch loss: 0.11194530874490738\n",
            "epoch 5 batch number 28    batch loss: 0.12214402854442596\n",
            "epoch 5 batch number 29    batch loss: 0.09490647166967392\n",
            "epoch 5 batch number 30    batch loss: 0.10749714821577072\n",
            "epoch 5 batch number 31    batch loss: 0.1290537714958191\n",
            "epoch 5 batch number 32    batch loss: 0.10732683539390564\n",
            "epoch 5 batch number 33    batch loss: 0.10633636265993118\n",
            "epoch 5 batch number 34    batch loss: 0.11631190776824951\n",
            "epoch 5 batch number 35    batch loss: 0.09353404492139816\n",
            "epoch 5 batch number 36    batch loss: 0.11314807087182999\n",
            "epoch 5 batch number 37    batch loss: 0.1212550550699234\n",
            "epoch 5 batch number 38    batch loss: 0.12353045493364334\n",
            "epoch 5 batch number 39    batch loss: 0.11300521343946457\n",
            "epoch 5 batch number 40    batch loss: 0.1195426806807518\n",
            "epoch 5 batch number 41    batch loss: 0.1204453855752945\n",
            "epoch 5 batch number 42    batch loss: 0.12421339750289917\n",
            "epoch 5 batch number 43    batch loss: 0.10999441146850586\n",
            "epoch 5 batch number 44    batch loss: 0.10122239589691162\n",
            "epoch 5 batch number 45    batch loss: 0.11270204186439514\n",
            "epoch 5 batch number 46    batch loss: 0.12475189566612244\n",
            "epoch 5 batch number 47    batch loss: 0.1089443787932396\n",
            "epoch 5 batch number 48    batch loss: 0.11494925618171692\n",
            "epoch 5 batch number 49    batch loss: 0.10760536789894104\n",
            " Average epoch losses: 0.11280686408281326 \n",
            "epoch  6\n",
            "epoch 6 batch number 0    batch loss: 0.12210436165332794\n",
            "epoch 6 batch number 1    batch loss: 0.1030036062002182\n",
            "epoch 6 batch number 2    batch loss: 0.11631603538990021\n",
            "epoch 6 batch number 3    batch loss: 0.11321011185646057\n",
            "epoch 6 batch number 4    batch loss: 0.11728806048631668\n",
            "epoch 6 batch number 5    batch loss: 0.10813409835100174\n",
            "epoch 6 batch number 6    batch loss: 0.10391528904438019\n",
            "epoch 6 batch number 7    batch loss: 0.10428828001022339\n",
            "epoch 6 batch number 8    batch loss: 0.10869727283716202\n",
            "epoch 6 batch number 9    batch loss: 0.10143087059259415\n",
            "epoch 6 batch number 10    batch loss: 0.12057150155305862\n",
            "epoch 6 batch number 11    batch loss: 0.10496590286493301\n",
            "epoch 6 batch number 12    batch loss: 0.09997761994600296\n",
            "epoch 6 batch number 13    batch loss: 0.11325091868638992\n",
            "epoch 6 batch number 14    batch loss: 0.11599034816026688\n",
            "epoch 6 batch number 15    batch loss: 0.11119832843542099\n",
            "epoch 6 batch number 16    batch loss: 0.11584010720252991\n",
            "epoch 6 batch number 17    batch loss: 0.09868775308132172\n",
            "epoch 6 batch number 18    batch loss: 0.11670136451721191\n",
            "epoch 6 batch number 19    batch loss: 0.11532504856586456\n",
            "epoch 6 batch number 20    batch loss: 0.10527417808771133\n",
            "epoch 6 batch number 21    batch loss: 0.11171972751617432\n",
            "epoch 6 batch number 22    batch loss: 0.1149941235780716\n",
            "epoch 6 batch number 23    batch loss: 0.11442742496728897\n",
            "epoch 6 batch number 24    batch loss: 0.1342504769563675\n",
            "epoch 6 batch number 25    batch loss: 0.10302786529064178\n",
            "epoch 6 batch number 26    batch loss: 0.10625407099723816\n",
            "epoch 6 batch number 27    batch loss: 0.09885997325181961\n",
            "epoch 6 batch number 28    batch loss: 0.1141928881406784\n",
            "epoch 6 batch number 29    batch loss: 0.111717589199543\n",
            "epoch 6 batch number 30    batch loss: 0.11441165953874588\n",
            "epoch 6 batch number 31    batch loss: 0.10656468570232391\n",
            "epoch 6 batch number 32    batch loss: 0.10638981312513351\n",
            "epoch 6 batch number 33    batch loss: 0.09535805881023407\n",
            "epoch 6 batch number 34    batch loss: 0.10206055641174316\n",
            "epoch 6 batch number 35    batch loss: 0.11338238418102264\n",
            "epoch 6 batch number 36    batch loss: 0.11235640197992325\n",
            "epoch 6 batch number 37    batch loss: 0.10719877481460571\n",
            "epoch 6 batch number 38    batch loss: 0.10770835727453232\n",
            "epoch 6 batch number 39    batch loss: 0.118329718708992\n",
            "epoch 6 batch number 40    batch loss: 0.09450866281986237\n",
            "epoch 6 batch number 41    batch loss: 0.10860206186771393\n",
            "epoch 6 batch number 42    batch loss: 0.1029847040772438\n",
            "epoch 6 batch number 43    batch loss: 0.10189817100763321\n",
            "epoch 6 batch number 44    batch loss: 0.1144164502620697\n",
            "epoch 6 batch number 45    batch loss: 0.09422998875379562\n",
            "epoch 6 batch number 46    batch loss: 0.10838974267244339\n",
            "epoch 6 batch number 47    batch loss: 0.10652509331703186\n",
            "epoch 6 batch number 48    batch loss: 0.11556914448738098\n",
            "epoch 6 batch number 49    batch loss: 0.11231276392936707\n",
            " Average epoch losses: 0.10937625169754028 \n",
            "epoch  7\n",
            "epoch 7 batch number 0    batch loss: 0.1026519387960434\n",
            "epoch 7 batch number 1    batch loss: 0.1078125610947609\n",
            "epoch 7 batch number 2    batch loss: 0.09593499451875687\n",
            "epoch 7 batch number 3    batch loss: 0.0968276783823967\n",
            "epoch 7 batch number 4    batch loss: 0.10416274517774582\n",
            "epoch 7 batch number 5    batch loss: 0.09609892219305038\n",
            "epoch 7 batch number 6    batch loss: 0.09801602363586426\n",
            "epoch 7 batch number 7    batch loss: 0.11383818835020065\n",
            "epoch 7 batch number 8    batch loss: 0.0906253457069397\n",
            "epoch 7 batch number 9    batch loss: 0.09941377490758896\n",
            "epoch 7 batch number 10    batch loss: 0.11306033283472061\n",
            "epoch 7 batch number 11    batch loss: 0.10756522417068481\n",
            "epoch 7 batch number 12    batch loss: 0.12064789980649948\n",
            "epoch 7 batch number 13    batch loss: 0.08746694773435593\n",
            "epoch 7 batch number 14    batch loss: 0.10368888080120087\n",
            "epoch 7 batch number 15    batch loss: 0.10417740792036057\n",
            "epoch 7 batch number 16    batch loss: 0.10326474905014038\n",
            "epoch 7 batch number 17    batch loss: 0.10634642094373703\n",
            "epoch 7 batch number 18    batch loss: 0.09881935268640518\n",
            "epoch 7 batch number 19    batch loss: 0.09997577965259552\n",
            "epoch 7 batch number 20    batch loss: 0.0993388444185257\n",
            "epoch 7 batch number 21    batch loss: 0.10638203471899033\n",
            "epoch 7 batch number 22    batch loss: 0.09539282321929932\n",
            "epoch 7 batch number 23    batch loss: 0.1013108417391777\n",
            "epoch 7 batch number 24    batch loss: 0.10531872510910034\n",
            "epoch 7 batch number 25    batch loss: 0.12078706175088882\n",
            "epoch 7 batch number 26    batch loss: 0.09470763802528381\n",
            "epoch 7 batch number 27    batch loss: 0.12225864827632904\n",
            "epoch 7 batch number 28    batch loss: 0.09674663096666336\n",
            "epoch 7 batch number 29    batch loss: 0.11796300113201141\n",
            "epoch 7 batch number 30    batch loss: 0.11033820360898972\n",
            "epoch 7 batch number 31    batch loss: 0.11803003400564194\n",
            "epoch 7 batch number 32    batch loss: 0.1070384755730629\n",
            "epoch 7 batch number 33    batch loss: 0.11591808497905731\n",
            "epoch 7 batch number 34    batch loss: 0.11195152252912521\n",
            "epoch 7 batch number 35    batch loss: 0.10381066054105759\n",
            "epoch 7 batch number 36    batch loss: 0.10548143088817596\n",
            "epoch 7 batch number 37    batch loss: 0.10636521130800247\n",
            "epoch 7 batch number 38    batch loss: 0.11111278831958771\n",
            "epoch 7 batch number 39    batch loss: 0.10768973082304001\n",
            "epoch 7 batch number 40    batch loss: 0.11146368086338043\n",
            "epoch 7 batch number 41    batch loss: 0.11238104104995728\n",
            "epoch 7 batch number 42    batch loss: 0.10199127346277237\n",
            "epoch 7 batch number 43    batch loss: 0.11275807023048401\n",
            "epoch 7 batch number 44    batch loss: 0.10559696704149246\n",
            "epoch 7 batch number 45    batch loss: 0.10324422270059586\n",
            "epoch 7 batch number 46    batch loss: 0.13107959926128387\n",
            "epoch 7 batch number 47    batch loss: 0.11787699908018112\n",
            "epoch 7 batch number 48    batch loss: 0.11108802258968353\n",
            "epoch 7 batch number 49    batch loss: 0.11297308653593063\n",
            " Average epoch losses: 0.10657581686973572 \n",
            "epoch  8\n",
            "epoch 8 batch number 0    batch loss: 0.10383669286966324\n",
            "epoch 8 batch number 1    batch loss: 0.10649129748344421\n",
            "epoch 8 batch number 2    batch loss: 0.12561747431755066\n",
            "epoch 8 batch number 3    batch loss: 0.10367199033498764\n",
            "epoch 8 batch number 4    batch loss: 0.11480515450239182\n",
            "epoch 8 batch number 5    batch loss: 0.1210237368941307\n",
            "epoch 8 batch number 6    batch loss: 0.10256502032279968\n",
            "epoch 8 batch number 7    batch loss: 0.10318521410226822\n",
            "epoch 8 batch number 8    batch loss: 0.1153680831193924\n",
            "epoch 8 batch number 9    batch loss: 0.09676390886306763\n",
            "epoch 8 batch number 10    batch loss: 0.10475753247737885\n",
            "epoch 8 batch number 11    batch loss: 0.1162019819021225\n",
            "epoch 8 batch number 12    batch loss: 0.09826884418725967\n",
            "epoch 8 batch number 13    batch loss: 0.10973936319351196\n",
            "epoch 8 batch number 14    batch loss: 0.09731432795524597\n",
            "epoch 8 batch number 15    batch loss: 0.10460814088582993\n",
            "epoch 8 batch number 16    batch loss: 0.09274551272392273\n",
            "epoch 8 batch number 17    batch loss: 0.10181806981563568\n",
            "epoch 8 batch number 18    batch loss: 0.11236976832151413\n",
            "epoch 8 batch number 19    batch loss: 0.10464152693748474\n",
            "epoch 8 batch number 20    batch loss: 0.10411915928125381\n",
            "epoch 8 batch number 21    batch loss: 0.11045438796281815\n",
            "epoch 8 batch number 22    batch loss: 0.09957664459943771\n",
            "epoch 8 batch number 23    batch loss: 0.09311340749263763\n",
            "epoch 8 batch number 24    batch loss: 0.0962165966629982\n",
            "epoch 8 batch number 25    batch loss: 0.10463976114988327\n",
            "epoch 8 batch number 26    batch loss: 0.09817209094762802\n",
            "epoch 8 batch number 27    batch loss: 0.09946376085281372\n",
            "epoch 8 batch number 28    batch loss: 0.09475697576999664\n",
            "epoch 8 batch number 29    batch loss: 0.092971071600914\n",
            "epoch 8 batch number 30    batch loss: 0.11080830544233322\n",
            "epoch 8 batch number 31    batch loss: 0.10292311012744904\n",
            "epoch 8 batch number 32    batch loss: 0.11142204701900482\n",
            "epoch 8 batch number 33    batch loss: 0.1044980138540268\n",
            "epoch 8 batch number 34    batch loss: 0.11064907908439636\n",
            "epoch 8 batch number 35    batch loss: 0.10456619411706924\n",
            "epoch 8 batch number 36    batch loss: 0.11197308450937271\n",
            "epoch 8 batch number 37    batch loss: 0.10985033959150314\n",
            "epoch 8 batch number 38    batch loss: 0.10106408596038818\n",
            "epoch 8 batch number 39    batch loss: 0.09594210982322693\n",
            "epoch 8 batch number 40    batch loss: 0.10223531723022461\n",
            "epoch 8 batch number 41    batch loss: 0.11281384527683258\n",
            "epoch 8 batch number 42    batch loss: 0.09334118664264679\n",
            "epoch 8 batch number 43    batch loss: 0.09270943701267242\n",
            "epoch 8 batch number 44    batch loss: 0.09495113790035248\n",
            "epoch 8 batch number 45    batch loss: 0.11354951560497284\n",
            "epoch 8 batch number 46    batch loss: 0.1312883496284485\n",
            "epoch 8 batch number 47    batch loss: 0.10057564824819565\n",
            "epoch 8 batch number 48    batch loss: 0.10101804882287979\n",
            "epoch 8 batch number 49    batch loss: 0.11486222594976425\n",
            " Average epoch losses: 0.10500636696815491 \n",
            "epoch  9\n",
            "epoch 9 batch number 0    batch loss: 0.10097479820251465\n",
            "epoch 9 batch number 1    batch loss: 0.09970264881849289\n",
            "epoch 9 batch number 2    batch loss: 0.09074436873197556\n",
            "epoch 9 batch number 3    batch loss: 0.10461822152137756\n",
            "epoch 9 batch number 4    batch loss: 0.10223720967769623\n",
            "epoch 9 batch number 5    batch loss: 0.10205056518316269\n",
            "epoch 9 batch number 6    batch loss: 0.09422396868467331\n",
            "epoch 9 batch number 7    batch loss: 0.10864368081092834\n",
            "epoch 9 batch number 8    batch loss: 0.09999050199985504\n",
            "epoch 9 batch number 9    batch loss: 0.1012556329369545\n",
            "epoch 9 batch number 10    batch loss: 0.10504311323165894\n",
            "epoch 9 batch number 11    batch loss: 0.10655874758958817\n",
            "epoch 9 batch number 12    batch loss: 0.0988667830824852\n",
            "epoch 9 batch number 13    batch loss: 0.09311361610889435\n",
            "epoch 9 batch number 14    batch loss: 0.10526003688573837\n",
            "epoch 9 batch number 15    batch loss: 0.09788927435874939\n",
            "epoch 9 batch number 16    batch loss: 0.1066732183098793\n",
            "epoch 9 batch number 17    batch loss: 0.09278830140829086\n",
            "epoch 9 batch number 18    batch loss: 0.10705165565013885\n",
            "epoch 9 batch number 19    batch loss: 0.09469551593065262\n",
            "epoch 9 batch number 20    batch loss: 0.09360422194004059\n",
            "epoch 9 batch number 21    batch loss: 0.11344622820615768\n",
            "epoch 9 batch number 22    batch loss: 0.10830727964639664\n",
            "epoch 9 batch number 23    batch loss: 0.11662086844444275\n",
            "epoch 9 batch number 24    batch loss: 0.09562703967094421\n",
            "epoch 9 batch number 25    batch loss: 0.09948410838842392\n",
            "epoch 9 batch number 26    batch loss: 0.10537431389093399\n",
            "epoch 9 batch number 27    batch loss: 0.11526048928499222\n",
            "epoch 9 batch number 28    batch loss: 0.0976589247584343\n",
            "epoch 9 batch number 29    batch loss: 0.10491251945495605\n",
            "epoch 9 batch number 30    batch loss: 0.10824857652187347\n",
            "epoch 9 batch number 31    batch loss: 0.10337425768375397\n",
            "epoch 9 batch number 32    batch loss: 0.13056212663650513\n",
            "epoch 9 batch number 33    batch loss: 0.1127907857298851\n",
            "epoch 9 batch number 34    batch loss: 0.10992402583360672\n",
            "epoch 9 batch number 35    batch loss: 0.09693530946969986\n",
            "epoch 9 batch number 36    batch loss: 0.1136993020772934\n",
            "epoch 9 batch number 37    batch loss: 0.11220642179250717\n",
            "epoch 9 batch number 38    batch loss: 0.10873569548130035\n",
            "epoch 9 batch number 39    batch loss: 0.10136625170707703\n",
            "epoch 9 batch number 40    batch loss: 0.11119969189167023\n",
            "epoch 9 batch number 41    batch loss: 0.09409868717193604\n",
            "epoch 9 batch number 42    batch loss: 0.12029150873422623\n",
            "epoch 9 batch number 43    batch loss: 0.0955471321940422\n",
            "epoch 9 batch number 44    batch loss: 0.09417104721069336\n",
            "epoch 9 batch number 45    batch loss: 0.09466349333524704\n",
            "epoch 9 batch number 46    batch loss: 0.10258418321609497\n",
            "epoch 9 batch number 47    batch loss: 0.09850318729877472\n",
            "epoch 9 batch number 48    batch loss: 0.10754556208848953\n",
            "epoch 9 batch number 49    batch loss: 0.11744978278875351\n",
            " Average epoch losses: 0.1039314940571785 \n",
            "epoch  10\n",
            "epoch 10 batch number 0    batch loss: 0.11179589480161667\n",
            "epoch 10 batch number 1    batch loss: 0.1042168140411377\n",
            "epoch 10 batch number 2    batch loss: 0.09654196351766586\n",
            "epoch 10 batch number 3    batch loss: 0.10444168746471405\n",
            "epoch 10 batch number 4    batch loss: 0.11059045046567917\n",
            "epoch 10 batch number 5    batch loss: 0.10802250355482101\n",
            "epoch 10 batch number 6    batch loss: 0.09023220837116241\n",
            "epoch 10 batch number 7    batch loss: 0.13650700449943542\n",
            "epoch 10 batch number 8    batch loss: 0.10350983589887619\n",
            "epoch 10 batch number 9    batch loss: 0.10780099779367447\n",
            "epoch 10 batch number 10    batch loss: 0.10523466020822525\n",
            "epoch 10 batch number 11    batch loss: 0.09866039454936981\n",
            "epoch 10 batch number 12    batch loss: 0.08905301243066788\n",
            "epoch 10 batch number 13    batch loss: 0.10730082541704178\n",
            "epoch 10 batch number 14    batch loss: 0.09301691502332687\n",
            "epoch 10 batch number 15    batch loss: 0.10015420615673065\n",
            "epoch 10 batch number 16    batch loss: 0.10994768887758255\n",
            "epoch 10 batch number 17    batch loss: 0.0960286483168602\n",
            "epoch 10 batch number 18    batch loss: 0.09403588622808456\n",
            "epoch 10 batch number 19    batch loss: 0.10303270816802979\n",
            "epoch 10 batch number 20    batch loss: 0.09193296730518341\n",
            "epoch 10 batch number 21    batch loss: 0.10657288134098053\n",
            "epoch 10 batch number 22    batch loss: 0.09811386466026306\n",
            "epoch 10 batch number 23    batch loss: 0.10040541738271713\n",
            "epoch 10 batch number 24    batch loss: 0.1243276447057724\n",
            "epoch 10 batch number 25    batch loss: 0.10618388652801514\n",
            "epoch 10 batch number 26    batch loss: 0.10743476450443268\n",
            "epoch 10 batch number 27    batch loss: 0.09835323691368103\n",
            "epoch 10 batch number 28    batch loss: 0.09545914828777313\n",
            "epoch 10 batch number 29    batch loss: 0.1083499863743782\n",
            "epoch 10 batch number 30    batch loss: 0.09823779761791229\n",
            "epoch 10 batch number 31    batch loss: 0.09141179174184799\n",
            "epoch 10 batch number 32    batch loss: 0.10464715212583542\n",
            "epoch 10 batch number 33    batch loss: 0.112682044506073\n",
            "epoch 10 batch number 34    batch loss: 0.10737593472003937\n",
            "epoch 10 batch number 35    batch loss: 0.10083968192338943\n",
            "epoch 10 batch number 36    batch loss: 0.10950693488121033\n",
            "epoch 10 batch number 37    batch loss: 0.08233702927827835\n",
            "epoch 10 batch number 38    batch loss: 0.107118159532547\n",
            "epoch 10 batch number 39    batch loss: 0.0989231988787651\n",
            "epoch 10 batch number 40    batch loss: 0.1088152825832367\n",
            "epoch 10 batch number 41    batch loss: 0.10402238368988037\n",
            "epoch 10 batch number 42    batch loss: 0.10713116824626923\n",
            "epoch 10 batch number 43    batch loss: 0.1156436875462532\n",
            "epoch 10 batch number 44    batch loss: 0.11797655373811722\n",
            "epoch 10 batch number 45    batch loss: 0.10642989724874496\n",
            "epoch 10 batch number 46    batch loss: 0.0977809876203537\n",
            "epoch 10 batch number 47    batch loss: 0.1067790612578392\n",
            "epoch 10 batch number 48    batch loss: 0.09910523146390915\n",
            "epoch 10 batch number 49    batch loss: 0.10273484885692596\n",
            " Average epoch losses: 0.10373514145612717 \n",
            "epoch  11\n",
            "epoch 11 batch number 0    batch loss: 0.11597982048988342\n",
            "epoch 11 batch number 1    batch loss: 0.09597625583410263\n",
            "epoch 11 batch number 2    batch loss: 0.09636722505092621\n",
            "epoch 11 batch number 3    batch loss: 0.0964958667755127\n",
            "epoch 11 batch number 4    batch loss: 0.10089465230703354\n",
            "epoch 11 batch number 5    batch loss: 0.108832448720932\n",
            "epoch 11 batch number 6    batch loss: 0.1240464374423027\n",
            "epoch 11 batch number 7    batch loss: 0.11071917414665222\n",
            "epoch 11 batch number 8    batch loss: 0.11371134966611862\n",
            "epoch 11 batch number 9    batch loss: 0.09961637109518051\n",
            "epoch 11 batch number 10    batch loss: 0.10662731528282166\n",
            "epoch 11 batch number 11    batch loss: 0.0904296264052391\n",
            "epoch 11 batch number 12    batch loss: 0.11414041370153427\n",
            "epoch 11 batch number 13    batch loss: 0.09784232079982758\n",
            "epoch 11 batch number 14    batch loss: 0.10345904529094696\n",
            "epoch 11 batch number 15    batch loss: 0.11215609312057495\n",
            "epoch 11 batch number 16    batch loss: 0.0961051657795906\n",
            "epoch 11 batch number 17    batch loss: 0.10498615354299545\n",
            "epoch 11 batch number 18    batch loss: 0.10167685896158218\n",
            "epoch 11 batch number 19    batch loss: 0.09305337816476822\n",
            "epoch 11 batch number 20    batch loss: 0.10217565298080444\n",
            "epoch 11 batch number 21    batch loss: 0.09823203831911087\n",
            "epoch 11 batch number 22    batch loss: 0.09931612759828568\n",
            "epoch 11 batch number 23    batch loss: 0.11005084961652756\n",
            "epoch 11 batch number 24    batch loss: 0.1176072433590889\n",
            "epoch 11 batch number 25    batch loss: 0.10034116357564926\n",
            "epoch 11 batch number 26    batch loss: 0.10483279824256897\n",
            "epoch 11 batch number 27    batch loss: 0.09163638949394226\n",
            "epoch 11 batch number 28    batch loss: 0.09606122970581055\n",
            "epoch 11 batch number 29    batch loss: 0.10068736225366592\n",
            "epoch 11 batch number 30    batch loss: 0.1093016117811203\n",
            "epoch 11 batch number 31    batch loss: 0.10916846990585327\n",
            "epoch 11 batch number 32    batch loss: 0.1030002310872078\n",
            "epoch 11 batch number 33    batch loss: 0.09094879776239395\n",
            "epoch 11 batch number 34    batch loss: 0.09179037809371948\n",
            "epoch 11 batch number 35    batch loss: 0.09611593931913376\n",
            "epoch 11 batch number 36    batch loss: 0.0904652327299118\n",
            "epoch 11 batch number 37    batch loss: 0.09316406399011612\n",
            "epoch 11 batch number 38    batch loss: 0.09120027720928192\n",
            "epoch 11 batch number 39    batch loss: 0.1093454509973526\n",
            "epoch 11 batch number 40    batch loss: 0.11138025671243668\n",
            "epoch 11 batch number 41    batch loss: 0.0980539545416832\n",
            "epoch 11 batch number 42    batch loss: 0.09627798944711685\n",
            "epoch 11 batch number 43    batch loss: 0.10596392303705215\n",
            "epoch 11 batch number 44    batch loss: 0.10207550972700119\n",
            "epoch 11 batch number 45    batch loss: 0.10011827200651169\n",
            "epoch 11 batch number 46    batch loss: 0.09148842096328735\n",
            "epoch 11 batch number 47    batch loss: 0.08401580154895782\n",
            "epoch 11 batch number 48    batch loss: 0.10123251378536224\n",
            "epoch 11 batch number 49    batch loss: 0.11064118146896362\n",
            " Average epoch losses: 0.10179610550403595 \n",
            "epoch  12\n",
            "epoch 12 batch number 0    batch loss: 0.09817340970039368\n",
            "epoch 12 batch number 1    batch loss: 0.09905447065830231\n",
            "epoch 12 batch number 2    batch loss: 0.10862934589385986\n",
            "epoch 12 batch number 3    batch loss: 0.09654790163040161\n",
            "epoch 12 batch number 4    batch loss: 0.0973559096455574\n",
            "epoch 12 batch number 5    batch loss: 0.10015972703695297\n",
            "epoch 12 batch number 6    batch loss: 0.10359685122966766\n",
            "epoch 12 batch number 7    batch loss: 0.10090528428554535\n",
            "epoch 12 batch number 8    batch loss: 0.09956037998199463\n",
            "epoch 12 batch number 9    batch loss: 0.1066679060459137\n",
            "epoch 12 batch number 10    batch loss: 0.09751708060503006\n",
            "epoch 12 batch number 11    batch loss: 0.09065360575914383\n",
            "epoch 12 batch number 12    batch loss: 0.10261712223291397\n",
            "epoch 12 batch number 13    batch loss: 0.09740033745765686\n",
            "epoch 12 batch number 14    batch loss: 0.10349782556295395\n",
            "epoch 12 batch number 15    batch loss: 0.09710946679115295\n",
            "epoch 12 batch number 16    batch loss: 0.09301529079675674\n",
            "epoch 12 batch number 17    batch loss: 0.0980105772614479\n",
            "epoch 12 batch number 18    batch loss: 0.09045153856277466\n",
            "epoch 12 batch number 19    batch loss: 0.11337726563215256\n",
            "epoch 12 batch number 20    batch loss: 0.10516124218702316\n",
            "epoch 12 batch number 21    batch loss: 0.08429131656885147\n",
            "epoch 12 batch number 22    batch loss: 0.10301495343446732\n",
            "epoch 12 batch number 23    batch loss: 0.10433351993560791\n",
            "epoch 12 batch number 24    batch loss: 0.10474934428930283\n",
            "epoch 12 batch number 25    batch loss: 0.10073741525411606\n",
            "epoch 12 batch number 26    batch loss: 0.08893650770187378\n",
            "epoch 12 batch number 27    batch loss: 0.10314449667930603\n",
            "epoch 12 batch number 28    batch loss: 0.0971635952591896\n",
            "epoch 12 batch number 29    batch loss: 0.10461929440498352\n",
            "epoch 12 batch number 30    batch loss: 0.08997482061386108\n",
            "epoch 12 batch number 31    batch loss: 0.1060173287987709\n",
            "epoch 12 batch number 32    batch loss: 0.12104440480470657\n",
            "epoch 12 batch number 33    batch loss: 0.11174893379211426\n",
            "epoch 12 batch number 34    batch loss: 0.10469567030668259\n",
            "epoch 12 batch number 35    batch loss: 0.11807459592819214\n",
            "epoch 12 batch number 36    batch loss: 0.10928983986377716\n",
            "epoch 12 batch number 37    batch loss: 0.09822380542755127\n",
            "epoch 12 batch number 38    batch loss: 0.09405279904603958\n",
            "epoch 12 batch number 39    batch loss: 0.10310041159391403\n",
            "epoch 12 batch number 40    batch loss: 0.09887640178203583\n",
            "epoch 12 batch number 41    batch loss: 0.10172524303197861\n",
            "epoch 12 batch number 42    batch loss: 0.10524514317512512\n",
            "epoch 12 batch number 43    batch loss: 0.11082146316766739\n",
            "epoch 12 batch number 44    batch loss: 0.10703131556510925\n",
            "epoch 12 batch number 45    batch loss: 0.09115608781576157\n",
            "epoch 12 batch number 46    batch loss: 0.09805235266685486\n",
            "epoch 12 batch number 47    batch loss: 0.09083544462919235\n",
            "epoch 12 batch number 48    batch loss: 0.09360461682081223\n",
            "epoch 12 batch number 49    batch loss: 0.1096559390425682\n",
            " Average epoch losses: 0.10107359290122986 \n",
            "epoch  13\n",
            "epoch 13 batch number 0    batch loss: 0.08702311664819717\n",
            "epoch 13 batch number 1    batch loss: 0.0974627435207367\n",
            "epoch 13 batch number 2    batch loss: 0.09229598194360733\n",
            "epoch 13 batch number 3    batch loss: 0.10146191716194153\n",
            "epoch 13 batch number 4    batch loss: 0.11116470396518707\n",
            "epoch 13 batch number 5    batch loss: 0.09339676052331924\n",
            "epoch 13 batch number 6    batch loss: 0.11012125015258789\n",
            "epoch 13 batch number 7    batch loss: 0.10827631503343582\n",
            "epoch 13 batch number 8    batch loss: 0.0893564447760582\n",
            "epoch 13 batch number 9    batch loss: 0.08811039477586746\n",
            "epoch 13 batch number 10    batch loss: 0.08062219619750977\n",
            "epoch 13 batch number 11    batch loss: 0.09047630429267883\n",
            "epoch 13 batch number 12    batch loss: 0.10238040238618851\n",
            "epoch 13 batch number 13    batch loss: 0.09940455108880997\n",
            "epoch 13 batch number 14    batch loss: 0.09528392553329468\n",
            "epoch 13 batch number 15    batch loss: 0.11067944765090942\n",
            "epoch 13 batch number 16    batch loss: 0.08832065016031265\n",
            "epoch 13 batch number 17    batch loss: 0.08413887023925781\n",
            "epoch 13 batch number 18    batch loss: 0.10301963984966278\n",
            "epoch 13 batch number 19    batch loss: 0.08903957903385162\n",
            "epoch 13 batch number 20    batch loss: 0.10852080583572388\n",
            "epoch 13 batch number 21    batch loss: 0.0982588455080986\n",
            "epoch 13 batch number 22    batch loss: 0.09970734268426895\n",
            "epoch 13 batch number 23    batch loss: 0.11321567744016647\n",
            "epoch 13 batch number 24    batch loss: 0.10929925739765167\n",
            "epoch 13 batch number 25    batch loss: 0.09076075255870819\n",
            "epoch 13 batch number 26    batch loss: 0.11008184403181076\n",
            "epoch 13 batch number 27    batch loss: 0.10603311657905579\n",
            "epoch 13 batch number 28    batch loss: 0.10000496357679367\n",
            "epoch 13 batch number 29    batch loss: 0.10086403042078018\n",
            "epoch 13 batch number 30    batch loss: 0.09233807772397995\n",
            "epoch 13 batch number 31    batch loss: 0.09763718396425247\n",
            "epoch 13 batch number 32    batch loss: 0.10323584824800491\n",
            "epoch 13 batch number 33    batch loss: 0.10853280872106552\n",
            "epoch 13 batch number 34    batch loss: 0.09601608663797379\n",
            "epoch 13 batch number 35    batch loss: 0.09984671324491501\n",
            "epoch 13 batch number 36    batch loss: 0.09092000126838684\n",
            "epoch 13 batch number 37    batch loss: 0.09644093364477158\n",
            "epoch 13 batch number 38    batch loss: 0.09735715389251709\n",
            "epoch 13 batch number 39    batch loss: 0.10682499408721924\n",
            "epoch 13 batch number 40    batch loss: 0.10209748893976212\n",
            "epoch 13 batch number 41    batch loss: 0.10240204632282257\n",
            "epoch 13 batch number 42    batch loss: 0.10596463084220886\n",
            "epoch 13 batch number 43    batch loss: 0.09894979745149612\n",
            "epoch 13 batch number 44    batch loss: 0.09635935723781586\n",
            "epoch 13 batch number 45    batch loss: 0.09127182513475418\n",
            "epoch 13 batch number 46    batch loss: 0.114556685090065\n",
            "epoch 13 batch number 47    batch loss: 0.08424309641122818\n",
            "epoch 13 batch number 48    batch loss: 0.09776046872138977\n",
            "epoch 13 batch number 49    batch loss: 0.08046704530715942\n",
            " Average epoch losses: 0.09844009578227997 \n",
            "epoch  14\n",
            "epoch 14 batch number 0    batch loss: 0.08775649964809418\n",
            "epoch 14 batch number 1    batch loss: 0.0959894061088562\n",
            "epoch 14 batch number 2    batch loss: 0.07849038392305374\n",
            "epoch 14 batch number 3    batch loss: 0.09460476040840149\n",
            "epoch 14 batch number 4    batch loss: 0.10555002838373184\n",
            "epoch 14 batch number 5    batch loss: 0.10230012983083725\n",
            "epoch 14 batch number 6    batch loss: 0.09782213717699051\n",
            "epoch 14 batch number 7    batch loss: 0.1005534678697586\n",
            "epoch 14 batch number 8    batch loss: 0.09871195256710052\n",
            "epoch 14 batch number 9    batch loss: 0.09335903078317642\n",
            "epoch 14 batch number 10    batch loss: 0.11357942968606949\n",
            "epoch 14 batch number 11    batch loss: 0.09365005791187286\n",
            "epoch 14 batch number 12    batch loss: 0.09523531049489975\n",
            "epoch 14 batch number 13    batch loss: 0.10045821219682693\n",
            "epoch 14 batch number 14    batch loss: 0.10858334600925446\n",
            "epoch 14 batch number 15    batch loss: 0.10570576786994934\n",
            "epoch 14 batch number 16    batch loss: 0.09887316077947617\n",
            "epoch 14 batch number 17    batch loss: 0.09373648464679718\n",
            "epoch 14 batch number 18    batch loss: 0.11480720341205597\n",
            "epoch 14 batch number 19    batch loss: 0.10219103842973709\n",
            "epoch 14 batch number 20    batch loss: 0.09481511265039444\n",
            "epoch 14 batch number 21    batch loss: 0.09713256359100342\n",
            "epoch 14 batch number 22    batch loss: 0.09579982608556747\n",
            "epoch 14 batch number 23    batch loss: 0.09236424416303635\n",
            "epoch 14 batch number 24    batch loss: 0.09599754959344864\n",
            "epoch 14 batch number 25    batch loss: 0.09807232767343521\n",
            "epoch 14 batch number 26    batch loss: 0.09630168229341507\n",
            "epoch 14 batch number 27    batch loss: 0.09415946155786514\n",
            "epoch 14 batch number 28    batch loss: 0.10618084669113159\n",
            "epoch 14 batch number 29    batch loss: 0.097884900867939\n",
            "epoch 14 batch number 30    batch loss: 0.09413336217403412\n",
            "epoch 14 batch number 31    batch loss: 0.09310539066791534\n",
            "epoch 14 batch number 32    batch loss: 0.09374578297138214\n",
            "epoch 14 batch number 33    batch loss: 0.09339915961027145\n",
            "epoch 14 batch number 34    batch loss: 0.10975620895624161\n",
            "epoch 14 batch number 35    batch loss: 0.1071295365691185\n",
            "epoch 14 batch number 36    batch loss: 0.09060274809598923\n",
            "epoch 14 batch number 37    batch loss: 0.10172855854034424\n",
            "epoch 14 batch number 38    batch loss: 0.1049996018409729\n",
            "epoch 14 batch number 39    batch loss: 0.09714795649051666\n",
            "epoch 14 batch number 40    batch loss: 0.10212375223636627\n",
            "epoch 14 batch number 41    batch loss: 0.09152508527040482\n",
            "epoch 14 batch number 42    batch loss: 0.09999716281890869\n",
            "epoch 14 batch number 43    batch loss: 0.09475062042474747\n",
            "epoch 14 batch number 44    batch loss: 0.09497635811567307\n",
            "epoch 14 batch number 45    batch loss: 0.09437156468629837\n",
            "epoch 14 batch number 46    batch loss: 0.10954447835683823\n",
            "epoch 14 batch number 47    batch loss: 0.08854257315397263\n",
            "epoch 14 batch number 48    batch loss: 0.11254100501537323\n",
            "epoch 14 batch number 49    batch loss: 0.09327449649572372\n",
            " Average epoch losses: 0.09836123138666153 \n",
            "epoch  15\n",
            "epoch 15 batch number 0    batch loss: 0.10363277792930603\n",
            "epoch 15 batch number 1    batch loss: 0.09263673424720764\n",
            "epoch 15 batch number 2    batch loss: 0.09514692425727844\n",
            "epoch 15 batch number 3    batch loss: 0.10009319335222244\n",
            "epoch 15 batch number 4    batch loss: 0.09169887751340866\n",
            "epoch 15 batch number 5    batch loss: 0.09838695079088211\n",
            "epoch 15 batch number 6    batch loss: 0.08840765804052353\n",
            "epoch 15 batch number 7    batch loss: 0.09348046034574509\n",
            "epoch 15 batch number 8    batch loss: 0.08608310669660568\n",
            "epoch 15 batch number 9    batch loss: 0.09487315267324448\n",
            "epoch 15 batch number 10    batch loss: 0.09320037066936493\n",
            "epoch 15 batch number 11    batch loss: 0.09291835129261017\n",
            "epoch 15 batch number 12    batch loss: 0.09196881949901581\n",
            "epoch 15 batch number 13    batch loss: 0.0920262560248375\n",
            "epoch 15 batch number 14    batch loss: 0.08539438247680664\n",
            "epoch 15 batch number 15    batch loss: 0.08980470895767212\n",
            "epoch 15 batch number 16    batch loss: 0.09561985731124878\n",
            "epoch 15 batch number 17    batch loss: 0.10257520526647568\n",
            "epoch 15 batch number 18    batch loss: 0.09104342013597488\n",
            "epoch 15 batch number 19    batch loss: 0.10662471503019333\n",
            "epoch 15 batch number 20    batch loss: 0.1107746809720993\n",
            "epoch 15 batch number 21    batch loss: 0.09400524944067001\n",
            "epoch 15 batch number 22    batch loss: 0.09854059666395187\n",
            "epoch 15 batch number 23    batch loss: 0.09835213422775269\n",
            "epoch 15 batch number 24    batch loss: 0.09805651009082794\n",
            "epoch 15 batch number 25    batch loss: 0.10315347462892532\n",
            "epoch 15 batch number 26    batch loss: 0.08996907621622086\n",
            "epoch 15 batch number 27    batch loss: 0.0855235680937767\n",
            "epoch 15 batch number 28    batch loss: 0.0921531543135643\n",
            "epoch 15 batch number 29    batch loss: 0.10422409325838089\n",
            "epoch 15 batch number 30    batch loss: 0.11318798363208771\n",
            "epoch 15 batch number 31    batch loss: 0.09669927507638931\n",
            "epoch 15 batch number 32    batch loss: 0.09781207889318466\n",
            "epoch 15 batch number 33    batch loss: 0.0979611948132515\n",
            "epoch 15 batch number 34    batch loss: 0.09150027483701706\n",
            "epoch 15 batch number 35    batch loss: 0.08794020116329193\n",
            "epoch 15 batch number 36    batch loss: 0.08936645835638046\n",
            "epoch 15 batch number 37    batch loss: 0.09197431057691574\n",
            "epoch 15 batch number 38    batch loss: 0.08887697756290436\n",
            "epoch 15 batch number 39    batch loss: 0.09127327799797058\n",
            "epoch 15 batch number 40    batch loss: 0.09486713260412216\n",
            "epoch 15 batch number 41    batch loss: 0.09850393980741501\n",
            "epoch 15 batch number 42    batch loss: 0.10064713656902313\n",
            "epoch 15 batch number 43    batch loss: 0.10586854815483093\n",
            "epoch 15 batch number 44    batch loss: 0.09793003648519516\n",
            "epoch 15 batch number 45    batch loss: 0.09364213049411774\n",
            "epoch 15 batch number 46    batch loss: 0.10590849071741104\n",
            "epoch 15 batch number 47    batch loss: 0.09186647087335587\n",
            "epoch 15 batch number 48    batch loss: 0.08364962041378021\n",
            "epoch 15 batch number 49    batch loss: 0.09449263662099838\n",
            " Average epoch losses: 0.09548673033714294 \n",
            "epoch  16\n",
            "epoch 16 batch number 0    batch loss: 0.08453472703695297\n",
            "epoch 16 batch number 1    batch loss: 0.09977052360773087\n",
            "epoch 16 batch number 2    batch loss: 0.10093839466571808\n",
            "epoch 16 batch number 3    batch loss: 0.08727367222309113\n",
            "epoch 16 batch number 4    batch loss: 0.0962943360209465\n",
            "epoch 16 batch number 5    batch loss: 0.09674660116434097\n",
            "epoch 16 batch number 6    batch loss: 0.10535398870706558\n",
            "epoch 16 batch number 7    batch loss: 0.09318225830793381\n",
            "epoch 16 batch number 8    batch loss: 0.09502459317445755\n",
            "epoch 16 batch number 9    batch loss: 0.0960426926612854\n",
            "epoch 16 batch number 10    batch loss: 0.09571920335292816\n",
            "epoch 16 batch number 11    batch loss: 0.08625321835279465\n",
            "epoch 16 batch number 12    batch loss: 0.09310303628444672\n",
            "epoch 16 batch number 13    batch loss: 0.08843088895082474\n",
            "epoch 16 batch number 14    batch loss: 0.09818866848945618\n",
            "epoch 16 batch number 15    batch loss: 0.0932372659444809\n",
            "epoch 16 batch number 16    batch loss: 0.08989253640174866\n",
            "epoch 16 batch number 17    batch loss: 0.09674447029829025\n",
            "epoch 16 batch number 18    batch loss: 0.08852075785398483\n",
            "epoch 16 batch number 19    batch loss: 0.09629978239536285\n",
            "epoch 16 batch number 20    batch loss: 0.09591648727655411\n",
            "epoch 16 batch number 21    batch loss: 0.08938953280448914\n",
            "epoch 16 batch number 22    batch loss: 0.08943744003772736\n",
            "epoch 16 batch number 23    batch loss: 0.0928872674703598\n",
            "epoch 16 batch number 24    batch loss: 0.09413616359233856\n",
            "epoch 16 batch number 25    batch loss: 0.08739068359136581\n",
            "epoch 16 batch number 26    batch loss: 0.11504779011011124\n",
            "epoch 16 batch number 27    batch loss: 0.09881088137626648\n",
            "epoch 16 batch number 28    batch loss: 0.0764121264219284\n",
            "epoch 16 batch number 29    batch loss: 0.09440989792346954\n",
            "epoch 16 batch number 30    batch loss: 0.08774764835834503\n",
            "epoch 16 batch number 31    batch loss: 0.09583177417516708\n",
            "epoch 16 batch number 32    batch loss: 0.09515232592821121\n",
            "epoch 16 batch number 33    batch loss: 0.10180460661649704\n",
            "epoch 16 batch number 34    batch loss: 0.09162548184394836\n",
            "epoch 16 batch number 35    batch loss: 0.0881228819489479\n",
            "epoch 16 batch number 36    batch loss: 0.0896737352013588\n",
            "epoch 16 batch number 37    batch loss: 0.10321176797151566\n",
            "epoch 16 batch number 38    batch loss: 0.09319733828306198\n",
            "epoch 16 batch number 39    batch loss: 0.08987363427877426\n",
            "epoch 16 batch number 40    batch loss: 0.09542593359947205\n",
            "epoch 16 batch number 41    batch loss: 0.0934881940484047\n",
            "epoch 16 batch number 42    batch loss: 0.10783504694700241\n",
            "epoch 16 batch number 43    batch loss: 0.0891755223274231\n",
            "epoch 16 batch number 44    batch loss: 0.09964747726917267\n",
            "epoch 16 batch number 45    batch loss: 0.0925913006067276\n",
            "epoch 16 batch number 46    batch loss: 0.09207285195589066\n",
            "epoch 16 batch number 47    batch loss: 0.11124473065137863\n",
            "epoch 16 batch number 48    batch loss: 0.10208993405103683\n",
            "epoch 16 batch number 49    batch loss: 0.1052168682217598\n",
            " Average epoch losses: 0.09480837732553482 \n",
            "epoch  17\n",
            "epoch 17 batch number 0    batch loss: 0.1105002909898758\n",
            "epoch 17 batch number 1    batch loss: 0.09152670949697495\n",
            "epoch 17 batch number 2    batch loss: 0.0882817879319191\n",
            "epoch 17 batch number 3    batch loss: 0.08126217871904373\n",
            "epoch 17 batch number 4    batch loss: 0.07893121242523193\n",
            "epoch 17 batch number 5    batch loss: 0.07999972254037857\n",
            "epoch 17 batch number 6    batch loss: 0.11349968612194061\n",
            "epoch 17 batch number 7    batch loss: 0.09514214843511581\n",
            "epoch 17 batch number 8    batch loss: 0.09222860634326935\n",
            "epoch 17 batch number 9    batch loss: 0.09558486193418503\n",
            "epoch 17 batch number 10    batch loss: 0.0922420397400856\n",
            "epoch 17 batch number 11    batch loss: 0.08890669792890549\n",
            "epoch 17 batch number 12    batch loss: 0.10414449125528336\n",
            "epoch 17 batch number 13    batch loss: 0.09315580129623413\n",
            "epoch 17 batch number 14    batch loss: 0.08830047398805618\n",
            "epoch 17 batch number 15    batch loss: 0.08838079869747162\n",
            "epoch 17 batch number 16    batch loss: 0.08988409489393234\n",
            "epoch 17 batch number 17    batch loss: 0.08840592205524445\n",
            "epoch 17 batch number 18    batch loss: 0.08977936953306198\n",
            "epoch 17 batch number 19    batch loss: 0.09099811315536499\n",
            "epoch 17 batch number 20    batch loss: 0.09246934950351715\n",
            "epoch 17 batch number 21    batch loss: 0.07858368754386902\n",
            "epoch 17 batch number 22    batch loss: 0.09558958560228348\n",
            "epoch 17 batch number 23    batch loss: 0.09517700970172882\n",
            "epoch 17 batch number 24    batch loss: 0.10106312483549118\n",
            "epoch 17 batch number 25    batch loss: 0.10335738956928253\n",
            "epoch 17 batch number 26    batch loss: 0.08732407540082932\n",
            "epoch 17 batch number 27    batch loss: 0.08248357474803925\n",
            "epoch 17 batch number 28    batch loss: 0.08853038400411606\n",
            "epoch 17 batch number 29    batch loss: 0.0946221575140953\n",
            "epoch 17 batch number 30    batch loss: 0.09917394071817398\n",
            "epoch 17 batch number 31    batch loss: 0.09480729699134827\n",
            "epoch 17 batch number 32    batch loss: 0.10577627271413803\n",
            "epoch 17 batch number 33    batch loss: 0.08713753521442413\n",
            "epoch 17 batch number 34    batch loss: 0.08745431900024414\n",
            "epoch 17 batch number 35    batch loss: 0.09920160472393036\n",
            "epoch 17 batch number 36    batch loss: 0.09187488257884979\n",
            "epoch 17 batch number 37    batch loss: 0.08300099521875381\n",
            "epoch 17 batch number 38    batch loss: 0.09901729226112366\n",
            "epoch 17 batch number 39    batch loss: 0.10951340943574905\n",
            "epoch 17 batch number 40    batch loss: 0.09656862169504166\n",
            "epoch 17 batch number 41    batch loss: 0.09077484905719757\n",
            "epoch 17 batch number 42    batch loss: 0.1071859672665596\n",
            "epoch 17 batch number 43    batch loss: 0.09139597415924072\n",
            "epoch 17 batch number 44    batch loss: 0.10580331087112427\n",
            "epoch 17 batch number 45    batch loss: 0.09638243168592453\n",
            "epoch 17 batch number 46    batch loss: 0.09995325654745102\n",
            "epoch 17 batch number 47    batch loss: 0.10417783260345459\n",
            "epoch 17 batch number 48    batch loss: 0.089998260140419\n",
            "epoch 17 batch number 49    batch loss: 0.08776552230119705\n",
            " Average epoch losses: 0.09374638646841049 \n",
            "epoch  18\n",
            "epoch 18 batch number 0    batch loss: 0.0953717902302742\n",
            "epoch 18 batch number 1    batch loss: 0.09972899407148361\n",
            "epoch 18 batch number 2    batch loss: 0.08751936256885529\n",
            "epoch 18 batch number 3    batch loss: 0.09109660983085632\n",
            "epoch 18 batch number 4    batch loss: 0.08597434312105179\n",
            "epoch 18 batch number 5    batch loss: 0.12255877256393433\n",
            "epoch 18 batch number 6    batch loss: 0.11202921718358994\n",
            "epoch 18 batch number 7    batch loss: 0.09593197703361511\n",
            "epoch 18 batch number 8    batch loss: 0.0861705094575882\n",
            "epoch 18 batch number 9    batch loss: 0.1029042974114418\n",
            "epoch 18 batch number 10    batch loss: 0.08565889298915863\n",
            "epoch 18 batch number 11    batch loss: 0.0901070386171341\n",
            "epoch 18 batch number 12    batch loss: 0.0841110348701477\n",
            "epoch 18 batch number 13    batch loss: 0.08859337121248245\n",
            "epoch 18 batch number 14    batch loss: 0.09548003226518631\n",
            "epoch 18 batch number 15    batch loss: 0.09838848561048508\n",
            "epoch 18 batch number 16    batch loss: 0.09530267864465714\n",
            "epoch 18 batch number 17    batch loss: 0.09215936064720154\n",
            "epoch 18 batch number 18    batch loss: 0.10129392147064209\n",
            "epoch 18 batch number 19    batch loss: 0.09191913902759552\n",
            "epoch 18 batch number 20    batch loss: 0.08792176842689514\n",
            "epoch 18 batch number 21    batch loss: 0.08977431058883667\n",
            "epoch 18 batch number 22    batch loss: 0.08438901603221893\n",
            "epoch 18 batch number 23    batch loss: 0.10285629332065582\n",
            "epoch 18 batch number 24    batch loss: 0.0862809345126152\n",
            "epoch 18 batch number 25    batch loss: 0.09312426298856735\n",
            "epoch 18 batch number 26    batch loss: 0.09519079327583313\n",
            "epoch 18 batch number 27    batch loss: 0.10163774341344833\n",
            "epoch 18 batch number 28    batch loss: 0.10208583623170853\n",
            "epoch 18 batch number 29    batch loss: 0.0863361805677414\n",
            "epoch 18 batch number 30    batch loss: 0.09718066453933716\n",
            "epoch 18 batch number 31    batch loss: 0.08599527925252914\n",
            "epoch 18 batch number 32    batch loss: 0.10411494225263596\n",
            "epoch 18 batch number 33    batch loss: 0.09515734761953354\n",
            "epoch 18 batch number 34    batch loss: 0.09049085527658463\n",
            "epoch 18 batch number 35    batch loss: 0.08589883148670197\n",
            "epoch 18 batch number 36    batch loss: 0.08956632763147354\n",
            "epoch 18 batch number 37    batch loss: 0.09227881580591202\n",
            "epoch 18 batch number 38    batch loss: 0.09476161748170853\n",
            "epoch 18 batch number 39    batch loss: 0.08925478905439377\n",
            "epoch 18 batch number 40    batch loss: 0.08622588962316513\n",
            "epoch 18 batch number 41    batch loss: 0.10238539427518845\n",
            "epoch 18 batch number 42    batch loss: 0.1010444387793541\n",
            "epoch 18 batch number 43    batch loss: 0.10217858105897903\n",
            "epoch 18 batch number 44    batch loss: 0.08346681296825409\n",
            "epoch 18 batch number 45    batch loss: 0.08790428191423416\n",
            "epoch 18 batch number 46    batch loss: 0.0958079919219017\n",
            "epoch 18 batch number 47    batch loss: 0.09376788139343262\n",
            "epoch 18 batch number 48    batch loss: 0.08559700846672058\n",
            "epoch 18 batch number 49    batch loss: 0.0974791869521141\n",
            " Average epoch losses: 0.0938490778207779 \n",
            "epoch  19\n",
            "epoch 19 batch number 0    batch loss: 0.09916089475154877\n",
            "epoch 19 batch number 1    batch loss: 0.08260633796453476\n",
            "epoch 19 batch number 2    batch loss: 0.11041905730962753\n",
            "epoch 19 batch number 3    batch loss: 0.09276660531759262\n",
            "epoch 19 batch number 4    batch loss: 0.09197960793972015\n",
            "epoch 19 batch number 5    batch loss: 0.08070269972085953\n",
            "epoch 19 batch number 6    batch loss: 0.10428579151630402\n",
            "epoch 19 batch number 7    batch loss: 0.0914061889052391\n",
            "epoch 19 batch number 8    batch loss: 0.1049487367272377\n",
            "epoch 19 batch number 9    batch loss: 0.09038038551807404\n",
            "epoch 19 batch number 10    batch loss: 0.09741382300853729\n",
            "epoch 19 batch number 11    batch loss: 0.08870116621255875\n",
            "epoch 19 batch number 12    batch loss: 0.09102426469326019\n",
            "epoch 19 batch number 13    batch loss: 0.08852093666791916\n",
            "epoch 19 batch number 14    batch loss: 0.08794198930263519\n",
            "epoch 19 batch number 15    batch loss: 0.09213356673717499\n",
            "epoch 19 batch number 16    batch loss: 0.08628948777914047\n",
            "epoch 19 batch number 17    batch loss: 0.0880536213517189\n",
            "epoch 19 batch number 18    batch loss: 0.09695939719676971\n",
            "epoch 19 batch number 19    batch loss: 0.09089051187038422\n",
            "epoch 19 batch number 20    batch loss: 0.08804398775100708\n",
            "epoch 19 batch number 21    batch loss: 0.0917748361825943\n",
            "epoch 19 batch number 22    batch loss: 0.08439920842647552\n",
            "epoch 19 batch number 23    batch loss: 0.108976811170578\n",
            "epoch 19 batch number 24    batch loss: 0.09857801347970963\n",
            "epoch 19 batch number 25    batch loss: 0.09004893153905869\n",
            "epoch 19 batch number 26    batch loss: 0.08481395989656448\n",
            "epoch 19 batch number 27    batch loss: 0.0777016282081604\n",
            "epoch 19 batch number 28    batch loss: 0.0964169129729271\n",
            "epoch 19 batch number 29    batch loss: 0.08676369488239288\n",
            "epoch 19 batch number 30    batch loss: 0.10589798539876938\n",
            "epoch 19 batch number 31    batch loss: 0.10311749577522278\n",
            "epoch 19 batch number 32    batch loss: 0.08235733211040497\n",
            "epoch 19 batch number 33    batch loss: 0.09478405117988586\n",
            "epoch 19 batch number 34    batch loss: 0.1047295480966568\n",
            "epoch 19 batch number 35    batch loss: 0.08574925363063812\n",
            "epoch 19 batch number 36    batch loss: 0.08519697189331055\n",
            "epoch 19 batch number 37    batch loss: 0.08314735442399979\n",
            "epoch 19 batch number 38    batch loss: 0.09578977525234222\n",
            "epoch 19 batch number 39    batch loss: 0.09066833555698395\n",
            "epoch 19 batch number 40    batch loss: 0.08506181836128235\n",
            "epoch 19 batch number 41    batch loss: 0.11159719526767731\n",
            "epoch 19 batch number 42    batch loss: 0.10462448000907898\n",
            "epoch 19 batch number 43    batch loss: 0.07508184015750885\n",
            "epoch 19 batch number 44    batch loss: 0.10039907693862915\n",
            "epoch 19 batch number 45    batch loss: 0.09649885445833206\n",
            "epoch 19 batch number 46    batch loss: 0.09459710121154785\n",
            "epoch 19 batch number 47    batch loss: 0.09545713663101196\n",
            "epoch 19 batch number 48    batch loss: 0.09292677044868469\n",
            "epoch 19 batch number 49    batch loss: 0.09209553152322769\n",
            " Average epoch losses: 0.09287761896848679 \n"
          ]
        }
      ],
      "source": [
        "init = tf.global_variables_initializer()\n",
        "epochs_average_loss=[]\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(n_epochs):\n",
        "        print('epoch ',epoch)\n",
        "        LOSS=[]\n",
        "        for i in range(num_batches):\n",
        "          images, blur, _ = train_images.get_batch(batch_size)\n",
        "          batch_loss = sess.run([loss, train], feed_dict={x: images, y: blur})\n",
        "          LOSS.append(batch_loss[0])\n",
        "          print('epoch {} batch number {}    batch loss: {}'.format(epoch, i ,batch_loss[0]))\n",
        "        mean_epoch_Losses=np.mean(LOSS)\n",
        "        epochs_average_loss.append(mean_epoch_Losses)\n",
        "        print(' Average epoch losses: {} '.format(mean_epoch_Losses))\n",
        "       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "hCv1y10rsafJ",
        "outputId": "0fb7a03f-58f5-4a0b-a2b9-d12393efee58"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x720 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "t=np.arange(0,n_epochs,1)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.title('Average Loss Error for {} Epochs. Learning Rate: {}'.format(n_epochs,learning_rate),fontsize=22)\n",
        "plt.plot(t,epochs_average_loss,color='r',label='Mean squar Error loss')\n",
        "#plt.plot(t,Gaverageloss,color='r',label='Generator Overal Error Gloss')\n",
        "#plt.plot(t,ADVloss,color='g',label='Gnerator Adversarial Error G_Adv')\n",
        "#plt.plot(t,MSEloss,color='y',label='Generator MS Error G_MSE')\n",
        "plt.xticks(np.arange(0,n_epochs+1,5))\n",
        "plt.yticks(np.arange(0,0.3,0.05))\n",
        "plt.xlabel('EPOCHS',fontsize=18)\n",
        "plt.ylabel('Error',fontsize=18)\n",
        "plt.gca().legend(prop={'size': 18})\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYjYPWWZwXoc"
      },
      "outputs": [],
      "source": [
        "from testdatareader import TDataReader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "batch_size=1\n",
        "test_data=TDataReader('train')\n",
        "num_batches= test_data.num_batches_of_size(batch_size)\n",
        "A=np.arange(1601,1700,1)  \n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    for vb in range(num_batches):\n",
        "            images,images_names = test_data.get_batch(batchsize)\n",
        "            predictions = sess.run([normalized_output], feed_dict={x:images})\n",
        "            p = predictions[0]\n",
        "            p_arr = (p * 255.0).astype(np.uint8)\n",
        "            p_arr=p_arr.reshape(224,224)\n",
        "            p_image = Image.fromarray(p_arr)\n",
        "            \n",
        "            #print(p_image.shape)\n",
        "            p_image.save('predictions/'+ str(vb) + '_prediction' + '.' + 'jpeg')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "image_blur_detection.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPp/kvuDygGYYMpEzJqSFuc",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}