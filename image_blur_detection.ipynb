{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MasoudMoeini/Image-blur-detection/blob/main/image_blur_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-5xmsZ5RQes",
        "outputId": "12863719-ad4d-43a6-d036-d52a5d6fcf47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ],
      "source": [
        "%load_ext tensorboard\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import numpy as np\n",
        "import matplotlib . pyplot as plt\n",
        "from tensorflow.keras import layers, losses\n",
        "# Base CNN\n",
        "x = tf.placeholder(tf.float32, (None,224, 224, 3))\n",
        "y = tf.placeholder(tf.float32, (None,224, 224, 3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJfwkZX9TC3O"
      },
      "outputs": [],
      "source": [
        "#!unzip -qq BlurDatasetResultShi.zip\n",
        "#!unzip -qq ccv_data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pmv42pPJ4SW"
      },
      "outputs": [],
      "source": [
        "rm -rf ./logs/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BbR2mvcKA5S",
        "outputId": "21f45e77-f84c-48c8-8ed0-b2d24b1593f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.7/dist-packages/keras/legacy_tf_layers/convolutional.py:575: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: UserWarning: `tf.layers.max_pooling2d` is deprecated and will be removed in a future version. Please use `tf.keras.layers.MaxPooling2D` instead.\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/keras/legacy_tf_layers/pooling.py:600: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: `tf.layers.max_pooling2d` is deprecated and will be removed in a future version. Please use `tf.keras.layers.MaxPooling2D` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:34: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:39: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:44: UserWarning: `tf.layers.max_pooling2d` is deprecated and will be removed in a future version. Please use `tf.keras.layers.MaxPooling2D` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:49: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: `tf.layers.max_pooling2d` is deprecated and will be removed in a future version. Please use `tf.keras.layers.MaxPooling2D` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:68: UserWarning: `tf.layers.max_pooling2d` is deprecated and will be removed in a future version. Please use `tf.keras.layers.MaxPooling2D` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:79: UserWarning: `tf.layers.conv2d_transpose` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2DTranspose` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/keras/legacy_tf_layers/convolutional.py:1736: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:83: UserWarning: `tf.layers.conv2d_transpose` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2DTranspose` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:87: UserWarning: `tf.layers.conv2d_transpose` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2DTranspose` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:91: UserWarning: `tf.layers.conv2d_transpose` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2DTranspose` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:99: UserWarning: `tf.layers.conv2d_transpose` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2DTranspose` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:103: UserWarning: `tf.layers.conv2d_transpose` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2DTranspose` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:107: UserWarning: `tf.layers.conv2d_transpose` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2DTranspose` instead.\n"
          ]
        }
      ],
      "source": [
        "# Conv1\n",
        "# Input Tensor Shape: [batch_size, 224, 224, 3]\n",
        "# Output Tensor Shape: [batch_size, 224, 224, 32]\n",
        "conv1 = tf.layers.conv2d(x, filters=32, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu)\n",
        "\n",
        "# conv2\n",
        "# Input Tensor Shape: [batch_size, 224, 224, 32]\n",
        "# Output Tensor Shape: [batch_size, 224, 224, 32]\n",
        "conv2 = tf.layers.conv2d(conv1, filters=32, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu)\n",
        "\n",
        "# pool1\n",
        "# Input Tensor Shape: [batch_size, 224, 224, 32]\n",
        "# Output Tensor Shape: [batch_size, 112, 112, 32]\n",
        "pool1 = tf.layers.max_pooling2d(conv2, pool_size=[2,2], strides=2, padding=\"same\")\n",
        "\n",
        "#conv3\n",
        "# Input Tensor Shape: [batch_size, 112, 112, 32]\n",
        "# Output Tensor Shape: [batch_size, 112, 112, 64]\n",
        "conv3 = tf.layers.conv2d(conv2, filters=64, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu)\n",
        "\n",
        "#conv4\n",
        "# Input Tensor Shape: [batch_size, 112, 112, 64]\n",
        "# Output Tensor Shape: [batch_size, 112, 112, 64]\n",
        "conv4 = tf.layers.conv2d(conv3, filters=64, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu)\n",
        "\n",
        "#pool2\n",
        "# Input Tensor Shape: [batch_size, 112, 112, 64]\n",
        "# Output Tensor Shape: [batch_size, 56, 56, 64]\n",
        "pool2 = tf.layers.max_pooling2d(conv4, pool_size=[2,2], strides=2, padding=\"same\")\n",
        "\n",
        "#conv5\n",
        "# Input Tensor Shape: [batch_size, 56, 56, 64]\n",
        "# Output Tensor Shape: [batch_size, 56, 56, 128]\n",
        "conv5 = tf.layers.conv2d(pool2, filters=128, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu)\n",
        "\n",
        "#conv6\n",
        "# Input Tensor Shape: [batch_size, 56, 56, 128]\n",
        "# Output Tensor Shape: [batch_size, 56, 56, 128]\n",
        "conv6 = tf.layers.conv2d(conv5, filters=128, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu)\n",
        "\n",
        "#pool3\n",
        "# Input Tensor Shape: [batch_size, 56, 56, 128]\n",
        "# Output Tensor Shape: [batch_size, 28, 28, 128]\n",
        "pool3 = tf.layers.max_pooling2d(conv6, pool_size=[2,2], strides=2, padding=\"same\")\n",
        "\n",
        "#conv7\n",
        "# Input Tensor Shape: [batch_size, 28, 28, 128]\n",
        "# Output Tensor Shape: [batch_size, 28, 28, 256]\n",
        "conv7 = tf.layers.conv2d(pool3, filters=256, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu)\n",
        "#conv8\n",
        "# Input Tensor Shape: [batch_size, 28, 28, 256]\n",
        "# Output Tensor Shape: [batch_size, 28, 28, 256]\n",
        "conv8 = tf.layers.conv2d(conv7, filters=256, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu)\n",
        "\n",
        "#pool4\n",
        "# Input Tensor Shape: [batch_size, 28, 28, 256]\n",
        "# Output Tensor Shape: [batch_size, 14, 14, 256]\n",
        "\n",
        "pool4 = tf.layers.max_pooling2d(conv8, pool_size=[2,2], strides=2, padding=\"same\")\n",
        "#conv9\n",
        "# Input Tensor Shape: [batch_size, 14, 14, 256]\n",
        "# Output Tensor Shape: [batch_size, 14, 14, 512]\n",
        "conv9 = tf.layers.conv2d(pool4, filters=512, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu)\n",
        "\n",
        "#pool5\n",
        "# Input Tensor Shape: [batch_size, 14, 14, 512]\n",
        "# Output Tensor Shape: [batch_size, 7, 7, 512]\n",
        "pool5 = tf.layers.max_pooling2d(conv9, pool_size=[2,2], strides=2, padding=\"same\")\n",
        "\n",
        "#------------------------------------------decode---------------------------------------\n",
        "\n",
        "\n",
        "#dim = int(np.prod(pool5.get_shape()[1:])) #7*7*512\n",
        "#fcl = tf.reshape(pool5, shape=[-1, dim], name ='fc1')#[batch_size,7*7*512]\n",
        "# decoder\n",
        "\n",
        "# Input Tensor Shape: [batch_size, 7, 7, 512]\n",
        "# Output Tensor Shape: [batch_size, 14, 14, 512]\n",
        "net=tf.layers.conv2d_transpose(pool5,512,[3, 3],strides = 2,padding='SAME')\n",
        "\n",
        "# Input Tensor Shape: [batch_size, 14, 14, 512]\n",
        "# Output Tensor Shape: [batch_size, 28, 28, 256]\n",
        "net=tf.layers.conv2d_transpose(net,256,[3, 3],strides = 2,padding='SAME')\n",
        "\n",
        "# Input Tensor Shape: [batch_size, 28, 28, 256]\n",
        "# Output Tensor Shape: [batch_size, 56, 56, 128]\n",
        "net=tf.layers.conv2d_transpose(net,128,[3, 3],strides = 2,padding='SAME')\n",
        "\n",
        "# Input Tensor Shape: [batch_size, 56, 56, 128]\n",
        "# Output Tensor Shape: [batch_size, 112, 112, 128]\n",
        "net=tf.layers.conv2d_transpose(net,128,[3, 3],strides = 2,padding='SAME',activation = tf.nn.relu)\n",
        "\n",
        "# Input Tensor Shape: [batch_size, 112, 112, 128]\n",
        "# Output Tensor Shape: [batch_size, 224, 224, 64]\n",
        "#net=tf.layers.conv2d_transpose(net,64,[3, 3],strides = 2,padding='SAME')\n",
        "\n",
        "# Input Tensor Shape: [batch_size, 224, 224, 64]\n",
        "# Output Tensor Shape: [batch_size, 224, 224, 64]\n",
        "net=tf.layers.conv2d_transpose(net,64,[3, 3],strides = 1,padding='SAME')\n",
        "\n",
        "# Input Tensor Shape: [batch_size, 224, 224, 64]\n",
        "# Output Tensor Shape: [batch_size, 224, 224, 32]\n",
        "net=tf.layers.conv2d_transpose(net,32,[3, 3],strides = 1, padding='SAME', activation = tf.nn.relu)\n",
        "\n",
        "# Input Tensor Shape: [batch_size, 224, 224, 32]\n",
        "# Output Tensor Shape: [batch_size, 224, 224, 3]\n",
        "net=tf.layers.conv2d_transpose(net,3,[3, 3],strides = 1, padding='SAME', activation = tf.nn.relu)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNzgCa05zB0d"
      },
      "outputs": [],
      "source": [
        "#normlizing output\n",
        "#wmax = tf.reduce_max(net, axis=2, keepdims=True) # along width dimension\n",
        "#output_max = tf.reduce_max(wmax, axis=1, keepdims=True) # along height dimension\n",
        "#normalized_output = net / output_max\n",
        "#alpha = 1.1\n",
        "#loss = tf.reduce_mean( tf.square( (1.0/(alpha - y)) * (normalized_output - y) ))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkQpeiENg4tD"
      },
      "outputs": [],
      "source": [
        "!unzip -qq train.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "P-CIQbYmlE3j"
      },
      "outputs": [],
      "source": [
        "from datareader import DataReader\n",
        "train_images = DataReader('train')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "FBu6qF1NS4I6"
      },
      "outputs": [],
      "source": [
        "## Optimize\n",
        "batch_size=10\n",
        "num_batches= train_images.num_batches_of_size(batch_size)\n",
        "learning_rate = 0.001\n",
        "n_epochs = 3\n",
        "loss = tf.reduce_mean(tf.square(net - y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "train  = optimizer.minimize(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPqqHoDRP0e_",
        "outputId": "71808fe6-60ae-4ec3-8d02-e3858191862b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch  0\n",
            "epoch 0 batch number 0    batch loss: 0.340368390083313\n",
            "epoch 0 batch number 1    batch loss: 0.3254561722278595\n",
            "epoch 0 batch number 2    batch loss: 2.6406750679016113\n",
            "epoch 0 batch number 3    batch loss: 0.3854941427707672\n",
            "epoch 0 batch number 4    batch loss: 0.40098246932029724\n",
            "epoch 0 batch number 5    batch loss: 0.4304885268211365\n",
            "epoch 0 batch number 6    batch loss: 0.4166190028190613\n",
            "epoch 0 batch number 7    batch loss: 0.34722408652305603\n",
            "epoch 0 batch number 8    batch loss: 0.3732069432735443\n",
            "epoch 0 batch number 9    batch loss: 0.4045940041542053\n",
            "epoch 0 batch number 10    batch loss: 0.3675190806388855\n",
            "epoch 0 batch number 11    batch loss: 0.4346528649330139\n",
            "epoch 0 batch number 12    batch loss: 0.3147587180137634\n",
            "epoch 0 batch number 13    batch loss: 0.363247811794281\n",
            "epoch 0 batch number 14    batch loss: 0.27712592482566833\n",
            "epoch 0 batch number 15    batch loss: 0.35433706641197205\n",
            "epoch 0 batch number 16    batch loss: 0.30794551968574524\n",
            "epoch 0 batch number 17    batch loss: 0.3343803882598877\n",
            "epoch 0 batch number 18    batch loss: 0.3222905099391937\n",
            "epoch 0 batch number 19    batch loss: 0.2944141924381256\n",
            "epoch 0 batch number 20    batch loss: 0.2384883165359497\n",
            "epoch 0 batch number 21    batch loss: 0.21842385828495026\n",
            "epoch 0 batch number 22    batch loss: 0.24268653988838196\n",
            "epoch 0 batch number 23    batch loss: 0.23517996072769165\n",
            "epoch 0 batch number 24    batch loss: 0.2208586186170578\n",
            "epoch 0 batch number 25    batch loss: 0.20429782569408417\n",
            "epoch 0 batch number 26    batch loss: 0.18606439232826233\n",
            "epoch 0 batch number 27    batch loss: 0.2019425332546234\n",
            "epoch 0 batch number 28    batch loss: 0.2163369506597519\n",
            "epoch 0 batch number 29    batch loss: 0.2109578251838684\n",
            "epoch 0 batch number 30    batch loss: 0.19053326547145844\n",
            "epoch 0 batch number 31    batch loss: 0.2006574124097824\n",
            "epoch 0 batch number 32    batch loss: 0.20165878534317017\n",
            "epoch 0 batch number 33    batch loss: 0.20911641418933868\n",
            "epoch 0 batch number 34    batch loss: 0.20542162656784058\n",
            "epoch 0 batch number 35    batch loss: 0.21031394600868225\n",
            "epoch 0 batch number 36    batch loss: 0.19606074690818787\n",
            "epoch 0 batch number 37    batch loss: 0.17893487215042114\n",
            "epoch 0 batch number 38    batch loss: 0.1851959377527237\n",
            "epoch 0 batch number 39    batch loss: 0.20348228514194489\n",
            "epoch 0 batch number 40    batch loss: 0.19168680906295776\n",
            "epoch 0 batch number 41    batch loss: 0.19716639816761017\n",
            "epoch 0 batch number 42    batch loss: 0.19631966948509216\n",
            "epoch 0 batch number 43    batch loss: 0.18836693465709686\n",
            "epoch 0 batch number 44    batch loss: 0.19302326440811157\n",
            "epoch 0 batch number 45    batch loss: 0.16666412353515625\n",
            "epoch 0 batch number 46    batch loss: 0.18885353207588196\n",
            "epoch 0 batch number 47    batch loss: 0.18440791964530945\n",
            "epoch 0 batch number 48    batch loss: 0.20272773504257202\n",
            "epoch 0 batch number 49    batch loss: 0.18553628027439117\n",
            "epoch 0 batch number 50    batch loss: 0.20970210433006287\n",
            "epoch 0 batch number 51    batch loss: 0.1897197961807251\n",
            "epoch 0 batch number 52    batch loss: 0.1914135217666626\n",
            "epoch 0 batch number 53    batch loss: 0.17955122888088226\n",
            "epoch 0 batch number 54    batch loss: 0.16696667671203613\n",
            "epoch 0 batch number 55    batch loss: 0.17949162423610687\n",
            "epoch 0 batch number 56    batch loss: 0.1906188577413559\n",
            "epoch 0 batch number 57    batch loss: 0.19847717881202698\n",
            "epoch 0 batch number 58    batch loss: 0.20373091101646423\n",
            "epoch 0 batch number 59    batch loss: 0.1947735995054245\n",
            "epoch 0 batch number 60    batch loss: 0.17731426656246185\n",
            "epoch 0 batch number 61    batch loss: 0.20189860463142395\n",
            "epoch 0 batch number 62    batch loss: 0.1946362406015396\n",
            "epoch 0 batch number 63    batch loss: 0.1847335696220398\n",
            "epoch 0 batch number 64    batch loss: 0.19458416104316711\n",
            "epoch 0 batch number 65    batch loss: 0.1951696127653122\n",
            "epoch 0 batch number 66    batch loss: 0.20538711547851562\n",
            "epoch 0 batch number 67    batch loss: 0.19341255724430084\n",
            "epoch 0 batch number 68    batch loss: 0.19234351813793182\n",
            "epoch 0 batch number 69    batch loss: 0.19182075560092926\n",
            "epoch 0 batch number 70    batch loss: 0.2043565809726715\n",
            "epoch 0 batch number 71    batch loss: 0.20047837495803833\n",
            "epoch 0 batch number 72    batch loss: 0.1919851303100586\n",
            "epoch 0 batch number 73    batch loss: 0.18807244300842285\n",
            "epoch 0 batch number 74    batch loss: 0.18504470586776733\n",
            "epoch 0 batch number 75    batch loss: 0.18098238110542297\n",
            "epoch 0 batch number 76    batch loss: 0.18206681311130524\n",
            "epoch 0 batch number 77    batch loss: 0.198741152882576\n",
            "epoch 0 batch number 78    batch loss: 0.18211190402507782\n",
            "epoch 0 batch number 79    batch loss: 0.19423212110996246\n",
            "epoch 0 batch number 80    batch loss: 0.17763563990592957\n",
            "epoch 0 batch number 81    batch loss: 0.18770945072174072\n",
            "epoch 0 batch number 82    batch loss: 0.17242752015590668\n",
            "epoch 0 batch number 83    batch loss: 0.17514090240001678\n",
            "epoch 0 batch number 84    batch loss: 0.16556410491466522\n",
            "epoch 0 batch number 85    batch loss: 0.1600777953863144\n",
            "epoch 0 batch number 86    batch loss: 0.1801302582025528\n",
            "epoch 0 batch number 87    batch loss: 0.1852198988199234\n",
            "epoch 0 batch number 88    batch loss: 0.16814175248146057\n",
            "epoch 0 batch number 89    batch loss: 0.1627119481563568\n",
            "epoch 0 batch number 90    batch loss: 0.183808833360672\n",
            "epoch 0 batch number 91    batch loss: 0.1627494990825653\n",
            "epoch 0 batch number 92    batch loss: 0.17044669389724731\n",
            "epoch 0 batch number 93    batch loss: 0.13938450813293457\n",
            "epoch 0 batch number 94    batch loss: 0.17395421862602234\n",
            "epoch 0 batch number 95    batch loss: 0.13496103882789612\n",
            "epoch 0 batch number 96    batch loss: 0.1786361187696457\n",
            "epoch 0 batch number 97    batch loss: 0.1780109852552414\n",
            "epoch 0 batch number 98    batch loss: 0.14565078914165497\n",
            "epoch 0 batch number 99    batch loss: 0.14976176619529724\n",
            " Average epoch losses: 0.24583087861537933 \n",
            "epoch  1\n",
            "epoch 1 batch number 0    batch loss: 0.1441754847764969\n",
            "epoch 1 batch number 1    batch loss: 0.11690331250429153\n",
            "epoch 1 batch number 2    batch loss: 0.1543084979057312\n",
            "epoch 1 batch number 3    batch loss: 0.19466504454612732\n",
            "epoch 1 batch number 4    batch loss: 0.14463339745998383\n",
            "epoch 1 batch number 5    batch loss: 0.14671865105628967\n",
            "epoch 1 batch number 6    batch loss: 0.15500059723854065\n",
            "epoch 1 batch number 7    batch loss: 0.16058987379074097\n",
            "epoch 1 batch number 8    batch loss: 0.15878058969974518\n",
            "epoch 1 batch number 9    batch loss: 0.1407446414232254\n",
            "epoch 1 batch number 10    batch loss: 0.16056638956069946\n",
            "epoch 1 batch number 11    batch loss: 0.14382216334342957\n",
            "epoch 1 batch number 12    batch loss: 0.1383301466703415\n",
            "epoch 1 batch number 13    batch loss: 0.15876330435276031\n",
            "epoch 1 batch number 14    batch loss: 0.14286823570728302\n",
            "epoch 1 batch number 15    batch loss: 0.14740045368671417\n",
            "epoch 1 batch number 16    batch loss: 0.1437232494354248\n",
            "epoch 1 batch number 17    batch loss: 0.15089890360832214\n",
            "epoch 1 batch number 18    batch loss: 0.13604645431041718\n",
            "epoch 1 batch number 19    batch loss: 0.13986235857009888\n",
            "epoch 1 batch number 20    batch loss: 0.15612278878688812\n",
            "epoch 1 batch number 21    batch loss: 0.14859852194786072\n",
            "epoch 1 batch number 22    batch loss: 0.13233257830142975\n",
            "epoch 1 batch number 23    batch loss: 0.13694654405117035\n",
            "epoch 1 batch number 24    batch loss: 0.14329715073108673\n",
            "epoch 1 batch number 25    batch loss: 0.1425285041332245\n",
            "epoch 1 batch number 26    batch loss: 0.12090539932250977\n",
            "epoch 1 batch number 27    batch loss: 0.1573207676410675\n",
            "epoch 1 batch number 28    batch loss: 0.16398514807224274\n",
            "epoch 1 batch number 29    batch loss: 0.13613702356815338\n",
            "epoch 1 batch number 30    batch loss: 0.20101119577884674\n",
            "epoch 1 batch number 31    batch loss: 0.16796572506427765\n",
            "epoch 1 batch number 32    batch loss: 0.13870185613632202\n",
            "epoch 1 batch number 33    batch loss: 0.13950401544570923\n",
            "epoch 1 batch number 34    batch loss: 0.14176233112812042\n",
            "epoch 1 batch number 35    batch loss: 0.12252442538738251\n",
            "epoch 1 batch number 36    batch loss: 0.14592893421649933\n",
            "epoch 1 batch number 37    batch loss: 0.14414966106414795\n",
            "epoch 1 batch number 38    batch loss: 0.12582235038280487\n",
            "epoch 1 batch number 39    batch loss: 0.16017957031726837\n",
            "epoch 1 batch number 40    batch loss: 0.12490573525428772\n",
            "epoch 1 batch number 41    batch loss: 0.12267329543828964\n",
            "epoch 1 batch number 42    batch loss: 0.1197739765048027\n",
            "epoch 1 batch number 43    batch loss: 0.1220734715461731\n",
            "epoch 1 batch number 44    batch loss: 0.17493093013763428\n",
            "epoch 1 batch number 45    batch loss: 0.1352258324623108\n",
            "epoch 1 batch number 46    batch loss: 0.14441561698913574\n",
            "epoch 1 batch number 47    batch loss: 0.13693967461585999\n",
            "epoch 1 batch number 48    batch loss: 0.13548985123634338\n",
            "epoch 1 batch number 49    batch loss: 0.13118670880794525\n",
            "epoch 1 batch number 50    batch loss: 0.15983901917934418\n",
            "epoch 1 batch number 51    batch loss: 0.12816673517227173\n",
            "epoch 1 batch number 52    batch loss: 0.12403148412704468\n",
            "epoch 1 batch number 53    batch loss: 0.12763409316539764\n",
            "epoch 1 batch number 54    batch loss: 0.12622293829917908\n",
            "epoch 1 batch number 55    batch loss: 0.13754808902740479\n",
            "epoch 1 batch number 56    batch loss: 0.14534319937229156\n",
            "epoch 1 batch number 57    batch loss: 0.12363246083259583\n",
            "epoch 1 batch number 58    batch loss: 0.10619910806417465\n",
            "epoch 1 batch number 59    batch loss: 0.145798459649086\n",
            "epoch 1 batch number 60    batch loss: 0.12032178044319153\n",
            "epoch 1 batch number 61    batch loss: 0.1307501345872879\n",
            "epoch 1 batch number 62    batch loss: 0.13035903871059418\n",
            "epoch 1 batch number 63    batch loss: 0.11692161113023758\n",
            "epoch 1 batch number 64    batch loss: 0.119364432990551\n",
            "epoch 1 batch number 65    batch loss: 0.12906771898269653\n",
            "epoch 1 batch number 66    batch loss: 0.12120623141527176\n",
            "epoch 1 batch number 67    batch loss: 0.12329035997390747\n",
            "epoch 1 batch number 68    batch loss: 0.1333274096250534\n",
            "epoch 1 batch number 69    batch loss: 0.11250075697898865\n",
            "epoch 1 batch number 70    batch loss: 0.1309930980205536\n",
            "epoch 1 batch number 71    batch loss: 0.10729430615901947\n",
            "epoch 1 batch number 72    batch loss: 0.1284152865409851\n",
            "epoch 1 batch number 73    batch loss: 0.12071309238672256\n",
            "epoch 1 batch number 74    batch loss: 0.10308536887168884\n",
            "epoch 1 batch number 75    batch loss: 0.0913911685347557\n",
            "epoch 1 batch number 76    batch loss: 0.11384761333465576\n",
            "epoch 1 batch number 77    batch loss: 0.1260356605052948\n",
            "epoch 1 batch number 78    batch loss: 0.12256335467100143\n",
            "epoch 1 batch number 79    batch loss: 0.11089145392179489\n",
            "epoch 1 batch number 80    batch loss: 0.12836073338985443\n",
            "epoch 1 batch number 81    batch loss: 0.12477238476276398\n",
            "epoch 1 batch number 82    batch loss: 0.12020208686590195\n",
            "epoch 1 batch number 83    batch loss: 0.12846462428569794\n",
            "epoch 1 batch number 84    batch loss: 0.13274775445461273\n",
            "epoch 1 batch number 85    batch loss: 0.10796560347080231\n",
            "epoch 1 batch number 86    batch loss: 0.12263540923595428\n",
            "epoch 1 batch number 87    batch loss: 0.10508328676223755\n",
            "epoch 1 batch number 88    batch loss: 0.10236002504825592\n",
            "epoch 1 batch number 89    batch loss: 0.10804934054613113\n",
            "epoch 1 batch number 90    batch loss: 0.09279614686965942\n",
            "epoch 1 batch number 91    batch loss: 0.11563979834318161\n",
            "epoch 1 batch number 92    batch loss: 0.09503170847892761\n",
            "epoch 1 batch number 93    batch loss: 0.12746573984622955\n",
            "epoch 1 batch number 94    batch loss: 0.17103074491024017\n",
            "epoch 1 batch number 95    batch loss: 0.1390296071767807\n",
            "epoch 1 batch number 96    batch loss: 0.10335725545883179\n",
            "epoch 1 batch number 97    batch loss: 0.10906579345464706\n",
            "epoch 1 batch number 98    batch loss: 0.13106481730937958\n",
            "epoch 1 batch number 99    batch loss: 0.12678727507591248\n",
            " Average epoch losses: 0.13360805809497833 \n",
            "epoch  2\n",
            "epoch 2 batch number 0    batch loss: 0.12237482517957687\n",
            "epoch 2 batch number 1    batch loss: 0.12173004448413849\n",
            "epoch 2 batch number 2    batch loss: 0.12415675818920135\n",
            "epoch 2 batch number 3    batch loss: 0.11047220975160599\n",
            "epoch 2 batch number 4    batch loss: 0.11027437448501587\n",
            "epoch 2 batch number 5    batch loss: 0.11943744122982025\n",
            "epoch 2 batch number 6    batch loss: 0.11048532277345657\n",
            "epoch 2 batch number 7    batch loss: 0.11035262793302536\n",
            "epoch 2 batch number 8    batch loss: 0.09148353338241577\n",
            "epoch 2 batch number 9    batch loss: 0.13687433302402496\n",
            "epoch 2 batch number 10    batch loss: 0.12246198952198029\n",
            "epoch 2 batch number 11    batch loss: 0.1297457069158554\n",
            "epoch 2 batch number 12    batch loss: 0.12081920355558395\n",
            "epoch 2 batch number 13    batch loss: 0.09869036823511124\n",
            "epoch 2 batch number 14    batch loss: 0.11192398518323898\n",
            "epoch 2 batch number 15    batch loss: 0.13564758002758026\n",
            "epoch 2 batch number 16    batch loss: 0.09719489514827728\n",
            "epoch 2 batch number 17    batch loss: 0.08998152613639832\n",
            "epoch 2 batch number 18    batch loss: 0.20989371836185455\n",
            "epoch 2 batch number 19    batch loss: 0.09859767556190491\n",
            "epoch 2 batch number 20    batch loss: 0.10528448224067688\n",
            "epoch 2 batch number 21    batch loss: 0.12354335933923721\n",
            "epoch 2 batch number 22    batch loss: 0.13369517028331757\n",
            "epoch 2 batch number 23    batch loss: 0.08591030538082123\n",
            "epoch 2 batch number 24    batch loss: 0.098993681371212\n",
            "epoch 2 batch number 25    batch loss: 0.10778269916772842\n",
            "epoch 2 batch number 26    batch loss: 0.1201682761311531\n",
            "epoch 2 batch number 27    batch loss: 0.12029127031564713\n",
            "epoch 2 batch number 28    batch loss: 0.12962327897548676\n",
            "epoch 2 batch number 29    batch loss: 0.11919579654932022\n",
            "epoch 2 batch number 30    batch loss: 0.11183499544858932\n",
            "epoch 2 batch number 31    batch loss: 0.1211743950843811\n",
            "epoch 2 batch number 32    batch loss: 0.10137317329645157\n",
            "epoch 2 batch number 33    batch loss: 0.10804582387208939\n",
            "epoch 2 batch number 34    batch loss: 0.10040906816720963\n",
            "epoch 2 batch number 35    batch loss: 0.10358988493680954\n",
            "epoch 2 batch number 36    batch loss: 0.11121144890785217\n",
            "epoch 2 batch number 37    batch loss: 0.10015735030174255\n",
            "epoch 2 batch number 38    batch loss: 0.09817089885473251\n",
            "epoch 2 batch number 39    batch loss: 0.08262039721012115\n",
            "epoch 2 batch number 40    batch loss: 0.117216095328331\n",
            "epoch 2 batch number 41    batch loss: 0.08532249927520752\n",
            "epoch 2 batch number 42    batch loss: 0.0972377359867096\n",
            "epoch 2 batch number 43    batch loss: 0.1296519935131073\n",
            "epoch 2 batch number 44    batch loss: 0.14473499357700348\n",
            "epoch 2 batch number 45    batch loss: 0.11697617173194885\n",
            "epoch 2 batch number 46    batch loss: 0.10305028408765793\n",
            "epoch 2 batch number 47    batch loss: 0.10200276225805283\n",
            "epoch 2 batch number 48    batch loss: 0.10026202350854874\n",
            "epoch 2 batch number 49    batch loss: 0.09537136554718018\n",
            "epoch 2 batch number 50    batch loss: 0.11331324279308319\n",
            "epoch 2 batch number 51    batch loss: 0.0949760302901268\n",
            "epoch 2 batch number 52    batch loss: 0.1104021742939949\n",
            "epoch 2 batch number 53    batch loss: 0.12853363156318665\n",
            "epoch 2 batch number 54    batch loss: 0.1416521966457367\n",
            "epoch 2 batch number 55    batch loss: 0.11800862848758698\n",
            "epoch 2 batch number 56    batch loss: 0.09171915054321289\n",
            "epoch 2 batch number 57    batch loss: 0.10569314658641815\n",
            "epoch 2 batch number 58    batch loss: 0.11919334530830383\n",
            "epoch 2 batch number 59    batch loss: 0.11394306272268295\n",
            "epoch 2 batch number 60    batch loss: 0.10784642398357391\n",
            "epoch 2 batch number 61    batch loss: 0.11960359662771225\n",
            "epoch 2 batch number 62    batch loss: 0.1348634511232376\n",
            "epoch 2 batch number 63    batch loss: 0.10509301722049713\n",
            "epoch 2 batch number 64    batch loss: 0.12785978615283966\n",
            "epoch 2 batch number 65    batch loss: 0.1216820776462555\n",
            "epoch 2 batch number 66    batch loss: 0.09314480423927307\n",
            "epoch 2 batch number 67    batch loss: 0.11553143709897995\n",
            "epoch 2 batch number 68    batch loss: 0.09334897249937057\n",
            "epoch 2 batch number 69    batch loss: 0.09987423568964005\n",
            "epoch 2 batch number 70    batch loss: 0.14115917682647705\n",
            "epoch 2 batch number 71    batch loss: 0.12554128468036652\n",
            "epoch 2 batch number 72    batch loss: 0.1067495122551918\n",
            "epoch 2 batch number 73    batch loss: 0.11542289704084396\n",
            "epoch 2 batch number 74    batch loss: 0.10200309753417969\n",
            "epoch 2 batch number 75    batch loss: 0.10812417417764664\n",
            "epoch 2 batch number 76    batch loss: 0.0952647477388382\n",
            "epoch 2 batch number 77    batch loss: 0.12053035944700241\n",
            "epoch 2 batch number 78    batch loss: 0.07489361613988876\n",
            "epoch 2 batch number 79    batch loss: 0.09607180953025818\n",
            "epoch 2 batch number 80    batch loss: 0.10000177472829819\n",
            "epoch 2 batch number 81    batch loss: 0.11464695632457733\n",
            "epoch 2 batch number 82    batch loss: 0.13597914576530457\n",
            "epoch 2 batch number 83    batch loss: 0.14829939603805542\n",
            "epoch 2 batch number 84    batch loss: 0.10887500643730164\n",
            "epoch 2 batch number 85    batch loss: 0.09752348065376282\n",
            "epoch 2 batch number 86    batch loss: 0.10970528423786163\n",
            "epoch 2 batch number 87    batch loss: 0.11772635579109192\n",
            "epoch 2 batch number 88    batch loss: 0.11248889565467834\n",
            "epoch 2 batch number 89    batch loss: 0.1471615582704544\n",
            "epoch 2 batch number 90    batch loss: 0.12216221541166306\n",
            "epoch 2 batch number 91    batch loss: 0.1223190650343895\n",
            "epoch 2 batch number 92    batch loss: 0.11948919296264648\n",
            "epoch 2 batch number 93    batch loss: 0.11643248051404953\n",
            "epoch 2 batch number 94    batch loss: 0.11314112693071365\n",
            "epoch 2 batch number 95    batch loss: 0.11533521860837936\n",
            "epoch 2 batch number 96    batch loss: 0.11628345400094986\n",
            "epoch 2 batch number 97    batch loss: 0.10933683067560196\n",
            "epoch 2 batch number 98    batch loss: 0.15708540380001068\n",
            "epoch 2 batch number 99    batch loss: 0.13885708153247833\n",
            " Average epoch losses: 0.11412391811609268 \n"
          ]
        }
      ],
      "source": [
        "init = tf.global_variables_initializer()\n",
        "epochs_average_loss=[]\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(n_epochs):\n",
        "        print('epoch ',epoch)\n",
        "        LOSS=[]\n",
        "        for i in range(num_batches):\n",
        "          images, blur, _ = train_images.get_batch(batch_size)\n",
        "          batch_loss = sess.run([loss, train], feed_dict={x: images, y: blur})\n",
        "          LOSS.append(batch_loss[0])\n",
        "          print('epoch {} batch number {}    batch loss: {}'.format(epoch, i ,batch_loss[0]))\n",
        "        mean_epoch_Losses=np.mean(LOSS)\n",
        "        epochs_average_loss.append(mean_epoch_Losses)\n",
        "        print(' Average epoch losses: {} '.format(mean_epoch_Losses))\n",
        "       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 656
        },
        "id": "hCv1y10rsafJ",
        "outputId": "ecf107bc-3271-4732-8cbc-1cc1b54b7b33"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "t=np.arange(0,n_epochs,1)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.title('Average Loss Error for {} Epochs. Learning Rate: {}'.format(n_epochs,learning_rate),fontsize=22)\n",
        "plt.plot(t,epochs_average_loss,color='r',label='Mean squar Error loss')\n",
        "#plt.plot(t,Gaverageloss,color='r',label='Generator Overal Error Gloss')\n",
        "#plt.plot(t,ADVloss,color='g',label='Gnerator Adversarial Error G_Adv')\n",
        "#plt.plot(t,MSEloss,color='y',label='Generator MS Error G_MSE')\n",
        "plt.xticks(np.arange(0,n_epochs,1))\n",
        "plt.yticks(np.arange(0,2,0.5))\n",
        "plt.xlabel('EPOCHS',fontsize=18)\n",
        "plt.ylabel('Error',fontsize=18)\n",
        "plt.gca().legend(prop={'size': 18})\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYjYPWWZwXoc"
      },
      "outputs": [],
      "source": [
        "from testdatareader import TDataReader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "batch_size=1\n",
        "test_data=TDataReader('train')\n",
        "num_batches= test_data.num_batches_of_size(batch_size)\n",
        "A=np.arange(1601,1700,1)  \n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    for vb in range(num_batches):\n",
        "            images,images_names = test_data.get_batch(batchsize)\n",
        "            predictions = sess.run([normalized_output], feed_dict={x:images})\n",
        "            p = predictions[0]\n",
        "            p_arr = (p * 255.0).astype(np.uint8)\n",
        "            p_arr=p_arr.reshape(224,224)\n",
        "            p_image = Image.fromarray(p_arr)\n",
        "            \n",
        "            #print(p_image.shape)\n",
        "            p_image.save('predictions/'+ str(vb) + '_prediction' + '.' + 'jpeg')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "image_blur_detection.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPKSc9hA2lm8cP+qu9+DfGk",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}