{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "image_blur_detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPk18IjDbq3/l2fDt+tEC6K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MasoudMoeini/Image-blur-detection/blob/main/image_blur_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "p-5xmsZ5RQes",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e066ea6-e603-465c-de12-7ce360fdbcc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ],
      "source": [
        "%load_ext tensorboard\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import numpy as np\n",
        "import matplotlib . pyplot as plt\n",
        "from tensorflow.keras import layers, losses\n",
        "# Base CNN\n",
        "x = tf.placeholder(tf.float32, (None,224, 224, 3))\n",
        "y = tf.placeholder(tf.float32, (None,224, 224, 1))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!unzip -qq BlurDatasetResultShi.zip\n",
        "#!unzip -qq ccv_data.zip"
      ],
      "metadata": {
        "id": "wJfwkZX9TC3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rm -rf ./logs/"
      ],
      "metadata": {
        "id": "8pmv42pPJ4SW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Conv1\n",
        "# Input Tensor Shape: [batch_size, 224, 224, 3]\n",
        "# Output Tensor Shape: [batch_size, 224, 228, 32]\n",
        "conv1 = tf.layers.conv2d(x, filters=32, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu)\n",
        "\n",
        "# conv2\n",
        "# Input Tensor Shape: [batch_size, 224, 224, 32]\n",
        "# Output Tensor Shape: [batch_size, 224, 228, 32]\n",
        "conv2 = tf.layers.conv2d(conv1, filters=32, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu)\n",
        "\n",
        "# pool1\n",
        "# Input Tensor Shape: [batch_size, 224, 224, 32]\n",
        "# Output Tensor Shape: [batch_size, 112, 112, 32]\n",
        "pool1 = tf.layers.max_pooling2d(conv2, pool_size=[2,2], strides=2, padding=\"same\")\n",
        "\n",
        "#conv3\n",
        "# Input Tensor Shape: [batch_size, 112, 112, 32]\n",
        "# Output Tensor Shape: [batch_size, 112, 112, 64]\n",
        "conv3 = tf.layers.conv2d(conv2, filters=64, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu)\n",
        "\n",
        "#conv4\n",
        "# Input Tensor Shape: [batch_size, 112, 112, 64]\n",
        "# Output Tensor Shape: [batch_size, 112, 112, 64]\n",
        "conv4 = tf.layers.conv2d(conv3, filters=64, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu)\n",
        "\n",
        "#pool2\n",
        "# Input Tensor Shape: [batch_size, 112, 112, 64]\n",
        "# Output Tensor Shape: [batch_size, 56, 56, 64]\n",
        "pool2 = tf.layers.max_pooling2d(conv4, pool_size=[2,2], strides=2, padding=\"same\")\n",
        "\n",
        "#conv5\n",
        "# Input Tensor Shape: [batch_size, 56, 56, 64]\n",
        "# Output Tensor Shape: [batch_size, 56, 56, 128]\n",
        "conv5 = tf.layers.conv2d(pool2, filters=128, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu)\n",
        "\n",
        "#conv6\n",
        "# Input Tensor Shape: [batch_size, 56, 56, 128]\n",
        "# Output Tensor Shape: [batch_size, 56, 56, 128]\n",
        "conv6 = tf.layers.conv2d(conv5, filters=128, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu)\n",
        "\n",
        "\n",
        "#pool3\n",
        "# Input Tensor Shape: [batch_size, 56, 56, 128]\n",
        "# Output Tensor Shape: [batch_size, 28, 28, 128]\n",
        "pool3 = tf.layers.max_pooling2d(conv6, pool_size=[2,2], strides=2, padding=\"same\")\n",
        "\n",
        "#conv7\n",
        "# Input Tensor Shape: [batch_size, 28, 28, 128]\n",
        "# Output Tensor Shape: [batch_size, 28, 28, 256]\n",
        "conv7 = tf.layers.conv2d(pool3, filters=256, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu)\n",
        "#conv8\n",
        "# Input Tensor Shape: [batch_size, 28, 28, 256]\n",
        "# Output Tensor Shape: [batch_size, 28, 28, 256]\n",
        "conv8 = tf.layers.conv2d(conv7, filters=128, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu)\n",
        "\n",
        "#pool4\n",
        "# Input Tensor Shape: [batch_size, 28, 28, 256]\n",
        "# Output Tensor Shape: [batch_size, 14, 14, 256]\n",
        "\n",
        "pool4 = tf.layers.max_pooling2d(conv8, pool_size=[2,2], strides=2, padding=\"same\")\n",
        "#conv9\n",
        "# Input Tensor Shape: [batch_size, 14, 14, 256]\n",
        "# Output Tensor Shape: [batch_size, 14, 14, 512]\n",
        "conv9 = tf.layers.conv2d(pool4, filters=512, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu)\n",
        "\n",
        "#pool5\n",
        "# Input Tensor Shape: [batch_size, 14, 14, 512]\n",
        "# Output Tensor Shape: [batch_size, 7, 7, 512]\n",
        "pool5 = tf.layers.max_pooling2d(conv9, pool_size=[2,2], strides=2, padding=\"same\")\n",
        "\n",
        "#dim = int(np.prod(pool5.get_shape()[1:])) #7*7*512\n",
        "#fcl = tf.reshape(pool5, shape=[-1, dim], name ='fc1')#[batch_size,7*7*512]\n",
        "# decoder\n",
        "\n",
        "# Input Tensor Shape: [batch_size, 7, 7, 512]\n",
        "# Output Tensor Shape: [batch_size, 14, 14, 512]\n",
        "net=tf.layers.conv2d_transpose(pool5,512,[3, 3],strides = 2,padding='SAME')\n",
        "\n",
        "# Input Tensor Shape: [batch_size, 7, 7, 512]\n",
        "# Output Tensor Shape: [batch_size, 28, 28, 256]\n",
        "net=tf.layers.conv2d_transpose(net,256,[3, 3],strides = 2,padding='SAME')\n",
        "\n",
        "# Input Tensor Shape: [batch_size, 28, 28, 128]\n",
        "# Output Tensor Shape: [batch_size, 224, 224, 128]\n",
        "net=tf.layers.conv2d_transpose(net,128,[3, 3],strides = 4,padding='SAME')\n",
        "\n",
        "# Input Tensor Shape: [batch_size, 224, 224, 128]\n",
        "# Output Tensor Shape: [batch_size, 224, 224, 64]\n",
        "net=tf.layers.conv2d_transpose(net,64,[3, 3],strides = 1,padding='SAME')\n",
        "\n",
        "# Input Tensor Shape: [batch_size, 224, 224, 64]\n",
        "# Output Tensor Shape: [batch_size, 224, 224, 32]\n",
        "net=tf.layers.conv2d_transpose(net,32,[3, 3],strides = 1, padding='SAME', activation = tf.nn.tanh)\n",
        "\n",
        "# Input Tensor Shape: [batch_size, 224, 224, 32]\n",
        "# Output Tensor Shape: [batch_size, 224, 224, 1]\n",
        "net=tf.layers.conv2d_transpose(net,1,[3, 3],strides = 1, padding='SAME', activation = tf.nn.tanh)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BbR2mvcKA5S",
        "outputId": "ebeeca51-9e90-4a4f-cafa-6cb214aa5a7d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.7/dist-packages/keras/legacy_tf_layers/convolutional.py:575: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: UserWarning: `tf.layers.max_pooling2d` is deprecated and will be removed in a future version. Please use `tf.keras.layers.MaxPooling2D` instead.\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/keras/legacy_tf_layers/pooling.py:600: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: `tf.layers.max_pooling2d` is deprecated and will be removed in a future version. Please use `tf.keras.layers.MaxPooling2D` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:34: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:39: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: UserWarning: `tf.layers.max_pooling2d` is deprecated and will be removed in a future version. Please use `tf.keras.layers.MaxPooling2D` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:50: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:54: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: UserWarning: `tf.layers.max_pooling2d` is deprecated and will be removed in a future version. Please use `tf.keras.layers.MaxPooling2D` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:64: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:69: UserWarning: `tf.layers.max_pooling2d` is deprecated and will be removed in a future version. Please use `tf.keras.layers.MaxPooling2D` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:77: UserWarning: `tf.layers.conv2d_transpose` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2DTranspose` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/keras/legacy_tf_layers/convolutional.py:1736: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:81: UserWarning: `tf.layers.conv2d_transpose` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2DTranspose` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:85: UserWarning: `tf.layers.conv2d_transpose` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2DTranspose` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:89: UserWarning: `tf.layers.conv2d_transpose` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2DTranspose` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:93: UserWarning: `tf.layers.conv2d_transpose` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2DTranspose` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:97: UserWarning: `tf.layers.conv2d_transpose` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2DTranspose` instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qq ccv_data_fix.zip\n"
      ],
      "metadata": {
        "id": "JkQpeiENg4tD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datareader import DataReader\n",
        "train_images = DataReader('ccv_data_fix/train')"
      ],
      "metadata": {
        "id": "P-CIQbYmlE3j"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Optimize\n",
        "batch_size=50\n",
        "num_batches= train_images.num_batches_of_size(batch_size)\n",
        "learning_rate = 0.01\n",
        "n_epochs = 3\n",
        "loss = tf.reduce_mean(tf.square(net - y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "train  = optimizer.minimize(loss)"
      ],
      "metadata": {
        "id": "FBu6qF1NS4I6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "init = tf.global_variables_initializer()\n",
        "averageloss=[]\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(n_epochs):\n",
        "        print('epoch ',epoch)\n",
        "        DLOSS=[]\n",
        "        for i in range(num_batches):\n",
        "          images, fixations, images_to_read = train_images.get_batch(batch_size)\n",
        "          batch_loss = sess.run([loss, train], feed_dict={x: images, y: fixations})\n",
        "          DLOSS.append(batch_loss[0])\n",
        "          print('epoch {} batch number {}    batch loss: {}'.format(epoch, i ,batch_loss[0]))\n",
        "        MeanDloss=np.mean(DLOSS)\n",
        "        averageloss.append(np.mean(DLOSS))\n",
        "        print(' Average batches loss: {} '.format(MeanDloss))\n",
        "       "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPqqHoDRP0e_",
        "outputId": "da9e0e15-6c12-45f6-d466-d47d4ec57f94"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch  0\n",
            "epoch 0 batch number 0    batch loss: 0.023021379485726357\n",
            "epoch 0 batch number 1    batch loss: 0.8725675344467163\n",
            "epoch 0 batch number 2    batch loss: 0.6789113283157349\n",
            "epoch 0 batch number 3    batch loss: 0.7397556304931641\n",
            "epoch 0 batch number 4    batch loss: 0.7735946774482727\n",
            "epoch 0 batch number 5    batch loss: 0.6340379118919373\n",
            "epoch 0 batch number 6    batch loss: 0.3940880298614502\n",
            "epoch 0 batch number 7    batch loss: 0.5706986784934998\n",
            "epoch 0 batch number 8    batch loss: 0.5776880383491516\n",
            "epoch 0 batch number 9    batch loss: 0.41244059801101685\n",
            "epoch 0 batch number 10    batch loss: 0.3525676131248474\n",
            "epoch 0 batch number 11    batch loss: 0.44757622480392456\n",
            "epoch 0 batch number 12    batch loss: 0.41148003935813904\n",
            "epoch 0 batch number 13    batch loss: 0.3123934268951416\n",
            "epoch 0 batch number 14    batch loss: 0.31603360176086426\n",
            "epoch 0 batch number 15    batch loss: 0.3648665249347687\n",
            "epoch 0 batch number 16    batch loss: 0.3230607807636261\n",
            "epoch 0 batch number 17    batch loss: 0.26229432225227356\n",
            "epoch 0 batch number 18    batch loss: 0.31421390175819397\n",
            "epoch 0 batch number 19    batch loss: 0.3491474390029907\n",
            "epoch 0 batch number 20    batch loss: 0.3383766710758209\n",
            "epoch 0 batch number 21    batch loss: 0.3364241421222687\n",
            "epoch 0 batch number 22    batch loss: 0.36735162138938904\n",
            "epoch 0 batch number 23    batch loss: 0.3513305187225342\n",
            " Average batches loss: 0.4384966790676117 \n",
            "epoch  1\n",
            "epoch 1 batch number 0    batch loss: 0.3331705331802368\n",
            "epoch 1 batch number 1    batch loss: 0.30684933066368103\n",
            "epoch 1 batch number 2    batch loss: 0.28010496497154236\n",
            "epoch 1 batch number 3    batch loss: 0.267126202583313\n",
            "epoch 1 batch number 4    batch loss: 0.2659793794155121\n",
            "epoch 1 batch number 5    batch loss: 0.25103452801704407\n",
            "epoch 1 batch number 6    batch loss: 0.23785211145877838\n",
            "epoch 1 batch number 7    batch loss: 0.23405806720256805\n",
            "epoch 1 batch number 8    batch loss: 0.23290911316871643\n",
            "epoch 1 batch number 9    batch loss: 0.2176101803779602\n",
            "epoch 1 batch number 10    batch loss: 0.23736880719661713\n",
            "epoch 1 batch number 11    batch loss: 0.21879932284355164\n",
            "epoch 1 batch number 12    batch loss: 0.22503475844860077\n",
            "epoch 1 batch number 13    batch loss: 0.21783581376075745\n",
            "epoch 1 batch number 14    batch loss: 0.2090381532907486\n",
            "epoch 1 batch number 15    batch loss: 0.19522717595100403\n",
            "epoch 1 batch number 16    batch loss: 0.19713139533996582\n",
            "epoch 1 batch number 17    batch loss: 0.18967768549919128\n",
            "epoch 1 batch number 18    batch loss: 0.17812931537628174\n",
            "epoch 1 batch number 19    batch loss: 0.16073167324066162\n",
            "epoch 1 batch number 20    batch loss: 0.15280483663082123\n",
            "epoch 1 batch number 21    batch loss: 0.16184298694133759\n",
            "epoch 1 batch number 22    batch loss: 0.15858767926692963\n",
            "epoch 1 batch number 23    batch loss: 0.1534709632396698\n",
            " Average batches loss: 0.22009897232055664 \n",
            "epoch  2\n",
            "epoch 2 batch number 0    batch loss: 0.1466342657804489\n",
            "epoch 2 batch number 1    batch loss: 0.14022471010684967\n",
            "epoch 2 batch number 2    batch loss: 0.13346049189567566\n",
            "epoch 2 batch number 3    batch loss: 0.13093723356723785\n",
            "epoch 2 batch number 4    batch loss: 0.12585227191448212\n",
            "epoch 2 batch number 5    batch loss: 0.12046390771865845\n",
            "epoch 2 batch number 6    batch loss: 0.1134740337729454\n",
            "epoch 2 batch number 7    batch loss: 0.1071571335196495\n",
            "epoch 2 batch number 8    batch loss: 0.11557457596063614\n",
            "epoch 2 batch number 9    batch loss: 0.11000585556030273\n",
            "epoch 2 batch number 10    batch loss: 0.1141679435968399\n",
            "epoch 2 batch number 11    batch loss: 0.11916201561689377\n",
            "epoch 2 batch number 12    batch loss: 0.10275670886039734\n",
            "epoch 2 batch number 13    batch loss: 0.09975816309452057\n",
            "epoch 2 batch number 14    batch loss: 0.09615747630596161\n",
            "epoch 2 batch number 15    batch loss: 0.09649867564439774\n",
            "epoch 2 batch number 16    batch loss: 0.092743881046772\n",
            "epoch 2 batch number 17    batch loss: 0.08770471811294556\n",
            "epoch 2 batch number 18    batch loss: 0.09062078595161438\n",
            "epoch 2 batch number 19    batch loss: 0.08945026993751526\n",
            "epoch 2 batch number 20    batch loss: 0.0788906067609787\n",
            "epoch 2 batch number 21    batch loss: 0.0793372094631195\n",
            "epoch 2 batch number 22    batch loss: 0.07269713282585144\n",
            "epoch 2 batch number 23    batch loss: 0.06692594289779663\n",
            " Average batches loss: 0.10544400662183762 \n"
          ]
        }
      ]
    }
  ]
}